{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 實作"
      ],
      "metadata": {
        "id": "_RDyiyZklpAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我們將注意力機制用在GNN中，實現GAT。"
      ],
      "metadata": {
        "id": "YaST7g_NltMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 論文分類"
      ],
      "metadata": {
        "id": "-l_xGcrlmdTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "搭建GAT模型，對數據集中的論文信息進行分析，使模型學習已有論文的分類特徵，以便預測出未知分類的論文類別。"
      ],
      "metadata": {
        "id": "OtB_hO9hmoxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 載入套件"
      ],
      "metadata": {
        "id": "ziXdwK7UpzLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import coo_matrix,csr_matrix,diags,eye\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NuJj_pc-kUp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 載入數據集"
      ],
      "metadata": {
        "id": "40QIorhWp8vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用CORA數據集，它是由機器學習的論文整理而來。"
      ],
      "metadata": {
        "id": "DERF8wlJHDAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在該數據集中，記錄了每篇論文用到的關鍵詞，以及論文之間互相引用的關係。"
      ],
      "metadata": {
        "id": "Ha-L4JR0HSNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "論文共分為7類:\n",
        "* 基於案例\n",
        "* 遺傳算法\n",
        "* 神經網絡\n",
        "* 機率方法\n",
        "* 強化學習\n",
        "* 規則學習\n",
        "* 理論"
      ],
      "metadata": {
        "id": "an7ZL8tSHqaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "數據集中共有2708篇論文，每一篇論文都引用或至少被一篇其他論文所引用。"
      ],
      "metadata": {
        "id": "cKcxkDE3H1eT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "整個語料庫共有2708篇論文，同時，有將所有論文中，詞幹、停止詞、低頻詞刪除，留下1433個關鍵詞，作為論文的特體特徵。"
      ],
      "metadata": {
        "id": "J4iROT_kIKx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* content文件包含以下格式的論文說明:\n",
        "\n",
        "每列的第一條目，包含論文的為一字符串ID，隨後用一個二進制值，指示詞彙表中的每個單詞，在紙張中存在(1)或不存在(0)。列中的最後一項，包含紙張的類標籤。\n",
        "\n",
        "* cites文件包含了語料庫的引文圖，每一列用以下格式，描述一個鏈接:\n",
        "\n",
        "每列包含兩個紙張ID。第一條目是被引用論文的ID，第二條目是引用論文的ID。鏈接的方式是\"從右向左\"。例如，如果一列用\"paper1 paper2\"表示，則鏈接為\"paper2→paper1\"。"
      ],
      "metadata": {
        "id": "Bo1hOXNhIl3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 解壓縮數據集\n",
        "\n",
        "!unzip -q /content/drive/MyDrive/Datasets/cora.zip"
      ],
      "metadata": {
        "id": "8Z6wV3rop1ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "path = Path('./cora')\n",
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wey7eCiAqYet",
        "outputId": "cad9e2f0-708e-4e73-9bbb-f8f52750df39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "cora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 讀取並解析數據"
      ],
      "metadata": {
        "id": "iYY0_i1oql4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper_features_label = np.genfromtxt(path/'cora.content', dtype=np.str)\n",
        "print(paper_features_label,np.shape(paper_features_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPcgCZzUqfXA",
        "outputId": "4580c6ff-b5c6-4c0c-ddb0-ec78c347924a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['31336' '0' '0' ... '0' '0' 'Neural_Networks']\n",
            " ['1061127' '0' '0' ... '0' '0' 'Rule_Learning']\n",
            " ['1106406' '0' '0' ... '0' '0' 'Reinforcement_Learning']\n",
            " ...\n",
            " ['1128978' '0' '0' ... '0' '0' 'Genetic_Algorithms']\n",
            " ['117328' '0' '0' ... '0' '0' 'Case_Based']\n",
            " ['24043' '0' '0' ... '0' '0' 'Neural_Networks']] (2708, 1435)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2708代表論文數量，每篇論文共有1435個屬性。"
      ],
      "metadata": {
        "id": "6jFH6ZVqMMnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "papers = paper_features_label[:,0].astype(np.int32)\n",
        "print(papers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fifwi-MYqn2x",
        "outputId": "3918ce27-881c-48b7-b35d-1bfef30af71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  31336 1061127 1106406 ... 1128978  117328   24043]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper2idx = {k:v for v,k in enumerate(papers)}\n",
        "print(paper2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTK7h0HZqw38",
        "outputId": "59c878cc-03ab-4ed2-9701-5ae4ad01250e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{31336: 0, 1061127: 1, 1106406: 2, 13195: 3, 37879: 4, 1126012: 5, 1107140: 6, 1102850: 7, 31349: 8, 1106418: 9, 1123188: 10, 1128990: 11, 109323: 12, 217139: 13, 31353: 14, 32083: 15, 1126029: 16, 1118017: 17, 49482: 18, 753265: 19, 249858: 20, 1113739: 21, 48766: 22, 646195: 23, 1126050: 24, 59626: 25, 340299: 26, 354004: 27, 242637: 28, 1106492: 29, 74975: 30, 1152272: 31, 100701: 32, 66982: 33, 13960: 34, 13966: 35, 66990: 36, 182093: 37, 182094: 38, 13972: 39, 13982: 40, 16819: 41, 273152: 42, 237521: 43, 1153703: 44, 32872: 45, 284025: 46, 218666: 47, 16843: 48, 1153724: 49, 1153728: 50, 158098: 51, 8699: 52, 1134865: 53, 28456: 54, 248425: 55, 1112319: 56, 28471: 57, 175548: 58, 696345: 59, 28485: 60, 1139195: 61, 35778: 62, 28491: 63, 310530: 64, 1153784: 65, 1481: 66, 1153786: 67, 13212: 68, 1111614: 69, 5055: 70, 4329: 71, 330148: 72, 1105062: 73, 4330: 74, 5062: 75, 4335: 76, 158812: 77, 40124: 78, 1103610: 79, 688361: 80, 302545: 81, 20534: 82, 1031453: 83, 5086: 84, 193742: 85, 58268: 86, 424: 87, 40151: 88, 636098: 89, 260121: 90, 950052: 91, 434: 92, 1131270: 93, 1131274: 94, 1131277: 95, 1110947: 96, 662279: 97, 1139928: 98, 153063: 99, 134199: 100, 641956: 101, 20584: 102, 1130567: 103, 171225: 104, 714879: 105, 37998: 106, 50336: 107, 50337: 108, 15429: 109, 23448: 110, 1122574: 111, 1110998: 112, 853150: 113, 15431: 114, 646286: 115, 1152307: 116, 1115291: 117, 1106547: 118, 68463: 119, 59715: 120, 69198: 121, 7272: 122, 163235: 123, 7276: 124, 34315: 125, 644843: 126, 7297: 127, 628815: 128, 35061: 129, 68495: 130, 1136310: 131, 18313: 132, 34355: 133, 45212: 134, 1153091: 135, 8703: 136, 126920: 137, 126927: 138, 595157: 139, 140005: 140, 1117476: 141, 59798: 142, 219446: 143, 44514: 144, 287787: 145, 157401: 146, 1154500: 147, 682666: 148, 399173: 149, 198866: 150, 51834: 151, 200630: 152, 782486: 153, 1136393: 154, 137849: 155, 1153811: 156, 24966: 157, 11148: 158, 51866: 159, 24974: 160, 137868: 161, 28542: 162, 35: 163, 116021: 164, 348305: 165, 10430: 166, 39403: 167, 40: 168, 282700: 169, 1105116: 170, 35854: 171, 63477: 172, 124064: 173, 1120431: 174, 949318: 175, 649944: 176, 63486: 177, 1153866: 178, 1140040: 179, 1112426: 180, 239800: 181, 1131314: 182, 1153891: 183, 1129835: 184, 310653: 185, 1130600: 186, 1111733: 187, 210871: 188, 210872: 189, 1132083: 190, 132806: 191, 12631: 192, 12638: 193, 38771: 194, 232605: 195, 232606: 196, 1107312: 197, 1114605: 198, 68505: 199, 133553: 200, 144408: 201, 23502: 202, 1108050: 203, 23507: 204, 83826: 205, 133563: 206, 85299: 207, 49660: 208, 593060: 209, 341188: 210, 714975: 211, 1115375: 212, 95435: 213, 145176: 214, 1113934: 215, 1132809: 216, 22835: 217, 1153148: 218, 41714: 219, 1118245: 220, 1152436: 221, 1153166: 222, 1153169: 223, 38000: 224, 1152448: 225, 1137140: 226, 30895: 227, 5966: 228, 1136422: 229, 27174: 230, 1128407: 231, 1124844: 232, 1153195: 233, 1113995: 234, 1136442: 235, 8821: 236, 46079: 237, 119761: 238, 1111052: 239, 315789: 240, 1108841: 241, 1135746: 242, 100935: 243, 353541: 244, 60682: 245, 253762: 246, 8872: 247, 714260: 248, 137956: 249, 35922: 250, 2354: 251, 168410: 252, 346292: 253, 1153933: 254, 1119751: 255, 17798: 256, 400356: 257, 10531: 258, 1110390: 259, 714289: 260, 733167: 261, 81714: 262, 428610: 263, 552469: 264, 164885: 265, 81722: 266, 111866: 267, 194617: 268, 93318: 269, 134307: 270, 203646: 271, 367312: 272, 650814: 273, 93320: 274, 134315: 275, 134316: 276, 976334: 277, 1095507: 278, 134320: 279, 662416: 280, 194645: 281, 1131421: 282, 161221: 283, 38839: 284, 38846: 285, 133615: 286, 1112574: 287, 521207: 288, 3828: 289, 593105: 290, 390693: 291, 642847: 292, 1122704: 293, 4584: 294, 7419: 295, 30901: 296, 1115456: 297, 7432: 298, 573553: 299, 1022969: 300, 143801: 301, 612306: 302, 417017: 303, 396412: 304, 1107455: 305, 91975: 306, 180187: 307, 27203: 308, 1152508: 309, 69392: 310, 1118332: 311, 189577: 312, 1114777: 313, 75969: 314, 1132922: 315, 1153254: 316, 1117618: 317, 6767: 318, 27241: 319, 27246: 320, 95589: 321, 6771: 322, 86840: 323, 108962: 324, 6786: 325, 108963: 326, 108974: 327, 1117653: 328, 1152569: 329, 1132968: 330, 370366: 331, 108983: 332, 399339: 333, 64319: 334, 1110426: 335, 1102407: 336, 1127812: 337, 1128542: 338, 65057: 339, 159084: 340, 159085: 341, 65074: 342, 33895: 343, 2440: 344, 1717: 345, 249421: 346, 3187: 347, 591016: 348, 1110494: 349, 29492: 350, 400473: 351, 644334: 352, 949511: 353, 205192: 354, 763009: 355, 169280: 356, 1120643: 357, 645088: 358, 5348: 359, 124296: 360, 1121398: 361, 950305: 362, 567018: 363, 52000: 364, 52003: 365, 52007: 366, 58540: 367, 436796: 368, 948846: 369, 8213: 370, 671293: 371, 1131550: 372, 899119: 373, 1105394: 374, 85452: 375, 1112686: 376, 69418: 377, 8224: 378, 145315: 379, 575077: 380, 20850: 381, 44017: 382, 1135125: 383, 286562: 384, 1123553: 385, 1135137: 386, 325314: 387, 662572: 388, 159897: 389, 1130856: 390, 96335: 391, 755082: 392, 1123576: 393, 1103979: 394, 593260: 395, 601567: 396, 1119140: 397, 189655: 398, 31769: 399, 1107567: 400, 88356: 401, 1033: 402, 1034: 403, 1106849: 404, 16470: 405, 35343: 406, 16471: 407, 1154074: 408, 16476: 409, 23774: 410, 16485: 411, 136665: 412, 94953: 413, 9708: 414, 38205: 415, 645897: 416, 216877: 417, 18619: 418, 559804: 419, 6898: 420, 166420: 421, 787016: 422, 73146: 423, 1136634: 424, 1111230: 425, 3218: 426, 3229: 427, 193347: 428, 84020: 429, 3231: 430, 52847: 431, 193352: 432, 193354: 433, 1110531: 434, 686532: 435, 711598: 436, 1063773: 437, 3243: 438, 78994: 439, 181782: 440, 284414: 441, 114189: 442, 686559: 443, 253971: 444, 1106103: 445, 1114125: 446, 75318: 447, 45599: 448, 97892: 449, 446271: 450, 1106112: 451, 280876: 452, 12182: 453, 175909: 454, 64484: 455, 6125: 456, 1120713: 457, 1114153: 458, 12197: 459, 248823: 460, 919885: 461, 94229: 462, 1120731: 463, 23069: 464, 6151: 465, 6155: 466, 23070: 467, 644448: 468, 1112723: 469, 31097: 470, 6169: 471, 1106172: 472, 6170: 473, 211875: 474, 1109017: 475, 5454: 476, 6184: 477, 10796: 478, 10798: 479, 1120777: 480, 86258: 481, 154134: 482, 6196: 483, 20920: 484, 20923: 485, 22386: 486, 1131639: 487, 77515: 488, 93555: 489, 17201: 490, 644494: 491, 17208: 492, 1125082: 493, 1131647: 494, 74698: 495, 13652: 496, 20942: 497, 390894: 498, 390896: 499, 1125092: 500, 13656: 501, 1116347: 502, 13658: 503, 114966: 504, 120013: 505, 1117089: 506, 57948: 507, 334153: 508, 160732: 509, 1154103: 510, 12946: 511, 1104787: 512, 17242: 513, 321861: 514, 189721: 515, 1119211: 516, 12960: 517, 95718: 518, 6910: 519, 180373: 520, 6917: 521, 358884: 522, 887: 523, 180399: 524, 358894: 525, 1154169: 526, 120084: 527, 1120019: 528, 1152711: 529, 1154176: 530, 424540: 531, 1118546: 532, 643003: 533, 112099: 534, 1104007: 535, 1120049: 536, 175256: 537, 45605: 538, 15889: 539, 35490: 540, 221302: 541, 562123: 542, 1104031: 543, 1129442: 544, 1129443: 545, 1137466: 546, 328370: 547, 1103315: 548, 12210: 549, 1104055: 550, 64519: 551, 114: 552, 1109873: 553, 128: 554, 12238: 555, 1112099: 556, 18774: 557, 18777: 558, 130: 559, 23116: 560, 948299: 561, 6209: 562, 197054: 563, 6210: 564, 6213: 565, 6214: 566, 6216: 567, 6217: 568, 2653: 569, 2658: 570, 753047: 571, 188318: 572, 74700: 573, 67415: 574, 6220: 575, 2665: 576, 28957: 577, 143323: 578, 340075: 579, 1949: 580, 1953: 581, 1955: 582, 1959: 583, 390922: 584, 22431: 585, 1113541: 586, 1132418: 587, 628500: 588, 648106: 589, 1104809: 590, 4804: 591, 648112: 592, 33301: 593, 33303: 594, 267824: 595, 1138970: 596, 13717: 597, 1131719: 598, 1120866: 599, 1106287: 600, 755217: 601, 647408: 602, 1116410: 603, 1132459: 604, 1105574: 605, 1133196: 606, 307336: 607, 906: 608, 1131745: 609, 1131748: 610, 910: 611, 943: 612, 31927: 613, 101261: 614, 101263: 615, 31932: 616, 779960: 617, 1135358: 618, 1154230: 619, 1135368: 620, 28227: 621, 32688: 622, 189856: 623, 27510: 624, 27514: 625, 1154276: 626, 27530: 627, 1152821: 628, 28265: 629, 103430: 630, 27543: 631, 39126: 632, 28278: 633, 39131: 634, 10169: 635, 28287: 636, 1129518: 637, 1272: 638, 194223: 639, 10177: 640, 18811: 641, 18812: 642, 73327: 643, 1117942: 644, 15984: 645, 202522: 646, 1152858: 647, 1152859: 648, 10183: 649, 81350: 650, 259126: 651, 13024: 652, 1120170: 653, 46452: 654, 26850: 655, 18832: 656, 18833: 657, 82098: 658, 103482: 659, 158614: 660, 46468: 661, 71904: 662, 80656: 663, 29708: 664, 1128839: 665, 1128846: 666, 12330: 667, 240321: 668, 1128853: 669, 219976: 670, 38480: 671, 12350: 672, 1104191: 673, 7022: 674, 63931: 675, 68224: 676, 1110768: 677, 384428: 678, 1107041: 679, 1114352: 680, 1107062: 681, 288: 682, 1107067: 683, 91581: 684, 39904: 685, 6334: 686, 123825: 687, 23258: 688, 66805: 689, 6346: 690, 55968: 691, 368431: 692, 179702: 693, 1140547: 694, 1114388: 695, 90888: 696, 510715: 697, 33412: 698, 188471: 699, 1152143: 700, 1120962: 701, 1125258: 702, 648232: 703, 143476: 704, 1152150: 705, 1117249: 706, 25413: 707, 1152162: 708, 241821: 709, 350362: 710, 1116530: 711, 61069: 712, 1110000: 713, 646809: 714, 1105698: 715, 1152194: 716, 198653: 717, 1116569: 718, 77758: 719, 854434: 720, 1128151: 721, 1123867: 722, 191404: 723, 1116594: 724, 126793: 725, 43639: 726, 44368: 727, 97390: 728, 87915: 729, 131117: 730, 8581: 731, 27606: 732, 1115886: 733, 184157: 734, 8594: 735, 1152904: 736, 1120211: 737, 28350: 738, 1152910: 739, 27627: 740, 649731: 741, 308920: 742, 289780: 743, 289781: 744, 19621: 745, 1129608: 746, 1365: 747, 103543: 748, 28387: 749, 28389: 750, 43698: 751, 54550: 752, 1129621: 753, 46536: 754, 1129629: 755, 294126: 756, 568857: 757, 447224: 758, 38537: 759, 1152975: 760, 34979: 761, 1104261: 762, 139865: 763, 56709: 764, 1128945: 765, 19697: 766, 107177: 767, 1131165: 768, 1128959: 769, 152219: 770, 184918: 771, 16008: 772, 1122425: 773, 928873: 774, 206259: 775, 714748: 776, 1131189: 777, 217115: 778, 560936: 779, 1131198: 780, 1128985: 781, 466170: 782, 429805: 783, 561674: 784, 654177: 785, 95225: 786, 37884: 787, 37888: 788, 1128997: 789, 545647: 790, 42207: 791, 42209: 792, 82920: 793, 128202: 794, 128203: 795, 1134056: 796, 1102873: 797, 42221: 798, 1107171: 799, 1133338: 800, 67633: 801, 375825: 802, 48781: 803, 75674: 804, 289088: 805, 1152244: 806, 13917: 807, 75695: 808, 34257: 809, 1117348: 810, 574710: 811, 34263: 812, 1128204: 813, 34266: 814, 1128208: 815, 1116629: 816, 110162: 817, 110163: 818, 110164: 819, 628751: 820, 708945: 821, 1123926: 822, 1152277: 823, 77826: 824, 77829: 825, 8617: 826, 242663: 827, 8619: 828, 628764: 829, 628766: 830, 1125393: 831, 66986: 832, 646913: 833, 578309: 834, 18251: 835, 1152290: 836, 954315: 837, 212107: 838, 578337: 839, 907845: 840, 1127530: 841, 1128267: 842, 28412: 843, 594387: 844, 1127541: 845, 44455: 846, 45188: 847, 45189: 848, 62607: 849, 1127551: 850, 1123991: 851, 1127558: 852, 105057: 853, 1128291: 854, 1127566: 855, 1154459: 856, 218682: 857, 28447: 858, 1153736: 859, 62634: 860, 211432: 861, 112378: 862, 1113035: 863, 1118848: 864, 137790: 865, 217984: 866, 949217: 867, 28473: 868, 1104300: 869, 1105033: 870, 11093: 871, 696342: 872, 696343: 873, 696346: 874, 28487: 875, 5038: 876, 195150: 877, 62676: 878, 13213: 879, 576973: 880, 35797: 881, 134128: 882, 166825: 883, 175576: 884, 509379: 885, 1113084: 886, 53942: 887, 642621: 888, 1131236: 889, 1112369: 890, 446610: 891, 644093: 892, 411092: 893, 642641: 894, 408885: 895, 1131258: 896, 1131267: 897, 13269: 898, 1104379: 899, 1114502: 900, 1107215: 901, 83725: 902, 84459: 903, 642681: 904, 445938: 905, 1103676: 906, 1130568: 907, 1153003: 908, 51045: 909, 12576: 910, 144330: 911, 105865: 912, 51052: 913, 746058: 914, 1153014: 915, 641976: 916, 561789: 917, 1130586: 918, 368605: 919, 1133428: 920, 1113828: 921, 129042: 922, 129045: 923, 6539: 924, 1153031: 925, 1122580: 926, 1132706: 927, 1152308: 928, 105899: 929, 50354: 930, 1121867: 931, 1113852: 932, 1153056: 933, 94641: 934, 1153065: 935, 1133469: 936, 35070: 937, 576257: 938, 368657: 939, 1129018: 940, 263069: 941, 1129027: 942, 1152358: 943, 1125467: 944, 1125469: 945, 72101: 946, 40922: 947, 1153097: 948, 1109439: 949, 423463: 950, 128383: 951, 683360: 952, 1129040: 953, 52515: 954, 41666: 955, 1128319: 956, 1152379: 957, 1136342: 958, 1125492: 959, 1108728: 960, 265203: 961, 628888: 962, 1127619: 963, 56112: 964, 56115: 965, 56119: 966, 89547: 967, 51831: 968, 91038: 969, 96847: 970, 521855: 971, 594483: 972, 1119623: 973, 96851: 974, 1136397: 975, 158172: 976, 1127657: 977, 131315: 978, 131318: 979, 289945: 980, 62718: 981, 229635: 982, 56167: 983, 1119654: 984, 51879: 985, 10435: 986, 137873: 987, 168332: 988, 330208: 989, 689152: 990, 1120444: 991, 1153877: 992, 111770: 993, 1153879: 994, 108047: 995, 1131300: 996, 362926: 997, 129896: 998, 129897: 999, 59045: 1000, 1153889: 1001, 239810: 1002, 20601: 1003, 20602: 1004, 416964: 1005, 38722: 1006, 72908: 1007, 116081: 1008, 1153897: 1009, 116084: 1010, 116087: 1011, 1113182: 1012, 1131330: 1013, 582139: 1014, 561809: 1015, 14062: 1016, 1104449: 1017, 39474: 1018, 27895: 1019, 167670: 1020, 1131345: 1021, 1131348: 1022, 14083: 1023, 1103737: 1024, 65650: 1025, 93273: 1026, 65653: 1027, 5194: 1028, 14090: 1029, 1131360: 1030, 1130634: 1031, 976284: 1032, 1130637: 1033, 593022: 1034, 1131374: 1035, 975567: 1036, 133550: 1037, 145134: 1038, 1130653: 1039, 1130657: 1040, 1104495: 1041, 133566: 1042, 133567: 1043, 1122642: 1044, 1114629: 1045, 91852: 1046, 91853: 1047, 376704: 1048, 1153101: 1049, 32276: 1050, 1130678: 1051, 83847: 1052, 8079: 1053, 593068: 1054, 285675: 1055, 1130680: 1056, 1106630: 1057, 278394: 1058, 285687: 1059, 69284: 1060, 6639: 1061, 14807: 1062, 152483: 1063, 683404: 1064, 593091: 1065, 1117501: 1066, 99023: 1067, 99025: 1068, 513189: 1069, 1152421: 1070, 1153150: 1071, 99030: 1072, 1105932: 1073, 1153160: 1074, 1106671: 1075, 531348: 1076, 577086: 1077, 531351: 1078, 25702: 1079, 87482: 1080, 135765: 1081, 135766: 1082, 1132864: 1083, 22886: 1084, 1118286: 1085, 162664: 1086, 1109542: 1087, 1116835: 1088, 1116839: 1089, 1103016: 1090, 1128425: 1091, 1116842: 1092, 1136446: 1093, 1136447: 1094, 27199: 1095, 1125597: 1096, 1132887: 1097, 593813: 1098, 594543: 1099, 917493: 1100, 1128430: 1101, 51909: 1102, 1108834: 1103, 1128437: 1104, 989397: 1105, 97645: 1106, 8832: 1107, 1103031: 1108, 346243: 1109, 1119708: 1110, 36620: 1111, 25772: 1112, 640617: 1113, 8865: 1114, 950986: 1115, 35905: 1116, 8875: 1117, 25791: 1118, 100961: 1119, 738941: 1120, 64271: 1121, 3084: 1122, 3085: 1123, 28649: 1124, 3095: 1125, 3097: 1126, 1153943: 1127, 1121254: 1128, 74427: 1129, 231249: 1130, 1105221: 1131, 28674: 1132, 1129907: 1133, 650807: 1134, 348437: 1135, 1688: 1136, 33013: 1137, 38829: 1138, 307015: 1139, 127033: 1140, 310742: 1141, 1694: 1142, 650834: 1143, 1131420: 1144, 193918: 1145, 85324: 1146, 642827: 1147, 38845: 1148, 193931: 1149, 193932: 1150, 4553: 1151, 1116146: 1152, 85352: 1153, 261040: 1154, 145215: 1155, 646412: 1156, 1131464: 1157, 1131466: 1158, 574264: 1159, 458439: 1160, 57764: 1161, 646440: 1162, 1111899: 1163, 521252: 1164, 1115471: 1165, 1123493: 1166, 601462: 1167, 421481: 1168, 385572: 1169, 30934: 1170, 84695: 1171, 189566: 1172, 69397: 1173, 6741: 1174, 177998: 1175, 395725: 1176, 61417: 1177, 54129: 1178, 1118347: 1179, 1106764: 1180, 102406: 1181, 75972: 1182, 95579: 1183, 54132: 1184, 27243: 1185, 1153262: 1186, 1153264: 1187, 30973: 1188, 1129208: 1189, 1106771: 1190, 27249: 1191, 95586: 1192, 95588: 1193, 255233: 1194, 6775: 1195, 129287: 1196, 27250: 1197, 19231: 1198, 1153275: 1199, 1132948: 1200, 1106789: 1201, 95597: 1202, 6784: 1203, 682815: 1204, 1153280: 1205, 148170: 1206, 263279: 1207, 1116922: 1208, 1152564: 1209, 1118388: 1210, 851968: 1211, 3101: 1212, 1129243: 1213, 170798: 1214, 3112: 1215, 503877: 1216, 17821: 1217, 503883: 1218, 561238: 1219, 1110438: 1220, 575795: 1221, 1116974: 1222, 272720: 1223, 415693: 1224, 18582: 1225, 11325: 1226, 11326: 1227, 1103162: 1228, 1111186: 1229, 578645: 1230, 578646: 1231, 578649: 1232, 1121313: 1233, 11335: 1234, 1102442: 1235, 11339: 1236, 52784: 1237, 11342: 1238, 1130080: 1239, 3191: 1240, 3192: 1241, 400455: 1242, 1135899: 1243, 591017: 1244, 751408: 1245, 1140230: 1246, 1140231: 1247, 1106052: 1248, 70970: 1249, 67245: 1250, 67246: 1251, 205196: 1252, 135130: 1253, 123556: 1254, 645084: 1255, 1786: 1256, 66556: 1257, 1130808: 1258, 4649: 1259, 582343: 1260, 395075: 1261, 582349: 1262, 20833: 1263, 1131549: 1264, 58552: 1265, 85449: 1266, 49811: 1267, 77438: 1268, 4660: 1269, 66594: 1270, 66596: 1271, 314459: 1272, 1116268: 1273, 1103960: 1274, 49843: 1275, 1103969: 1276, 593240: 1277, 207395: 1278, 593248: 1279, 943087: 1280, 7532: 1281, 7537: 1282, 25181: 1283, 25184: 1284, 16437: 1285, 1103985: 1286, 6814: 1287, 6818: 1288, 1154042: 1289, 23738: 1290, 1107558: 1291, 137359: 1292, 16451: 1293, 318071: 1294, 232860: 1295, 1107572: 1296, 49895: 1297, 16474: 1298, 1154076: 1299, 626999: 1300, 137380: 1301, 1119178: 1302, 33904: 1303, 1119180: 1304, 33907: 1305, 174418: 1306, 70281: 1307, 73119: 1308, 9716: 1309, 174425: 1310, 416455: 1311, 18615: 1312, 127940: 1313, 1152663: 1314, 675649: 1315, 1117760: 1316, 1138091: 1317, 1152673: 1318, 321004: 1319, 139547: 1320, 45533: 1321, 3217: 1322, 1111240: 1323, 523574: 1324, 1110515: 1325, 73162: 1326, 52835: 1327, 3220: 1328, 3223: 1329, 1129367: 1330, 1129368: 1331, 1129369: 1332, 84021: 1333, 1127913: 1334, 3233: 1335, 3235: 1336, 3236: 1337, 562067: 1338, 3240: 1339, 92065: 1340, 213246: 1341, 911198: 1342, 12158: 1343, 20178: 1344, 20179: 1345, 80491: 1346, 561364: 1347, 20180: 1348, 245955: 1349, 1102548: 1350, 1817: 1351, 31043: 1352, 1102550: 1353, 20193: 1354, 1110579: 1355, 213279: 1356, 1133010: 1357, 157761: 1358, 31055: 1359, 12194: 1360, 1133028: 1361, 578780: 1362, 12198: 1363, 12199: 1364, 90655: 1365, 6130: 1366, 337766: 1367, 112787: 1368, 1133047: 1369, 1105428: 1370, 785678: 1371, 644441: 1372, 672064: 1373, 41216: 1374, 1105433: 1375, 1113459: 1376, 55770: 1377, 6163: 1378, 259701: 1379, 259702: 1380, 1131607: 1381, 430329: 1382, 643734: 1383, 643735: 1384, 656048: 1385, 1131611: 1386, 617575: 1387, 1105450: 1388, 15076: 1389, 10793: 1390, 1117049: 1391, 647315: 1392, 33231: 1393, 1116328: 1394, 1104749: 1395, 594025: 1396, 315266: 1397, 643777: 1398, 1130927: 1399, 1132385: 1400, 1108329: 1401, 1130929: 1402, 1104769: 1403, 594047: 1404, 1130931: 1405, 1130934: 1406, 141868: 1407, 593329: 1408, 144701: 1409, 574462: 1410, 60170: 1411, 120039: 1412, 502574: 1413, 293974: 1414, 1119216: 1415, 1108363: 1416, 191216: 1417, 469504: 1418, 358866: 1419, 1116397: 1420, 191222: 1421, 36145: 1422, 1115677: 1423, 577331: 1424, 31863: 1425, 566488: 1426, 358887: 1427, 6935: 1428, 6939: 1429, 197783: 1430, 34708: 1431, 1107674: 1432, 248119: 1433, 318187: 1434, 1152714: 1435, 1154173: 1436, 300071: 1437, 1120020: 1438, 423816: 1439, 1106966: 1440, 148341: 1441, 136766: 1442, 325497: 1443, 136767: 1444, 136768: 1445, 409255: 1446, 1152740: 1447, 1117833: 1448, 309476: 1449, 1120059: 1450, 80515: 1451, 65212: 1452, 15892: 1453, 1120084: 1454, 576691: 1455, 148399: 1456, 175291: 1457, 1112071: 1458, 117: 1459, 157805: 1460, 300806: 1461, 31105: 1462, 154982: 1463, 141160: 1464, 112813: 1465, 98693: 1466, 98698: 1467, 192734: 1468, 12247: 1469, 1109891: 1470, 141171: 1471, 312409: 1472, 608190: 1473, 608191: 1474, 55801: 1475, 1136791: 1476, 815073: 1477, 1114222: 1478, 173884: 1479, 1102646: 1480, 63832: 1481, 211906: 1482, 83449: 1483, 2654: 1484, 815096: 1485, 277263: 1486, 1105505: 1487, 48550: 1488, 83461: 1489, 48555: 1490, 6238: 1491, 636500: 1492, 340078: 1493, 1113534: 1494, 578898: 1495, 1951: 1496, 1952: 1497, 1956: 1498, 636511: 1499, 463825: 1500, 1121569: 1501, 1105531: 1502, 14428: 1503, 14429: 1504, 74749: 1505, 14430: 1506, 14431: 1507, 1132434: 1508, 648121: 1509, 582511: 1510, 688849: 1511, 1997: 1512, 1131728: 1513, 1106298: 1514, 86359: 1515, 647413: 1516, 1120880: 1517, 1131734: 1518, 562940: 1519, 230879: 1520, 1104851: 1521, 1152075: 1522, 58758: 1523, 230884: 1524, 34082: 1525, 1132486: 1526, 39890: 1527, 66782: 1528, 218410: 1529, 647447: 1530, 1117184: 1531, 66794: 1532, 227178: 1533, 936: 1534, 940: 1535, 575292: 1536, 941: 1537, 1109185: 1538, 85688: 1539, 28202: 1540, 50807: 1541, 379288: 1542, 1154229: 1543, 1109199: 1544, 118682: 1545, 153598: 1546, 1154251: 1547, 62417: 1548, 1125909: 1549, 79809: 1550, 739280: 1551, 70441: 1552, 70442: 1553, 70444: 1554, 79817: 1555, 129558: 1556, 892139: 1557, 576725: 1558, 28254: 1559, 1246: 1560, 237376: 1561, 27531: 1562, 397488: 1563, 42847: 1564, 42848: 1565, 155736: 1566, 155738: 1567, 39124: 1568, 39127: 1569, 39130: 1570, 1153577: 1571, 335733: 1572, 28290: 1573, 18815: 1574, 1136814: 1575, 1120169: 1576, 82087: 1577, 178209: 1578, 139738: 1579, 82090: 1580, 18834: 1581, 39165: 1582, 190698: 1583, 1125992: 1584, 1109957: 1585, 46470: 1586, 46476: 1587, 1129570: 1588, 1071981: 1589, 1129573: 1590, 39199: 1591, 12337: 1592, 29723: 1593, 694759: 1594, 46491: 1595, 1128856: 1596, 1107010: 1597, 643199: 1598, 1104182: 1599, 12347: 1600, 63915: 1601, 519353: 1602, 608292: 1603, 1121603: 1604, 1130356: 1605, 12359: 1606, 192850: 1607, 7032: 1608, 1128881: 1609, 140569: 1610, 1114331: 1611, 7041: 1612, 561581: 1613, 561582: 1614, 192870: 1615, 1113614: 1616, 1102761: 1617, 116528: 1618, 561595: 1619, 94416: 1620, 5600: 1621, 1000012: 1622, 1114364: 1623, 1121659: 1624, 66809: 1625, 6343: 1626, 212777: 1627, 583318: 1628, 709518: 1629, 350319: 1630, 116553: 1631, 170338: 1632, 179706: 1633, 1112929: 1634, 656231: 1635, 14531: 1636, 1106370: 1637, 1109208: 1638, 1114398: 1639, 95188: 1640, 510718: 1641, 208345: 1642, 6378: 1643, 22563: 1644, 10981: 1645, 110041: 1646, 14549: 1647, 95198: 1648, 6385: 1649, 575331: 1650, 568045: 1651, 1136110: 1652, 1131828: 1653, 67584: 1654, 243274: 1655, 135464: 1656, 1105672: 1657, 93755: 1658, 756061: 1659, 522338: 1660, 219239: 1661, 61073: 1662, 262178: 1663, 686015: 1664, 1110024: 1665, 613409: 1666, 686030: 1667, 227286: 1668, 45061: 1669, 646836: 1670, 1108551: 1671, 13885: 1672, 1104999: 1673, 566653: 1674, 1127430: 1675, 299197: 1676, 1135455: 1677, 97377: 1678, 592826: 1679, 566664: 1680, 633030: 1681, 633031: 1682, 686061: 1683, 592830: 1684, 573964: 1685, 1155073: 1686, 17476: 1687, 17477: 1688, 190706: 1689, 28336: 1690, 573978: 1691, 1107861: 1692, 17488: 1693, 1128198: 1694, 1108597: 1695, 103515: 1696, 27623: 1697, 200480: 1698, 103529: 1699, 649730: 1700, 39210: 1701, 46501: 1702, 27632: 1703, 649739: 1704, 1119471: 1705, 103531: 1706, 470511: 1707, 509233: 1708, 236759: 1709, 237489: 1710, 1152944: 1711, 1118764: 1712, 643221: 1713, 212097: 1714, 608326: 1715, 643239: 1716, 1131116: 1717, 202639: 1718, 141324: 1719, 294145: 1720, 1128927: 1721, 561610: 1722, 561611: 1723, 147870: 1724, 248395: 1725, 1128935: 1726, 241133: 1727, 141342: 1728, 141347: 1729, 1128946: 1730, 1131164: 1731, 12439: 1732, 1131167: 1733, 1129683: 1734, 359067: 1735, 117315: 1736, 117316: 1737, 144212: 1738, 1106401: 1739, 1134022: 1740, 13193: 1741, 1131192: 1742, 1107136: 1743, 1131195: 1744, 1128982: 1745, 121792: 1746, 653441: 1747, 385251: 1748, 1126011: 1749, 1134031: 1750, 642593: 1751, 1115166: 1752, 737204: 1753, 118079: 1754, 1122460: 1755, 1114442: 1756, 589923: 1757, 1121739: 1758, 626574: 1759, 1126037: 1760, 645452: 1761, 753264: 1762, 1126044: 1763, 74920: 1764, 74921: 1765, 1105718: 1766, 48764: 1767, 48768: 1768, 1113742: 1769, 74937: 1770, 575402: 1771, 168958: 1772, 78508: 1773, 289085: 1774, 78511: 1775, 308232: 1776, 682508: 1777, 75691: 1778, 75693: 1779, 75694: 1780, 155158: 1781, 1105764: 1782, 1152259: 1783, 579008: 1784, 1128201: 1785, 1133390: 1786, 1118083: 1787, 78549: 1788, 604073: 1789, 595056: 1790, 1118092: 1791, 1125386: 1792, 78552: 1793, 78555: 1794, 78557: 1795, 646900: 1796, 595063: 1797, 648369: 1798, 1128227: 1799, 89416: 1800, 578306: 1801, 683294: 1802, 440815: 1803, 126867: 1804, 126868: 1805, 72056: 1806, 1119505: 1807, 1128256: 1808, 1108656: 1809, 71336: 1810, 1109392: 1811, 40886: 1812, 1115959: 1813, 578347: 1814, 284023: 1815, 345340: 1816, 621555: 1817, 118873: 1818, 8687: 1819, 226698: 1820, 578365: 1821, 1135589: 1822, 8696: 1823, 1118823: 1824, 411005: 1825, 509315: 1826, 171954: 1827, 230300: 1828, 1105011: 1829, 1121057: 1830, 592973: 1831, 592975: 1832, 48066: 1833, 248431: 1834, 1121063: 1835, 592986: 1836, 48075: 1837, 289885: 1838, 592993: 1839, 592996: 1840, 28489: 1841, 590022: 1842, 111676: 1843, 13205: 1844, 13208: 1845, 102938: 1846, 102939: 1847, 416867: 1848, 72805: 1849, 574009: 1850, 294239: 1851, 1131223: 1852, 77108: 1853, 5064: 1854, 5069: 1855, 1131230: 1856, 40125: 1857, 1123215: 1858, 20526: 1859, 20528: 1860, 77112: 1861, 107251: 1862, 107252: 1863, 5075: 1864, 126128: 1865, 734406: 1866, 40131: 1867, 703953: 1868, 40135: 1869, 1131257: 1870, 1123239: 1871, 1129778: 1872, 662250: 1873, 711994: 1874, 273949: 1875, 1131266: 1876, 1130539: 1877, 377303: 1878, 179180: 1879, 1129798: 1880, 1114512: 1881, 1110950: 1882, 12558: 1883, 853114: 1884, 853115: 1885, 853116: 1886, 853118: 1887, 1114526: 1888, 212930: 1889, 206371: 1890, 105856: 1891, 463: 1892, 20592: 1893, 51049: 1894, 20593: 1895, 83746: 1896, 124734: 1897, 106590: 1898, 1133417: 1899, 1125402: 1900, 1153024: 1901, 853155: 1902, 1118120: 1903, 1105810: 1904, 1113831: 1905, 646289: 1906, 1106546: 1907, 31479: 1908, 31483: 1909, 31489: 1910, 94639: 1911, 631015: 1912, 645571: 1913, 1106568: 1914, 430711: 1915, 7296: 1916, 1132731: 1917, 1153064: 1918, 93923: 1919, 1134197: 1920, 87363: 1921, 395540: 1922, 395547: 1923, 50381: 1924, 1129015: 1925, 126909: 1926, 143676: 1927, 395553: 1928, 752684: 1929, 1129021: 1930, 19045: 1931, 631052: 1932, 126912: 1933, 116790: 1934, 5869: 1935, 579108: 1936, 683355: 1937, 1105877: 1938, 59772: 1939, 243483: 1940, 126926: 1941, 155277: 1942, 1128314: 1943, 1105887: 1944, 1110209: 1945, 307656: 1946, 199571: 1947, 1152394: 1948, 60560: 1949, 595193: 1950, 990075: 1951, 119686: 1952, 1154520: 1953, 28504: 1954, 1154524: 1955, 1154525: 1956, 1129096: 1957, 1128369: 1958, 96845: 1959, 380341: 1960, 8766: 1961, 1110256: 1962, 55403: 1963, 389715: 1964, 1153816: 1965, 131317: 1966, 260979: 1967, 264556: 1968, 35852: 1969, 1119671: 1970, 1153853: 1971, 1112417: 1972, 1153860: 1973, 1153861: 1974, 35863: 1975, 1121176: 1976, 1131301: 1977, 1131305: 1978, 1105148: 1979, 134219: 1980, 671052: 1981, 1131312: 1982, 156794: 1983, 1153896: 1984, 1153899: 1985, 167656: 1986, 239829: 1987, 1104435: 1988, 187260: 1989, 231198: 1990, 1131334: 1991, 1131335: 1992, 142268: 1993, 504: 1994, 506: 1995, 228990: 1996, 228992: 1997, 1132073: 1998, 654326: 1999, 1116044: 2000, 1131359: 2001, 643485: 2002, 654339: 2003, 1107319: 2004, 132821: 2005, 360028: 2006, 214472: 2007, 646334: 2008, 653628: 2009, 1107325: 2010, 166989: 2011, 1111788: 2012, 151708: 2013, 118259: 2014, 32260: 2015, 137130: 2016, 92589: 2017, 118260: 2018, 124828: 2019, 141596: 2020, 197452: 2021, 646357: 2022, 1153106: 2023, 30817: 2024, 642798: 2025, 1130676: 2026, 1107355: 2027, 1118209: 2028, 987188: 2029, 87417: 2030, 23545: 2031, 23546: 2032, 1113926: 2033, 94713: 2034, 1107367: 2035, 987197: 2036, 521183: 2037, 1114664: 2038, 69296: 2039, 51180: 2040, 43165: 2041, 1132815: 2042, 1107385: 2043, 100197: 2044, 520471: 2045, 215912: 2046, 61312: 2047, 1129106: 2048, 43186: 2049, 1129111: 2050, 41732: 2051, 22869: 2052, 9513: 2053, 9515: 2054, 119712: 2055, 270456: 2056, 5959: 2057, 576362: 2058, 1153183: 2059, 22874: 2060, 22875: 2061, 22876: 2062, 1124837: 2063, 1132857: 2064, 594511: 2065, 22883: 2066, 238401: 2067, 1136449: 2068, 714208: 2069, 9559: 2070, 135798: 2071, 1152490: 2072, 1109566: 2073, 1103038: 2074, 177115: 2075, 523394: 2076, 1128453: 2077, 1109581: 2078, 101660: 2079, 101662: 2080, 9581: 2081, 9586: 2082, 1135750: 2083, 51934: 2084, 762980: 2085, 1153900: 2086, 593859: 2087, 714256: 2088, 8874: 2089, 25794: 2090, 75121: 2091, 28632: 2092, 1153922: 2093, 1119742: 2094, 63549: 2095, 1138619: 2096, 1102364: 2097, 28640: 2098, 28641: 2099, 409725: 2100, 292277: 2101, 606479: 2102, 1153942: 2103, 1153945: 2104, 1153946: 2105, 709113: 2106, 194609: 2107, 90470: 2108, 820661: 2109, 820662: 2110, 1105231: 2111, 73712: 2112, 54844: 2113, 684972: 2114, 134314: 2115, 735303: 2116, 824245: 2117, 195361: 2118, 529165: 2119, 1131414: 2120, 617378: 2121, 1120563: 2122, 47570: 2123, 684986: 2124, 735311: 2125, 187354: 2126, 1132157: 2127, 58436: 2128, 278403: 2129, 58453: 2130, 58454: 2131, 206524: 2132, 593104: 2133, 133628: 2134, 46887: 2135, 49720: 2136, 1131471: 2137, 643597: 2138, 1107418: 2139, 1129994: 2140, 573535: 2141, 814836: 2142, 1119004: 2143, 1134320: 2144, 1116181: 2145, 1108167: 2146, 1108169: 2147, 49753: 2148, 57773: 2149, 7430: 2150, 521251: 2151, 593155: 2152, 642894: 2153, 1126315: 2154, 1108175: 2155, 1059953: 2156, 521269: 2157, 1118302: 2158, 1130780: 2159, 1134346: 2160, 1134348: 2161, 1135082: 2162, 899085: 2163, 124952: 2164, 240791: 2165, 189571: 2166, 189572: 2167, 1126350: 2168, 189574: 2169, 177993: 2170, 27230: 2171, 1119078: 2172, 128540: 2173, 308529: 2174, 54131: 2175, 75983: 2176, 15670: 2177, 33818: 2178, 95594: 2179, 6782: 2180, 33823: 2181, 25805: 2182, 1153287: 2183, 596075: 2184, 817774: 2185, 18532: 2186, 18536: 2187, 235670: 2188, 235678: 2189, 235679: 2190, 739707: 2191, 17811: 2192, 503871: 2193, 235683: 2194, 1128531: 2195, 594649: 2196, 1128536: 2197, 1102400: 2198, 593921: 2199, 486840: 2200, 1127810: 2201, 503893: 2202, 399370: 2203, 387795: 2204, 220420: 2205, 593942: 2206, 8961: 2207, 645016: 2208, 481073: 2209, 11337: 2210, 578650: 2211, 1130069: 2212, 1127851: 2213, 124224: 2214, 37483: 2215, 578669: 2216, 1127863: 2217, 1135894: 2218, 645046: 2219, 22229: 2220, 149669: 2221, 365294: 2222, 169279: 2223, 1138755: 2224, 323128: 2225, 22241: 2226, 156977: 2227, 763010: 2228, 1120650: 2229, 1105344: 2230, 59244: 2231, 286500: 2232, 567005: 2233, 644361: 2234, 644363: 2235, 154023: 2236, 286513: 2237, 459206: 2238, 671269: 2239, 1105360: 2240, 1112650: 2241, 632796: 2242, 47682: 2243, 47683: 2244, 47684: 2245, 4637: 2246, 642920: 2247, 634902: 2248, 459213: 2249, 459214: 2250, 634904: 2251, 459216: 2252, 20821: 2253, 178718: 2254, 1108209: 2255, 1112665: 2256, 1104647: 2257, 1140289: 2258, 66563: 2259, 67292: 2260, 66564: 2261, 154047: 2262, 642930: 2263, 654519: 2264, 178727: 2265, 1135108: 2266, 593201: 2267, 162075: 2268, 593209: 2269, 107569: 2270, 1123530: 2271, 1135115: 2272, 1132285: 2273, 1131557: 2274, 162080: 2275, 3932: 2276, 593210: 2277, 118424: 2278, 1135122: 2279, 634938: 2280, 1131565: 2281, 20857: 2282, 118435: 2283, 118436: 2284, 643695: 2285, 1130847: 2286, 1111978: 2287, 1154012: 2288, 1108258: 2289, 49844: 2290, 49847: 2291, 189620: 2292, 189623: 2293, 1108267: 2294, 1050679: 2295, 634975: 2296, 1114838: 2297, 577227: 2298, 28026: 2299, 601561: 2300, 24476: 2301, 1026: 2302, 95642: 2303, 270600: 2304, 145384: 2305, 16461: 2306, 35335: 2307, 1138027: 2308, 1035: 2309, 1114864: 2310, 1154068: 2311, 449841: 2312, 1154071: 2313, 1106854: 2314, 210309: 2315, 801170: 2316, 251756: 2317, 645870: 2318, 144679: 2319, 1138043: 2320, 86923: 2321, 342802: 2322, 1152633: 2323, 711527: 2324, 684372: 2325, 216878: 2326, 62274: 2327, 72406: 2328, 101811: 2329, 246618: 2330, 1136631: 2331, 1152676: 2332, 235776: 2333, 57119: 2334, 119956: 2335, 948147: 2336, 739816: 2337, 3222: 2338, 1117786: 2339, 1110520: 2340, 36802: 2341, 3232: 2342, 3237: 2343, 1111265: 2344, 695284: 2345, 37541: 2346, 1110546: 2347, 71736: 2348, 1135955: 2349, 12155: 2350, 258259: 2351, 1114118: 2352, 606647: 2353, 12165: 2354, 1110563: 2355, 12169: 2356, 1133004: 2357, 1133008: 2358, 1102567: 2359, 12195: 2360, 28851: 2361, 644427: 2362, 1113438: 2363, 1121459: 2364, 689439: 2365, 633585: 2366, 31083: 2367, 6152: 2368, 1119987: 2369, 1114184: 2370, 82664: 2371, 82666: 2372, 672070: 2373, 672071: 2374, 632874: 2375, 1114192: 2376, 644470: 2377, 5462: 2378, 594011: 2379, 20924: 2380, 1131634: 2381, 1120786: 2382, 1112767: 2383, 180301: 2384, 160705: 2385, 628458: 2386, 628459: 2387, 1130915: 2388, 1116336: 2389, 390889: 2390, 57922: 2391, 594039: 2392, 13654: 2393, 57932: 2394, 73972: 2395, 198443: 2396, 60159: 2397, 101143: 2398, 101145: 2399, 763181: 2400, 44121: 2401, 593328: 2402, 259772: 2403, 189708: 2404, 60169: 2405, 24530: 2406, 467383: 2407, 20972: 2408, 13686: 2409, 152731: 2410, 118558: 2411, 118559: 2412, 1154123: 2413, 1154124: 2414, 1126503: 2415, 40583: 2416, 95719: 2417, 693143: 2418, 36131: 2419, 1123689: 2420, 6913: 2421, 256106: 2422, 36140: 2423, 1115670: 2424, 1108389: 2425, 6923: 2426, 6925: 2427, 36162: 2428, 62329: 2429, 36167: 2430, 6941: 2431, 245288: 2432, 62333: 2433, 189774: 2434, 1133846: 2435, 167205: 2436, 62347: 2437, 267003: 2438, 1114992: 2439, 1112026: 2440, 1119295: 2441, 1111304: 2442, 964248: 2443, 45603: 2444, 1109830: 2445, 1152761: 2446, 62389: 2447, 444191: 2448, 263482: 2449, 263486: 2450, 263498: 2451, 675756: 2452, 1125895: 2453, 627024: 2454, 12211: 2455, 643069: 2456, 1112075: 2457, 884094: 2458, 120817: 2459, 1110628: 2460, 18770: 2461, 18773: 2462, 173863: 2463, 1130243: 2464, 1102625: 2465, 63812: 2466, 18781: 2467, 18785: 2468, 1129494: 2469, 578845: 2470, 68115: 2471, 293271: 2472, 63835: 2473, 1919: 2474, 164: 2475, 293285: 2476, 12275: 2477, 1103383: 2478, 1114239: 2479, 6215: 2480, 288107: 2481, 385067: 2482, 1121537: 2483, 1103394: 2484, 6224: 2485, 2663: 2486, 104840: 2487, 632935: 2488, 1106236: 2489, 375605: 2490, 1132406: 2491, 28964: 2492, 308003: 2493, 47839: 2494, 753070: 2495, 563613: 2496, 1132416: 2497, 2695: 2498, 2696: 2499, 2698: 2500, 1105530: 2501, 1113551: 2502, 688824: 2503, 1138968: 2504, 1120858: 2505, 40605: 2506, 1132443: 2507, 1999: 2508, 33325: 2509, 644577: 2510, 66751: 2511, 594119: 2512, 1132461: 2513, 1115701: 2514, 1131741: 2515, 270085: 2516, 1136040: 2517, 1131752: 2518, 1131754: 2519, 4878: 2520, 1123756: 2521, 1135345: 2522, 1107728: 2523, 1154232: 2524, 1154233: 2525, 17363: 2526, 1213: 2527, 149139: 2528, 28230: 2529, 50838: 2530, 1125906: 2531, 32698: 2532, 754594: 2533, 1133930: 2534, 1115790: 2535, 28249: 2536, 1237: 2537, 684531: 2538, 238099: 2539, 131042: 2540, 444240: 2541, 1112106: 2542, 27535: 2543, 28267: 2544, 1120138: 2545, 1117920: 2546, 1125944: 2547, 1118658: 2548, 263553: 2549, 1125953: 2550, 114308: 2551, 630817: 2552, 687401: 2553, 594900: 2554, 10174: 2555, 73323: 2556, 46431: 2557, 202520: 2558, 15987: 2559, 10186: 2560, 294030: 2561, 675847: 2562, 190697: 2563, 576795: 2564, 1125993: 2565, 519318: 2566, 1120197: 2567, 1152896: 2568, 1122304: 2569, 2702: 2570, 1129572: 2571, 1112194: 2572, 29738: 2573, 1128868: 2574, 633721: 2575, 630890: 2576, 1123068: 2577, 561568: 2578, 733534: 2579, 1102751: 2580, 1114336: 2581, 1123087: 2582, 6311: 2583, 116512: 2584, 6318: 2585, 7047: 2586, 1123093: 2587, 1103499: 2588, 151430: 2589, 431206: 2590, 372862: 2591, 561593: 2592, 1106330: 2593, 1105603: 2594, 1132505: 2595, 74821: 2596, 6344: 2597, 116545: 2598, 733576: 2599, 1112911: 2600, 1105622: 2601, 1102794: 2602, 262108: 2603, 116552: 2604, 41417: 2605, 1140543: 2606, 14529: 2607, 1117219: 2608, 1107095: 2609, 1140548: 2610, 523010: 2611, 42156: 2612, 262121: 2613, 22564: 2614, 14545: 2615, 22566: 2616, 1106388: 2617, 429781: 2618, 335042: 2619, 219218: 2620, 610529: 2621, 250566: 2622, 1104946: 2623, 195792: 2624, 1152179: 2625, 89308: 2626, 350373: 2627, 628667: 2628, 628668: 2629, 102061: 2630, 430574: 2631, 1107808: 2632, 1110028: 2633, 45052: 2634, 89335: 2635, 252715: 2636, 4983: 2637, 646837: 2638, 1139009: 2639, 252725: 2640, 593544: 2641, 299195: 2642, 593559: 2643, 1108570: 2644, 272345: 2645, 593560: 2646, 70520: 2647, 131122: 2648, 8591: 2649, 217852: 2650, 264347: 2651, 7867: 2652, 27612: 2653, 1152917: 2654, 28359: 2655, 103528: 2656, 46500: 2657, 27631: 2658, 289779: 2659, 103537: 2660, 633081: 2661, 255628: 2662, 397590: 2663, 1129610: 2664, 50980: 2665, 28385: 2666, 427606: 2667, 616336: 2668, 1120252: 2669, 1152958: 2670, 1152959: 2671, 1385: 2672, 254923: 2673, 34961: 2674, 46547: 2675, 13136: 2676, 1131137: 2677, 233106: 2678, 561613: 2679, 1131149: 2680, 1104258: 2681, 1152991: 2682, 447250: 2683, 115188: 2684, 102879: 2685, 1131150: 2686, 56708: 2687, 1128943: 2688, 134060: 2689, 102884: 2690, 1131163: 2691, 4274: 2692, 1131172: 2693, 767763: 2694, 152226: 2695, 152227: 2696, 626530: 2697, 626531: 2698, 1131180: 2699, 1130454: 2700, 1131184: 2701, 1128974: 2702, 1128975: 2703, 1128977: 2704, 1128978: 2705, 117328: 2706, 24043: 2707}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = csr_matrix(paper_features_label[:, 1:-1], dtype=np.float32)\n",
        "print(np.shape(features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdUwiPwRqx02",
        "outputId": "c4550648-e7c4-4ae1-b6f2-783644655647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2708, 1433)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2708代表論文數量，1433代表有1433個關鍵字，在每篇論文中的出現情況，出現為1，沒出現為0。"
      ],
      "metadata": {
        "id": "IdlBeGsDL0Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = paper_features_label[:, -1]\n",
        "lbl2idx = {k:v for v,k in enumerate(sorted(np.unique(labels)))}\n",
        "labels = [lbl2idx[e] for e in labels]\n",
        "print(lbl2idx,labels[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SjxI4xCq8HC",
        "outputId": "aec57e62-6b0e-4946-fc88-ac789a293c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Case_Based': 0, 'Genetic_Algorithms': 1, 'Neural_Networks': 2, 'Probabilistic_Methods': 3, 'Reinforcement_Learning': 4, 'Rule_Learning': 5, 'Theory': 6} [2, 5, 4, 4, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 讀取並解析關係數據"
      ],
      "metadata": {
        "id": "fX4p_8q6ruY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "將每篇論文當作一個頂點，論文間的引用關係作為邊，這樣論文的關係數據，就可以用一個圖結構來表示。"
      ],
      "metadata": {
        "id": "z7ShPGuNMuCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我們要計算該圖結構的鄰接矩陣，並將其轉為無向圖的鄰接矩陣。"
      ],
      "metadata": {
        "id": "2uvvbw7INCwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "數據集中的論文引用關係數組:"
      ],
      "metadata": {
        "id": "hzntirrMOApe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edges = np.genfromtxt(path/'cora.cites', dtype=np.int32)\n",
        "print(edges,np.shape(edges))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCmuo27Xq-0A",
        "outputId": "c556c1bb-08bc-40a1-dfec-7039d445a21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[     35    1033]\n",
            " [     35  103482]\n",
            " [     35  103515]\n",
            " ...\n",
            " [ 853118 1140289]\n",
            " [ 853155  853118]\n",
            " [ 954315 1155073]] (5429, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "將論文ID換為重新編號後的引用關係數組:"
      ],
      "metadata": {
        "id": "GLhM7VHMN16y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edges = np.asarray([paper2idx[e] for e in edges.flatten()], np.int32).reshape(edges.shape)\n",
        "print(edges,edges.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0HIreOwr9Dm",
        "outputId": "ac2fe90d-592c-4aff-d962-b499b29bcd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 163  402]\n",
            " [ 163  659]\n",
            " [ 163 1696]\n",
            " ...\n",
            " [1887 2258]\n",
            " [1902 1887]\n",
            " [ 837 1686]] (5429, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "針對由論文引用關係所表示的圖結構，生成鄰接矩陣:"
      ],
      "metadata": {
        "id": "OuxnL1xNOTC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adj = coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                 shape=(len(labels), len(labels)), dtype=np.float32)\n",
        "adj\n",
        "# Symmetric adjacency matrix\n",
        "#adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iikgx5dMsBss",
        "outputId": "e3521bb5-0048-4aee-9787-b4d5612d2eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2708x2708 sparse matrix of type '<class 'numpy.float32'>'\n",
              "\twith 5429 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "將有向圖的鄰接矩陣，轉為無向圖的鄰接矩陣:"
      ],
      "metadata": {
        "id": "0Kojbf-bOfIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adj_long = adj.multiply(adj.T < adj)\n",
        "adj = adj_long+adj_long.T\n",
        "adj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3fafe_JsEcx",
        "outputId": "cbcd27ce-2785-4cf9-c17e-88acbec5bcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2708x2708 sparse matrix of type '<class 'numpy.float32'>'\n",
              "\twith 10254 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: \n",
        "* 因為任務是對論文進行分類，所以論文間的引用關係，為模型提供了單個論文特徵之間的關聯。\n",
        "* 因為在模型處理過程中，更看重的是論文之間有沒有聯繫，所以要用無向圖表示"
      ],
      "metadata": {
        "id": "hZd62cFeOuBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 加工圖結構"
      ],
      "metadata": {
        "id": "xrsbTE4RsSKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "對圖結構的矩陣進行加工，使其更好地表現出圖結構的特徵，並參與神經網絡的模型計算。"
      ],
      "metadata": {
        "id": "jUeaDtnRt2rR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "實現對鄰接矩陣進行，對稱正規化拉普拉斯矩陣的轉化。"
      ],
      "metadata": {
        "id": "7dWTSXjPsiVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(mx):\n",
        "    '''Row-normalize sparse matrix'''\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = (rowsum ** -1).flatten() \n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "features = normalize(features)\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = diags(r_inv)\n",
        "    return mx.dot(r_mat_inv).transpose().dot(r_mat_inv)\n",
        "\n",
        "#adj = normalize(adj + eye(adj.shape[0]))\n",
        "adj = normalize_adj(adj + eye(adj.shape[0]))"
      ],
      "metadata": {
        "id": "WpKnPzqMsHvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 將數據轉為張量"
      ],
      "metadata": {
        "id": "5R3scQYYtGPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "將加工好的圖結構矩陣數據，轉為PyTorch支持的張量類型。"
      ],
      "metadata": {
        "id": "YWhcfhhLP_ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "將數據集分為訓練、驗證，以及測試，三個部分。"
      ],
      "metadata": {
        "id": "SjaQUJWpQGS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data as tensors\n",
        "adj = torch.FloatTensor(adj.todense()) \n",
        "features = torch.FloatTensor(features.todense())\n",
        "labels = torch.LongTensor(labels) \n",
        "\n",
        "n_train = 200\n",
        "n_val = 300\n",
        "n_test = len(features) - n_train - n_val\n",
        "np.random.seed(34)\n",
        "idxs = np.random.permutation(len(features))\n",
        "\n",
        "idx_train = torch.LongTensor(idxs[:n_train])\n",
        "idx_val   = torch.LongTensor(idxs[n_train:n_train+n_val])\n",
        "idx_test  = torch.LongTensor(idxs[n_train+n_val:])\n",
        "\n",
        "adj = adj.to(device)\n",
        "features = features.to(device)\n",
        "labels = labels.to(device)\n",
        "idx_train = idx_train.to(device)\n",
        "idx_val = idx_val.to(device)\n",
        "idx_test = idx_test.to(device)"
      ],
      "metadata": {
        "id": "tkPfZv0ts8LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_labels = labels.max().item() + 1  \n",
        "n_features = features.shape[1]   \n",
        "print(n_labels, n_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIcAGPmxt6_l",
        "outputId": "9c14c813-6287-4755-e299-13dc774e60a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 1433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mish啟動函數"
      ],
      "metadata": {
        "id": "SLQ588trt_DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mish(x):\t\t\t\t\t\n",
        "    return x *( torch.tanh(F.softplus(x)))"
      ],
      "metadata": {
        "id": "ouurxn44t9eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定義圖注意力網絡"
      ],
      "metadata": {
        "id": "v_SfHA16uOlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttentionLayer(nn.Module): # 定義圖注意力層\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout=0.6):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features # 輸入特徵維度\n",
        "        self.out_features = out_features # 輸出特徵維度\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W) # 初始化全連接權重\n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1))) \n",
        "        nn.init.xavier_uniform_(self.a) # 初始化注意力權重\n",
        "\n",
        "    def forward(self, input, adj): # 正向傳播\n",
        "        h = torch.mm(input, self.W) # 全連接處理\n",
        "        N = h.size()[0]\n",
        "\n",
        "        # 將全連接後的特徵數據，分別進行基於批次維度和特徵維度的複製，並將複製結果連接在一起\n",
        "        # 這使得頂點中的特徵數據，進行了充分的排列組合，結果中的每列信息，都包含兩個頂點特徵\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features) \n",
        "        e = mish(torch.matmul(a_input, self.a).squeeze(2))# 計算注意力，[N,N]\n",
        "        \n",
        "        \n",
        "        \n",
        "        zero_vec = -9e15*torch.ones_like(e) # 初始化最小值\n",
        "        attention = torch.where(adj > 0, e, zero_vec) # 過濾注意力，如果對過濾掉的特徵值賦為0，則模型會無法收斂\n",
        "        # print(attention )\n",
        "        attention = F.softmax(attention, dim=1) # 對注意力分數，進行正規化，得到注意力分數\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training) # 使用注意力，處理特徵\n",
        "        h_prime = torch.matmul(attention, h)\n",
        "        return mish(h_prime)"
      ],
      "metadata": {
        "id": "Mc5mHgGRuC2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定義GAT"
      ],
      "metadata": {
        "id": "opMQtALyxzYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self,nfeat,  nclass,nhid, dropout,  nheads):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        # 注意力層\n",
        "        self.attentions = [GraphAttentionLayer(nfeat, nhid,dropout) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions): # 添加到模型中\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "        # 輸出層\n",
        "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout)\n",
        "\n",
        "    def forward(self, x, adj): # 正向傳播\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1) # 依次調用注意力層，將結果連接起來\n",
        "        return self.out_att(x, adj)"
      ],
      "metadata": {
        "id": "5eNlBQYjxuxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定義相關函數"
      ],
      "metadata": {
        "id": "AokmGmyMU7eC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意，在GAT中，無論是用模型進行預測還是訓練，都需要將全部的圖結構矩陣輸入。"
      ],
      "metadata": {
        "id": "lxZPM8IEVs9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output,y):\n",
        "    return (output.argmax(1) == y).type(torch.float32).mean().item()\n",
        "\n",
        "def step():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss = F.cross_entropy(output[idx_train], labels[idx_train])\n",
        "    acc = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item(), acc\n",
        "\n",
        "def evaluate(idx):\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss = F.cross_entropy(output[idx], labels[idx]).item()\n",
        "    return loss, accuracy(output[idx], labels[idx])"
      ],
      "metadata": {
        "id": "QpeMDk6Nx4Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定義Ranger優化器"
      ],
      "metadata": {
        "id": "uwEa-3LJWBbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import itertools as it\n",
        "\n",
        "\n",
        "\n",
        "class Ranger(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5, betas=(.95,0.999), eps=1e-5, weight_decay=0):\n",
        "        #parameter checks\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        if not lr > 0:\n",
        "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
        "        if not eps > 0:\n",
        "            raise ValueError(f'Invalid eps: {eps}')\n",
        "\n",
        "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas, N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params,defaults)\n",
        "\n",
        "        self.N_sma_threshhold = N_sma_threshhold\n",
        "\n",
        "        #for group in self.param_groups:\n",
        "        #    group[\"step_counter\"] = 0\n",
        "            #print(\"group step counter init\")\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.k = k \n",
        "\n",
        "        self.radam_buffer = [[None,None,None] for ind in range(10)]\n",
        "\n",
        "        #self.first_run_check=0\n",
        "\n",
        "        #self.slow_weights = [[p.clone().detach() for p in group['params']]\n",
        "        #                     for group in self.param_groups]\n",
        "\n",
        "        #for w in it.chain(*self.slow_weights):\n",
        "        #    w.requires_grad = False\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        print(\"set state called\")\n",
        "        super(Ranger, self).__setstate__(state)\n",
        "\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "\n",
        "        #if closure is not None:\n",
        "            #loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Ranger optimizer does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]  #get state dict for this param\n",
        "\n",
        "                if len(state) == 0:   #if first time to run...init dictionary with our desired entries\n",
        "                    #if self.first_run_check==0:\n",
        "                        #self.first_run_check=1\n",
        "                        #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "\n",
        "                    #look ahead weight storage now in state dict \n",
        "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
        "                    state['slow_buffer'].copy_(p.data)\n",
        "\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                #begin computations \n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                #compute variance mov avg\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                #compute mean moving avg\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "\n",
        "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > self.N_sma_threshhold:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                if N_sma > self.N_sma_threshhold:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "                if state['step'] % group['k'] == 0:\n",
        "                    slow_p = state['slow_buffer'] #get access to slow param tensor\n",
        "                    slow_p.add_(self.alpha, p.data - slow_p)  #(fast weights - slow weights) * alpha\n",
        "                    p.data.copy_(slow_p)  #copy interpolated weights to RAdam param tensor\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "q3O854p7V3W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練模型"
      ],
      "metadata": {
        "id": "hAmjTH8GzWKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GAT(n_features, n_labels, \n",
        "            16, # 輸出維度\n",
        "            0.1, # 丟棄率\n",
        "            8 # 注意力的計算套數\n",
        "            ).to(device)\n",
        "\n",
        "# 如果模型占用的顯存太大，則可以進行縮小:\n",
        "# model = GAT(n_features, n_labels, \n",
        "#             8, # 輸出維度\n",
        "#             0.1, # 丟棄率\n",
        "#             4 # 注意力的計算套數\n",
        "#             ).to(device)"
      ],
      "metadata": {
        "id": "BWBvV2vOzN4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Ranger(model.parameters())\n",
        "\n",
        "\n",
        "from tqdm import tqdm  #pip install tqdm\n",
        "\n",
        "epochs = 3000\n",
        "\n",
        "print_steps = 50\n",
        "train_loss, train_acc = [], []\n",
        "val_loss, val_acc = [], []\n",
        "for i in tqdm(range(epochs)):\n",
        "    tl, ta = step()\n",
        "    train_loss += [tl]\n",
        "    train_acc += [ta]\n",
        "    if (i+1)%print_steps == 0 or i == 0:\n",
        "        tl, ta = evaluate(idx_train)\n",
        "        vl, va = evaluate(idx_val)\n",
        "        val_loss += [vl]\n",
        "        val_acc += [va]\n",
        "        print(f'{i+1:6d}/{epochs}: train_loss={tl:.4f}, train_acc={ta:.4f}'+\n",
        "              f', val_loss={vl:.4f}, val_acc={va:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym3piYC30t4i",
        "outputId": "2256b9c8-1a1b-478e-ab42-ccb0c3262878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:107: UserWarning: This overload of addcmul_ is deprecated:\n",
            "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  0%|          | 1/3000 [00:03<2:53:57,  3.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     1/3000: train_loss=1.9460, train_acc=0.1150, val_loss=1.9461, val_acc=0.0833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 50/3000 [00:24<27:29,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    50/3000: train_loss=1.9446, train_acc=0.2250, val_loss=1.9449, val_acc=0.1733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 100/3000 [00:46<27:11,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   100/3000: train_loss=1.9408, train_acc=0.5550, val_loss=1.9420, val_acc=0.5100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 150/3000 [01:07<27:01,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   150/3000: train_loss=1.9358, train_acc=0.6200, val_loss=1.9381, val_acc=0.5667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 200/3000 [01:29<26:46,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   200/3000: train_loss=1.9276, train_acc=0.6650, val_loss=1.9318, val_acc=0.5800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 250/3000 [01:51<26:05,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   250/3000: train_loss=1.9157, train_acc=0.6500, val_loss=1.9225, val_acc=0.5800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 300/3000 [02:13<25:35,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   300/3000: train_loss=1.9007, train_acc=0.6400, val_loss=1.9107, val_acc=0.5867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 350/3000 [02:35<25:07,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   350/3000: train_loss=1.8767, train_acc=0.6250, val_loss=1.8917, val_acc=0.5733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 400/3000 [02:57<24:38,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   400/3000: train_loss=1.8423, train_acc=0.6200, val_loss=1.8646, val_acc=0.5633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 450/3000 [03:19<24:10,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   450/3000: train_loss=1.8005, train_acc=0.6200, val_loss=1.8318, val_acc=0.5633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 500/3000 [03:41<23:43,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   500/3000: train_loss=1.7366, train_acc=0.6250, val_loss=1.7824, val_acc=0.5700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 550/3000 [04:03<23:14,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   550/3000: train_loss=1.6520, train_acc=0.6500, val_loss=1.7181, val_acc=0.5900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 600/3000 [04:25<22:47,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   600/3000: train_loss=1.5592, train_acc=0.7000, val_loss=1.6492, val_acc=0.6167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 650/3000 [04:46<22:17,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   650/3000: train_loss=1.4373, train_acc=0.7400, val_loss=1.5608, val_acc=0.6367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 700/3000 [05:08<21:48,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   700/3000: train_loss=1.3033, train_acc=0.7800, val_loss=1.4660, val_acc=0.6733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 750/3000 [05:30<21:20,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   750/3000: train_loss=1.1810, train_acc=0.8050, val_loss=1.3808, val_acc=0.6867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 800/3000 [05:52<20:53,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   800/3000: train_loss=1.0447, train_acc=0.8450, val_loss=1.2850, val_acc=0.7100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 850/3000 [06:14<20:24,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   850/3000: train_loss=0.9146, train_acc=0.8750, val_loss=1.1920, val_acc=0.7433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 900/3000 [06:36<19:54,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   900/3000: train_loss=0.8062, train_acc=0.9200, val_loss=1.1132, val_acc=0.7667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 950/3000 [06:58<19:24,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   950/3000: train_loss=0.6937, train_acc=0.9350, val_loss=1.0303, val_acc=0.7867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 1000/3000 [07:20<18:56,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1000/3000: train_loss=0.5932, train_acc=0.9400, val_loss=0.9560, val_acc=0.7933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 1050/3000 [07:42<18:29,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1050/3000: train_loss=0.5144, train_acc=0.9550, val_loss=0.8979, val_acc=0.8033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 1100/3000 [08:03<18:01,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1100/3000: train_loss=0.4370, train_acc=0.9650, val_loss=0.8423, val_acc=0.8100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 1150/3000 [08:25<17:32,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1150/3000: train_loss=0.3723, train_acc=0.9700, val_loss=0.7957, val_acc=0.8200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 1200/3000 [08:47<17:04,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1200/3000: train_loss=0.3242, train_acc=0.9750, val_loss=0.7612, val_acc=0.8300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 1250/3000 [09:09<16:35,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1250/3000: train_loss=0.2780, train_acc=0.9750, val_loss=0.7292, val_acc=0.8300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 1300/3000 [09:31<16:06,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1300/3000: train_loss=0.2391, train_acc=0.9800, val_loss=0.7035, val_acc=0.8267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 1350/3000 [09:53<15:38,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1350/3000: train_loss=0.2103, train_acc=0.9800, val_loss=0.6880, val_acc=0.8233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 1400/3000 [10:15<15:09,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1400/3000: train_loss=0.1819, train_acc=0.9800, val_loss=0.6733, val_acc=0.8200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 1450/3000 [10:37<14:43,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1450/3000: train_loss=0.1576, train_acc=0.9800, val_loss=0.6604, val_acc=0.8200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1500/3000 [10:58<14:14,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1500/3000: train_loss=0.1392, train_acc=0.9850, val_loss=0.6527, val_acc=0.8167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 1550/3000 [11:20<13:45,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1550/3000: train_loss=0.1218, train_acc=0.9900, val_loss=0.6476, val_acc=0.8100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 1600/3000 [11:42<13:16,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1600/3000: train_loss=0.1066, train_acc=0.9950, val_loss=0.6413, val_acc=0.8033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 1650/3000 [12:04<12:48,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1650/3000: train_loss=0.0951, train_acc=0.9950, val_loss=0.6362, val_acc=0.8067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 1700/3000 [12:26<12:18,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1700/3000: train_loss=0.0836, train_acc=0.9950, val_loss=0.6320, val_acc=0.8067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 1750/3000 [12:48<11:51,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1750/3000: train_loss=0.0738, train_acc=1.0000, val_loss=0.6305, val_acc=0.8033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 1800/3000 [13:10<11:22,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1800/3000: train_loss=0.0662, train_acc=1.0000, val_loss=0.6294, val_acc=0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 1850/3000 [13:32<10:55,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1850/3000: train_loss=0.0586, train_acc=1.0000, val_loss=0.6269, val_acc=0.8033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 1900/3000 [13:53<10:25,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1900/3000: train_loss=0.0521, train_acc=1.0000, val_loss=0.6267, val_acc=0.8033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1950/3000 [14:15<09:57,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1950/3000: train_loss=0.0471, train_acc=1.0000, val_loss=0.6291, val_acc=0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 2000/3000 [14:37<09:28,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2000/3000: train_loss=0.0421, train_acc=1.0000, val_loss=0.6312, val_acc=0.7967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 2050/3000 [14:59<09:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2050/3000: train_loss=0.0377, train_acc=1.0000, val_loss=0.6334, val_acc=0.7933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 2100/3000 [15:21<08:31,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2100/3000: train_loss=0.0344, train_acc=1.0000, val_loss=0.6371, val_acc=0.7933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 2150/3000 [15:43<08:02,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2150/3000: train_loss=0.0313, train_acc=1.0000, val_loss=0.6401, val_acc=0.7933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 2200/3000 [16:05<07:35,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2200/3000: train_loss=0.0283, train_acc=1.0000, val_loss=0.6419, val_acc=0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 2250/3000 [16:27<07:06,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2250/3000: train_loss=0.0261, train_acc=1.0000, val_loss=0.6452, val_acc=0.7967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 2300/3000 [16:48<06:38,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2300/3000: train_loss=0.0236, train_acc=1.0000, val_loss=0.6476, val_acc=0.7967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 2350/3000 [17:10<06:09,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2350/3000: train_loss=0.0215, train_acc=1.0000, val_loss=0.6516, val_acc=0.7967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 2400/3000 [17:32<05:41,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2400/3000: train_loss=0.0200, train_acc=1.0000, val_loss=0.6575, val_acc=0.7967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 2450/3000 [17:54<05:12,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2450/3000: train_loss=0.0184, train_acc=1.0000, val_loss=0.6619, val_acc=0.7967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 2500/3000 [18:16<04:44,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2500/3000: train_loss=0.0168, train_acc=1.0000, val_loss=0.6641, val_acc=0.7967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 2550/3000 [18:38<04:16,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2550/3000: train_loss=0.0155, train_acc=1.0000, val_loss=0.6678, val_acc=0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 2600/3000 [19:00<03:47,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2600/3000: train_loss=0.0142, train_acc=1.0000, val_loss=0.6731, val_acc=0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 2650/3000 [19:22<03:18,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2650/3000: train_loss=0.0131, train_acc=1.0000, val_loss=0.6764, val_acc=0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 2700/3000 [19:43<02:50,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2700/3000: train_loss=0.0122, train_acc=1.0000, val_loss=0.6791, val_acc=0.8033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 2750/3000 [20:05<02:22,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2750/3000: train_loss=0.0113, train_acc=1.0000, val_loss=0.6844, val_acc=0.8033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 2800/3000 [20:27<01:53,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2800/3000: train_loss=0.0104, train_acc=1.0000, val_loss=0.6902, val_acc=0.8067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 2850/3000 [20:49<01:25,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2850/3000: train_loss=0.0098, train_acc=1.0000, val_loss=0.6958, val_acc=0.8033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 2900/3000 [21:11<00:56,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2900/3000: train_loss=0.0090, train_acc=1.0000, val_loss=0.7001, val_acc=0.8033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 2950/3000 [21:33<00:28,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2950/3000: train_loss=0.0083, train_acc=1.0000, val_loss=0.7051, val_acc=0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [21:55<00:00,  2.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  3000/3000: train_loss=0.0078, train_acc=1.0000, val_loss=0.7104, val_acc=0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#输出最终结果\n",
        "final_train, final_val, final_test = evaluate(idx_train), evaluate(idx_val), evaluate(idx_test)\n",
        "print(f'Train     : loss={final_train[0]:.4f}, accuracy={final_train[1]:.4f}')\n",
        "print(f'Validation: loss={final_val[0]:.4f}, accuracy={final_val[1]:.4f}')\n",
        "print(f'Test      : loss={final_test[0]:.4f}, accuracy={final_test[1]:.4f}')"
      ],
      "metadata": {
        "id": "UqgkrQEA0zFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb6f239-47b1-44b4-b0b3-ab018e9ff4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train     : loss=0.0078, accuracy=1.0000\n",
            "Validation: loss=0.7104, accuracy=0.8000\n",
            "Test      : loss=0.7758, accuracy=0.7677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
        "ax = axes[0]\n",
        "axes[0].plot(train_loss[::print_steps] + [train_loss[-1]], label='Train')\n",
        "axes[0].plot(val_loss, label='Validation')\n",
        "axes[1].plot(train_acc[::print_steps] + [train_acc[-1]], label='Train')\n",
        "axes[1].plot(val_acc, label='Validation')\n",
        "for ax,t in zip(axes, ['Loss', 'Accuracy']): ax.legend(), ax.set_title(t, size=15)"
      ],
      "metadata": {
        "id": "yNxf884h1KLf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "1af0eb5a-8737-4c80-b5cb-4ce342c45d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAFBCAYAAAA2U032AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3iUVdrH8e9JDymQSoAQQg29BpCigIoi2FAs2MC6uuva9dUtdnfXXdd13XXtgpViQxEUsYB0Cb33AKGFhBICpJ/3j2eAEBJIwmQm5fe5rrnIPOc8Z+6ZPGHmntOMtRYRERERERGp/ny8HYCIiIiIiIiUjxI4ERERERGRGkIJnIiIiIiISA2hBE5ERERERKSGUAInIiIiIiJSQyiBExERERERqSGUwIm4kTHmaWNMhrfjEBERcTdjzBZjjDXGtPJ2LCJ1mRI4ERERETktY0wfINF1d6QXQxGp85TAiYiIiMiZjAQOAwuoJgmcMcbXGBPg7ThEPE0JnIgHGWPON8YsMMbkGGP2GGP+Z4wJLVbub4x5yRizzRiTa4zZaYz58tgblDGmgTHmHdfxHFe9t733jEREpLYzxvgC1wJfA+8B7YwxXUrUOc8Y87MxJtsYc9AYM8MY061YeTNjzDhjTIYx5ogxZrkx5gZX2UDX0MyOJdqcYYz5rNj9scaYFGPMlcaYVUAO0NsY08gY854xZrMx5qgxZr0x5vmSyZ0xJtgY83djzFbXe+wWY8xfXWV/d51vSpwz2hiTZ4yJccdrKeIOft4OQKSuMMZ0AL4DpgNXA02BvwEtgCGuak8ANwKPA1uAOGAo4OsqfxnoCzwI7Ha1cZ5nnoGIiNRRg4CGwHhgNvBfnF64ZeAkYDjvbT8Do3B66voBTYAlxphYYB5wBHgE2A50xHkPq6hE4O/Aszjvg1uAaGAf8BCwH2gDPA3EAL9xxWiAr4A+wHPAIld857rafQ94FBgAzCj2eLcCk621eysRq0iVUAIn4jl/BrYCl1trCwGMMfuACcaYPtbaeUAv4BNr7fvFzptY7OdewGvW2gnFjn1UxXGLiEjdNhI4AHxnrc0zxnwPXG+MecJaa4G/4iRzF7vug/OF5TEPAvWBHtbaXa5jP1YylijgQmvt0mLH0nASQwCMMXNwksj3jDG/t9bmARcBg4ErrLVfFzv3AwBr7VrXebfiSuCMMS1wErzLKxmrSJXQEEoRz+kFfHkseXP5HCgA+rvuLwVGG2MeM8Z0LjmUw1X+qDHmt8aYNlUfsoiI1GWuYYhX4bx/5bkOjweaAX2MMSFAb+D9YslbSefjJH+7yiiviB0lkjeM4wFjzGpjzFEgH/gYCAQSisWwr0TyVtK7wNXFpjaMBvZwcjIq4nVK4EQ8pxHOG8FxrmQuE4h0HXoeeA34Lc63mduNMfcXO+VeYBLwJLDOGLPBGHN9VQcuIiJ11iVAA2Cqax52A5weqlycnrkIwACnS86izlBeEXtKOfYA8BLwJXAFzhemv3OVBVUgholAEXCt6wvUUcAH1tqCsw1axJ2UwIl4zi4gtvgB18TwKJyx+1hrc6y1T1prE3HG8E8AXjHGDHGVH7DW3metjQO64KwG9rExpr3nnoaIiNQhx1ac/BRnftl+nDlsgcA1rvtFOF9SliXzDOU5rn9LrigZUUrd0nr5rgE+s9b+0Vr7vbV2Ic4QyorEgLX2ME7v4micHrsEYMzpzhHxBiVwIp6zABjuStqOuQpnLurskpWttRtwxvTnAqckaNba5TgTrn2AtlURsIiI1F2u4ZGXAeNwFjIpfnsIZ2GTvjjvb7eUMuz/mB+Bi40xDcsoT3P9267YYzel/O9twTjvlcXdWEoMkcaYS8/Q1rs4896eBuZba9eWMwYRj9EiJiLuF2CMGVHK8TdwVumaZIx5HYgHXgSmuRYwwRjzJc7KWEuAo8AInL/TX1zls3GGiKzE+RbyTpxvGX+tyickIiJ10hVAPeDf1toFxQtcC378EaeH7nHgB+BbY8xbOO9LfYAUa+03wL+AW4BZxpgXcHrw2gEh1tq/W2vTjDEpwHPGmCM4X0z+AdfolHKYDtxnjFkAbMJJ3lqVUmca8Ikx5llgMU6P3HnW2t8cq2StXeDaoqA/rhUsRaobJXAi7heGM9SkpEE4cwn+AnwBZOF8q/lYsTpzges40bO2GrjaWpviKp+HM7QjESjESfQusdamISIi4l4jgQ0lkzcAa22+MWYicANwD84Kj8/hrIych/P+NMlVd68xph/O8v+v4Ay/3ICzemXxx3rHdX4aznvjg+WM81mcLQOed93/ArgPmFwsXmuMGe6K8QFX/Z3AJ6W0Nwlni5/x5Xx8EY8yZS8YJCIiIiJStxhjfgXWWWtv9nYsIqVRD5yIiIiI1HnGmGScxUt6cmIVS5FqRwmciIiIiAgsxNmw/AnXSpYi1ZKGUIqIiIiIiNQQ2kZARERERESkhlACJyIiIiIiUkNUyzlw0dHRNjEx0dthiIhIFVu0aFGGtTbG23HUFHp/FBGpO8p6j6yWCVxiYiIpKSlnrigiIjWaMWart2OoSfT+KCJSd5T1HnnGIZTGmKbGmJ+NMauNMauMMfeXUscYY141xmw0xiw3xnQvVjbKGLPBdRt1dk9DRERERESk7ipPD1wB8LC1drExJgxYZIyZbq1dXazOJUBr16038DrQ2xgTCTwFJAPWde7X1tr9bn0WIiIiIiIidcAZe+CstbustYtdPx8C1gBNSlS7AvjAOuYDDYwxjYCLgenW2n2upG06MMStz0BERERERKSOqNAcOGNMItANWFCiqAmwvdj9NNexso6X1vZdwF0ACQkJFQlLRMSt8vPzSUtLIycnx9uh1BpBQUHEx8fj7+/v7VBqHV2v7qfrVUSqs3IncMaYUOBz4AFrbZa7A7HWvgW8BZCcnKzdxUXEa9LS0ggLCyMxMRFjjLfDqfGstWRmZpKWlkbz5s29HU6to+vVvXS9ikh1V6594Iwx/jjJ28fW2i9KqbIDaFrsfrzrWFnHRUSqrZycHKKiovRh2E2MMURFRamHqIroenUvXa8iUt2VZxVKA7wLrLHWvlxGta+BW1yrUZ4DHLTW7gKmARcZYyKMMRHARa5jIiLVmj4Mu5dez6ql19e99HqKSHVWnh64fsDNwPnGmKWu21BjzN3GmLtddaYCm4GNwNvAbwGstfuA54CFrtuzrmMiIlKGzMxMunbtSteuXYmLi6NJkybH7+fl5Z323JSUFO677z4PRSplMca8Z4xJN8asLKO8zO13ahpdryIinnXGOXDW2tnAab+KstZa4HdllL0HvFep6ERE6qCoqCiWLl0KwNNPP01oaCiPPPLI8fKCggL8/Er/7zs5OZnk5GSPxCmnNRb4L/BBGeWlbr/jkcjcTNeriIhnVWgVypoiZfKbUJCLj18Avv6B+PgH4ucfhG9gICFhkYRHRhMaHoUJqg8+vt4OV0TkjEaPHk1QUBBLliyhX79+XH/99dx///3k5OQQHBzMmDFjSEpKYsaMGbz00kt88803PP3002zbto3Nmzezbds2HnjgAfV2eIi19hfXys1lOb79DjDfGNPAGNPINf2gxtP1KlIzHc4tYOb6vcRHBNO+UTh+vuVaLuOMrLVszjjMyh0HSYisR4fG9Qnwc0/b7mKtJWXrfjamZ5da3iI6hC5NGxDkX3bukJNfyIodB1m7K4ub+yRWUaS1NIFruuhFGpJZrrqHqccR33COBkRQEBQFITH4h8cSFBFH/dimBMS0gsgWEFS/iqMWETm9tLQ05s6di6+vL1lZWcyaNQs/Pz9++OEH/vCHP/D555+fcs7atWv5+eefOXToEElJSdxzzz1aGr16KGubnVMSuJq6zY6uV6kJcvILKSw6u8XPjYF6AeX7SG2t5UheYall9QJ8z2r+pbWWwiJbqaRr3+E8xs5N5f25qRw8mn88nm4JDUhuFknPxEg6NgnHv5xtF1nLxvRsUlL3szB1Hylb97Pv8Ikh1UH+PnRt2oCeiZH0aBZB92YRhAed3d96UZHFx6fir19RkWX6mj28MXMTS7YdOG1df19Dpyb16ZkYSXJiJG3jwli3+xALt+4jJXU/K9IOkldYBMCwzo2JDAmo1HM5k1qZwPncPYOdOUfJz8shLzeX/LxcCvNzKMg9Su7hA+Rm7yM/ez/26AFMzkF88w4QlLOfsMNpRO9bRRRZ+JuT/7iO+jegoH5zAhu2IqBRB2jcHRp3VWInUss9M3kVq3e6d+eU9o3DeeqyDhU+75prrsHX1/nm7+DBg4waNYoNGzZgjCE/P7/Uc4YNG0ZgYCCBgYHExsayZ88e4uPjzyp+8ayKbLOj61WkfI7mFfKXqWv4aMFWrBs2r2pcP4jkxEh6JkaQnBhJm4Zh+PoYCgqLWLPrECmuD/gLU/eRfii31Da6JTTg3VE9y/2hv7DIsm630/bC1P2kpO5j76FcOjSpT89mThzJiRFEhwaW2Uba/iO8M2sL4xduIye/iIvaN2RU30T2Hc4jJdVp9z8/beBsctzEqHqc3zaW5GYRdIqvz7bMI068W/fxvxmbKCyyBPj58NjFSdzWr3mFk7Bjz2HCwu1EhgTQo1nEKb+H0uQVFPHV0h28MXMTm/YepmlkMM9d2ZEL2sbiUyKRLigqchI11+s8Zk4qb/6y+Xj5scTu1n6JJLuS0qpK3qCWJnAxcZX7hjKvoIg9WTks3n+EjIx0MnduJmvnBuy+zUQdTSMxZzeJe3+k8apPj59TENEKv6Y9nISuWV9o2BF8qleXsIjUDiEhIcd//vOf/8ygQYP48ssvSU1NZeDAgaWeExh44o3b19eXgoKCqg5TyqfWb7Oj61Wqws4DR0nZ6nyIPpJXSPcE58N6y5jQcn/wX5F2kPsnLGHz3sOM7JVA8+h6ZxVTfqFl9a4sFmzJ5OtlOwEIC/KjZUwo6/ccOt7j1qRBMH1bRpEUF07JjqzDuYW8MXMT1745j49u701c/aAyH2/Nriz+MW0dC7fs41Cu8zcSFx5EcmIEjRsEs3T7AT6Yv5V3Zm8BoHl0CM2jQ05Z0CK3oIh5mzMxwJXdmnD3gBa0ig07Xn5Zl8YAHMrJZ8m2A6zfc4iiCmS78RH1SE6MIDbs5OfSoXF9LunUyPW8C1i2/QDvzUnl+Slr+GltOv+8tguN6gefsf21u7N4c+Zmvl62EwNc2rkR+UWW+ZtP/j10alKf4FKGPa7elcWugzm0axTOqyO7MbRj3Gl7L+Mj6nFBu4ZAsaGSuw/RJjb0jEMr3a1WJnCVFeDnQ9PIejSNrActo4H2x8sysnNZtTOLL3ccZO3mVPK2LaJ1wQa6ZGym24HviV4+AQBbLwqTeC60GADNBzjDL7UcsUiNVZmeB084ePAgTZo0AWDs2LHeDUYq42vgXmPMeJzFSw66Y/6brlepTYqKLOvTT/R6pKTuZ8eBowCEBPgS5O/LZ4vSAGhQz5/kY71Orp6eQL+TP1AXFllen7GRV37YQHRoIB/f0Zt+raLdFq+1lrT9R4/3iG3ck82IHvHHY2rc4PRJSZ+WUdzxfgoj3pjLR7f3JjE65KTyoiLLO7M389K09YQH+3F518auoXwRNGkQfNLwy9yCQlbuyDrei7Y76+gpj2cwjO6byO39m582trAgf85rE8N5bWIq+IqcWUigH31bRdOnZRQTU7bzzOTVXPyvX3hheKfjCWRxh3LySUndz0fzt/Lj2nTqBfie8hyO/R6ODd1ctTOLrJxTe/2T4sL461WdGNAmpsJDV4P8femZ6Awt9QYlcOUUHRrIgDYxDGgTA4NaUVh0AWt2ZTF/cyafbtlH6ub1dMxbxgC7mgHr59Bg9STnxAYJ0O1m6D4Kwhp690mISK3x2GOPMWrUKJ5//nmGDRvm7XCkBGPMOGAgEG2MSQOeAvwBrLVv4Gy/MxRn+50jwK3eidQzdL1KcbaMXpzcgiKWbT9wvIctZet+DuU4PUyxYYH0TIzkjnOb09M198jXx5CaecT5oO6q/8OadMD5Ur5LfP3jwxrjwoN58quVpGzdz6WdG/HClZ2oX8+98yuNMcc7AoZ3q/jQ33NaRDHuznO45b0FjHhjHh/e3ot2jcIBp+fx4YnLmLc5k4s7NOSvV3U+7RC9QD9fejSLoEezCH4zoNJPyWOMMVzXM4HezaN4cOJSfj9uCT+u2cO957dmza4Tieja3VkUWYio589Dg9twS59mNKgXcEpbx34PV3WvnUOwTVl/RN6UnJxsU1JSvB1GhRQUOt3Q3yzbxXcrdxGZu50LAtdwTegykrIXgo8/dLgSet0F8T3VKydSja1Zs4Z27dp5O4xap7TX1RizyFqrdeTLqbT3R12vVUOva9WYuzGDxz5fTtr+U3uEimsdG3q85yo5MYKEyHrl6iXJyM5lkSsBXJi6n5U7DlLgmsAVFujHc1d25Iqujav1Zu0b0w9x0zu/ciSvgDG39iJt/xH+NGklRUWWpy7vwDU94qt1/GeroLCI137exKs/bTi+wEywvy/dmzkLqiQnRpDcLJLggNq/knxZ75HqgXMTP18fzm0dw7mtY3juyo7M2ZjB5OXJDF85mCaFO3iuyXx6r/sWs+JTiOsMfX4Hna7VfDkRERGpVjamZ/PJgm2EBvqSnBhJt4QGhJ3lCoG5BYW8NG0db8/aQovoEO6/oPUp32X7GkO7RuH0aBZBRCUXgIgODeTiDnFc3CEOcBYrWbr9AOt2Z3Fh+4bER5zdfDdPaBUbxqd39+Hmdxdw3ZvzKCiy9GgWwb+u7UpCVPWP/2z5+fpw/4WtuaBdLEu27adL0wa0a1T+FTDrAiVwVSDAz4dBbWMZ1DaWx4fk8PL09dyQEkds4KX8s+06+mR+js+Xv4Ff34Zh/3RWsxQRERHxoiXb9vPGzE18v3oP/j4+FBQVUWTBx0C7RuH0TIyka9MG1Cul56NegB+dm9YvdSn4tbuzeGD8UtbuPsRN5yTwh6Htyr3s/tkKDvClT8so+rSM8sjjuUvTyHpMvLsP//fZcno0i+DuAS3dtidbTdGxSX06NtFq76VRAlfFYsOD+NvVnRnVN5G/TF3DjUv9SIxM5j+91tNp1Uvw1kDoeTuc/ycIjvB2uCIiIlKHWGv5ZUMGr8/YyPzN+6gf7M/vB7ViVN9EAv19WbJt//FFRCYs3M7YualltmUMJDUMO76wRo9mEXy3cjd//24d4cH+jBndk0FtYz335Gq42LAgxtzay9thSDWkBM5D2jUK54PbejFj/V7+MmUNl/3SlJGd3uPp8MkEprwDqybB4Gehy0gNqxQREZEqUVRk2bQ3+3hS9mvqPtL2HyUuPIg/DWvH9b0SCA088fHw2PQQgPzCIrZkHCavoOiUdg8cyXfmnm3dxxeL0/hw/tbjZYPbN+RvV3Ui6jT7kYlI+SmB8yBjDIOSYjm3VfTxyZkzwobw+iWX0XXF8/DVb2HZOBj+JtRv4u1wRUREpJaYtWEvY+ekkrJ1PwePOkuqR4cGkNwskvsvaM0VXZsQ4Hf6L5D9fX1o0zCszPL+rZ0l+QsKi1i7+xALU/cRGxbE0E5xtXrRDRFPUwLnBccmZw5MiuHBiUu58otD3Nr3n/yhUwr+0/8Ib/SDK16DtlpqWURERCovJ7+Qv327lrFzU2lcP4ghHeJIToygZ2IkzaLKt7JjRfn5+mj+kkgV0lg9L+rStAFTfn8uo/o0Y8zcbVwyuwXrr5zi7B03/gaY+ijk53g7TBHxsEGDBjFt2rSTjr3yyivcc889pdYfOHAgx5aWHzp0KAcOHDilztNPP81LL7102sedNGkSq1evPn7/ySef5Icffqho+FLH6HqtvlbuOMil/5nN2LmpjO6byE+PDOTFEZ25JrkpidEh6hUTqaGUwHlZcIAvz1zRkQ9v70V2TgGXj9/N9D4fQZ974de34O3zIX2tt8MUEQ8aOXIk48ePP+nY+PHjGTly5BnPnTp1Kg0aNKjU45b8QPzss89y4YUXVqotqTt0vVY/hUWW12dsYvj/5nAoJ58Pb+/F05d3IMi/9u+bJVIXKIGrJs5tHcPk3/cnKS6cu8atYEzoHXDDp5C9x1mpcvXX3g5RRDxkxIgRTJkyhby8PABSU1PZuXMn48aNIzk5mQ4dOvDUU0+Vem5iYiIZGRkAvPDCC7Rp04b+/fuzbt2643XefvttevbsSZcuXbj66qs5cuQIc+fO5euvv+bRRx+la9eubNq0idGjR/PZZ58B8OOPP9KtWzc6derEbbfdRm5u7vHHe+qpp+jevTudOnVi7Vp94VTX6HqtXpanHWDk2/N58bu1DG7fkO/uP+/4IiQiUjsogatGYsICGX/nOVzYriHPTF7Ns+viKfzNbIjrCJ/fDptnejtEEfGAyMhIevXqxbfffgs4vRnXXnstL7zwAikpKSxfvpyZM2eyfPnyMttYtGgR48ePZ+nSpUydOpWFCxceL7vqqqtYuHAhy5Yto127drz77rv07duXyy+/nH/84x8sXbqUli1bHq+fk5PD6NGjmTBhAitWrKCgoIDXX3/9eHl0dDSLFy/mnnvuOeOwN6l9dL16n7WW2RsyuOmdBVz+3zms2ZXFP6/pwms3dK/0htgiUn1pEZNqJjjAlzdu6sFz36zmvTlb2HHgCK9cM4Hgj4Y58+JGT9HG3yKe9O3jsHuFe9uM6wSX/O20VY4NS7viiisYP3487777LhMnTuStt96ioKCAXbt2sXr1ajp37lzq+bNmzWL48OHUq1cPgMsvv/x42cqVK/nTn/7EgQMHyM7O5uKLLz5tLOvWraN58+a0adMGgFGjRvHaa6/xwAMPAM4HbIAePXrwxRdflO81kKqh67VOXa+FRZZpq3bz+oxNrNhxkJiwQJ64pC039E4grJQNtUWkdlACVw35+hievrwDCZH1eG7KakZm5TLmqvFEjL8UPh4Bt02DqJZnbkhEaqwrrriCBx98kMWLF3PkyBEiIyN56aWXWLhwIREREYwePZqcnMotcjR69GgmTZpEly5dGDt2LDNmzDirWAMDnb2dfH19KSgoOKu2pGbS9eoZ1lq2ZBwmJdXZb23Oxkx2HDhK8+gQ/npVJ4Z3a6J5biJ1gBK4auy2/s1pEhHMfeOW8Mi0AN656QvMmCHw4XC4/XsIi/N2iCK13xl6HqpKaGgogwYN4rbbbmPkyJFkZWUREhJC/fr12bNnD99++y0DBw4s8/zzzjuP0aNH88QTT1BQUMDkyZP5zW9+A8ChQ4do1KgR+fn5fPzxxzRp4uw7GRYWxqFDh05pKykpidTUVDZu3EirVq348MMPGTBgQJU8bzlLul5r3fVqreXrZTuZumIXKan7yTzszDVsUM+f5GaR/GFoO4Z0jMPXRytKitQVSuCquYs7xPHoxUk8P2UN3/aIZ+iNn8LYy+Cjq53hlMGVW71LRKq/kSNHMnz4cMaPH0/btm3p1q0bbdu2pWnTpvTr1++053bv3p3rrruOLl26EBsbS8+ePY+XPffcc/Tu3ZuYmBh69+59/EPw9ddfz5133smrr756fDEIgKCgIMaMGcM111xDQUEBPXv25O67766aJy01lq5X98vMzuXxL1YwffUe4iOCGZAUQ8/ESHomRtAiOhQfJW0idZKx1no7hlMkJyfbY3vECBQUFnHl/+aQnpXLDw8PIDztF/jkOmjaC276AvyDvB2iSK2yZs0a2rVr5+0wap3SXldjzCJrbbKXQqpxSnt/1PVaNbz9uv68Np1HP1tO1tF8HhuSxG39mithE6ljynqP1CqUNYCfrw9/Hd6ZjOxc/v7dWmh1AQx/A7bOgR+f8XZ4IiIi4iZH8wr586SV3Dp2IdGhAXx1bz/uOLeFkjcROe6MQyiNMe8BlwLp1tqOpZQ/CtxYrL12QIy1dp8xJhU4BBQCBfqWtfI6xdfn1n7NeW/OFoZ3a0KPTiNg23yY/z9ofRG0HOTtEEVEROQsbEw/xF0fLmLz3sPc0b85j1ycpEVJROQU5emBGwsMKavQWvsPa21Xa21X4AlgprV2X7Eqg1zlSt7O0kOD29C4fjBPfLGCvIIiGPwsRLeBSb+FI/vO3ICIiIhUWy9MWcO+w3l8fEdv/nRpeyVvIlKqMyZw1tpfgPJmByOBcWcVkZQpJNCPZ6/owPo92bw9azME1IOr3oLD6TDlYaiG8xlFaqrqOD+4JtPrWbX0+rqXN17PzOxcftmQwfU9E+jXKtrjjy8iNYfb5sAZY+rh9NR9XuywBb43xiwyxtx1hvPvMsakGGNS9u7d666wap0L2jVkaKc4/v3jBlIzDkPjbjDwCVj1Baz41NvhidQKQUFBZGZm6kOxm1hryczMJChICy5VBV2v7uWt6/Wb5bsoLLJc2a2xRx9XRGoed24jcBkwp8Twyf7W2h3GmFhgujFmratH7xTW2reAt8BZZcuNcdU6T13WgVnrM/jjpBV8dHtvTP8HYcN0mPIIJPSBBk29HaJIjRYfH09aWhr6Msl9goKCiI+P93YYtZKuV/fzxvU6aekO2saF0TYu3KOPKyI1jzsTuOspMXzSWrvD9W+6MeZLoBdQagIn5dcwPIjHLmnLnyet5NuVuxnaqRFc9Sa83g++vBtGfQ0+GjcvUln+/v40b97c22GIlIuu15pva+Zhlmw7wOOXtPV2KCJSA7hlCKUxpj4wAPiq2LEQY0zYsZ+Bi4CV7ng8gRt6JdA6NpSXp6+nsMhCRCJc8iJsnQ3z/uvt8ERERKScvlq6E2Pg8i4aPikiZ3bGBM4YMw6YByQZY9KMMbcbY+42xtxdrNpw4Htr7eFixxoCs40xy4BfgSnW2u/cGXxd5utjeGhwGzamZ/PV0h3Owa43QrvL4KfnIX2NdwMUERGRM7LWMmnJDnolRtK4QbC3wxGRGuCMQyittSPLUWcsznYDxY9tBrpUNjA5s4s7xNGhcTiv/LCBy7o0xt/XBy59Bf7bE766F27/XkMpRUREqrEVOw6yOeMwd53XwtuhiEgN4bZVKMXzfHwMj1yUxLZ9R/g0Jc05GBINQ/8BO1Jg/uveDVBEREROa9KSnQT4+nBJp0beDkVEagglcDXcwKQYuic04D8/bSAnv9A52PFqaKP9TOwAACAASURBVHOJM5Qyc5N3AxQREZFSFRQWMXn5Tga1jaF+sL+3wxGRGkIJXA1njOGRi5PYdTCHTxZsO3YQLn0ZfP3h6/ugqMi7QYqIiMgp5m7KZO+hXK7s2sTboYhIDaIErhbo2zKavi2j+N+MjRzJK3AOhjeGi19wVqVcNMa7AYqIiMgpJi3dQViQH4Paxno7FBGpQZTA1RIPX5RERnYeY+emnjjY7WZoMRCmPwUH07wUmYiIiJR0NK+QaSt3M7RjI4L8teCYiJSfErhaokezCM5vG8ubMzdz8Gi+c9AYuOzfYAth8gNgrXeDFBEREQB+WLOHw3mFXNFNe7+JSMUogatFHhrchoNH83l39pYTByMS4cKnYeN0WDbeS5GJiIhIcZOW7CAuPIhzmkd5OxQRqWGUwNUiHZvUZ2inON6dtZnM7NwTBT3vhKa9YfqfIe9w2Q2IiIhIlcvIzmXm+r1c3rUxPj7G2+GISA2jBK6WeWhwG47mF/K/GcW2D/DxgcHPwuG9sPAd7wUnIiJSh+0/nMcrP6xn8MszKbKWq7pr9UkRqTglcLVMq9gwRvSI58N5W9lx4OiJgoRzoOUFMPsVyD3kvQBFRETqmB0HjvLM5FX0/dtPvPLDBno0i+Sze/rSNi7c26GJSA2kBK4Wuv/CNmDglenrTy4Y9Ec4ug8WvOmdwEREROqQgsIinvhiBQP+/jMfztvKJZ3i+P7B83hnVDLdEyK8HZ6I1FBK4GqhJg2CufmcZny+OI2N6cV62+J7QJshMPc/kHPQewGKiIjUAd+v3sO4X7dxTXI8Mx8bxMvXdqVNwzBvhyUiNZwSuFrqtwNbUi/Aj5emleiFG/gE5ByA+a97JzAREZE6YuycVOIjgnn+yk40aRDs7XBEpJZQAldLRYUGcue5Lfhu1W6Wbj9woqBxV2h7Kcx7DY7s816AIiIitdiqnQf5NXUfo/ok4quVJkXEjfy8HYBUndvPbc4H81L5x7S1fHzHOScKBj4Ba79xkrgL/uy1+ERERGqr9+emEuzvy7XJTb0dSsUd2gOLP3BWry7JxxcaNIPo1hDdBsKbOKtdi4jHKIGrxUID/bj3/FY8M3k1szdk0L91tFMQ1xE6DIcFb8A5v4UQbSIqIiLiLvsO5/HV0p1c3SOe+vX8vR1O+WVucubJL/0ECvMgqP6pdQrzIb/YnrJ+wRDdyknmotucSOyiWoG/ho2KVAUlcLXcDb0TeGfWFl78bi39WvXDGNcwjgGPw6pJMPffzh5xIiIi4hbjF24jt6CI0X0TvR1K+excCnNegdVfgY8fdL0R+v4eolqeWtdap2cuYz1kbHDd1kNaCqz8ArCuigYaND01sYtuAyExYDSsVKSylMDVcoF+vjw4uA2PfLqMb1fuZminRk5BbFvodA38+jb0uRdCY70bqIiISC1QUFjER/O20rdlVPVecdJa2PKLk7ht+gkCw6HvfXDOPRAWV/Z5xjifGUJjIbH/yWX5R2Hf5hPJ3d51kLkBts6F/CMn6gXWL713zxhokFAs2Ts2TDNewzRFilECVwcM79aEN2du4r8/bTyRwAEM+D9Y+RnM+Tdc/IL3AhQREaklpq/ew86DOTx9eQdvh1K6okJYOwVm/wt2LoaQWLjgKeh5e+lJVUX4B0PDDs7tpMcsgkM7i/XarYe8I6eeX5QP+7fCys9P3u4oMBxaD4akoc6/ZxunSA2nBK4O8PUxjOyVwLPfrGbT3mxaxoQ6BdGtoOMISBkD5z4M9SK9G6iISC1ijBkC/BvwBd6x1v6tRHkC8D7QwFXncWvtVI8HKm41dq6zdcAF7Rp6O5ST5R+FFZ86X9pmboSI5nDpv6DLDeAfVLWP7eMD9eOdW8vzz1zfWjic4Ur41sOORbD+Oyex8/GH5udB22FOQhfe6MztidQySuDqiKGdGvHsN6uZunwXv7+g9YmC/g/Ciomw4E0Y9IT3AhQRqUWMMb7Aa8BgIA1YaIz52lq7uli1PwETrbWvG2PaA1OBRI8HK26zZlcWC7bs4w9D21aPrQOO7IP105yVpzf95AxjjOsMI8ZA+yucFSWrI2MgNMa5JfaD5FudnsO0FOe5rP0Gpjzk3Jr0cJK5tpc6wy1Lzq0rKoKsHc7QzsL8Ux/L1w8iW2iYZl2Sf9RZsCfnoLPYTmhsjZuTqQSujoirH0TPxAimrCiRwDVs73yDteAN6HsvBFbj8foiIjVHL2CjtXYzgDFmPHAFUDyBs0C46+f6wE6PRihu9/7cVIL8fby7dcDR/bB8IqyZ7Mw9s4UQ1hi63uAkbYnn1rgPq4CTbCb0dm6Dn3Xm1639BtZNhR+fdW6RLaHtUAgIO9F7l7nx5Pl3ZfELdj7Ml5x/F9UKAuqVfk5OljPHL3MTFOSWErMfRCS6Fm7Rit9VJucgZGyEfWX8HgpyYN+WE9fEgW2cWGwHZ05m8d97SDRQyt9IeKNqMydTCVwdMqxTI56evJqN6YdoFVssUev/kPMf4KKxzqpTIiJytpoA24vdTwN6l6jzNPC9Meb3QAhwoWdCk6qw/3AeXy7ZwVXd42lQL8DzAWTthPn/c6ZF5GVDTDtnlE3bYdC4W81M2spijLMYW2xbOO8R57mvm+rM7Zv/utNbd2wFzMT+zofyyJbgX0oiVuDqjTk2N2/nYlj1JSd9wK9fbGGVovwTc/kO7Sp/zMGRp67GGd3a2VPPtwo/jh89ABt/cHpjj233EN6kelwPudlOApyxwUmqrD3zOQBYOLT7xO8he/eZT/ELdl7v+J7OKqvRrSEo3PW7dyV2m3+GZZ+Usy3XaxnZAnwDS6/X/8Eq+92esVVjzHvApUC6tbZjKeUDga+ALa5DX1hrn3WVnXb8v3jWJZ0a8cw3q5myfDf3X1gsgWva0/lGbu5/oddd4FfGhSgiIu40Ehhrrf2nMaYP8KExpqO1tqh4JWPMXcBdAAkJCV4IU8pjQsp2cguKGNW3mWcfOGODM69t2Xint63j1dDvfojr5Nk4vCm8MfS8w7nlZoPxKbvXrDTNzzv5/rEhdpnFtknIWA+L5zlz8KJbQ4tBJXrpQk5ttzCv2KqcrmRj/Xew5MMTdXz8ne0aSiZ2Ua4EozIO7jiR0KbOgqKCk8v9Q5wEJKq15xeEKSqAA1ud1yJrR+XbCawPMW2g1QWuntPT/B58/CC0Yem9Zq1KfG+Wk3XyAjrH2CI4mHbyQjw7FpXYOqOEvr/3XgIHjAX+C3xwmjqzrLWXFj9QzvH/4kENw4PomRjJlBU7uf/C1icXnvswfHils3ln8q3eCVBEpPbYARQfRxfvOlbc7cAQAGvtPGNMEBANpBevZK19C3gLIDk5ubxfUYsHrdxxkDdnbuKcFpG0javkh+6KONarsvIL54O6XyD0GO1MhYhIrPrHr84CQ8++Df9giOvo3Io71kNUkd6rqJbOypnFHdnnDO0sntilr4G1U50k/JiwRk5SEliBayprB+xa6vwc3cbZKqrtpU6P5PHHdG3xkPZr6auBViVjoH5Tp+OgeNIa0cxJZsvLx7dqehGDwstOnCOaOXMyiysqLLvnsArnmJ4xgbPW/mKMSaxE2+UZ/y8edmnnRjz51SrW7zl08v40LQY6Qyzm/Bu63Vy13fkiIrXfQqC1MaY5TuJ2PXBDiTrbgAuAscaYdkAQsNejUcpZW5i6j9vGLCQsyI+/XtW56h6otF6VkFg49yHofY+z4IdULXclDPUioV4vaNrr5OMFebA/9eTELnODk7CXV1A4XPg0JA1zeqiKC4s7de8+OTteWgjIXZ/S+xhjluFMwH7EWruK8o3/P05DRDxjSMc4nvp6FVOW76LN4GIJnDFOL9yEm2D1JOg0wntBiojUcNbaAmPMvcA0nGkE71lrVxljngVSrLVfAw8DbxtjHsQZgzPa2nJPApFqYMa6dO7+aBGN6wfz4R29adIg2P0PkpsNX9zpJG/gDHs71qvSpIfXF1MQN/ILcJKukomXSAnuSOAWA82stdnGmKHAJKD1Gc45hYaIeEZsWBC9m0cyZcUuHriwNab4t0lJwyA6CWa97Iyhrw4TXEVEaijXnm5TSxx7stjPq4F+Jc+TmmHK8l08MGEJrWPD+OD2XkSHVsH88SP74ONrYOcSGPC4896sD/cidd5Zf21jrc2y1ma7fp4K+Btjoinf+H/xgmGdG7MxPZv1e7JPLvDxcVbMSV/l7BsjIiIip5iwcBu/H7eYrk0bMO6uc6omeTu0G8YOg93L4boPnb1albyJCG5I4IwxccbVjWOM6eVqM5Ni4/+NMQE44/+/PtvHk7M3pEMcPgamLC9ly6FOI5zlcmf9swLLuYqIiNQNH83fyv99voJzW8fwwW29qR9cgYUXymt/Krw3BPZvhRs/dbYCEBFxOWMCZ4wZB8wDkowxacaY240xdxtj7nZVGQGsdM2BexW43joKgGPj/9cAE11z48TLYsICOadFFN+s2MUp0y18/aHffc7KRKmzvBOgiIhINbRq50GembyKQUkxvH1LMsEBVbCAQfpaJ3k7uh9Gfe0sMiYiUkx5VqEceYby/+JsM1Ba2Snj/6V6GNa5EX/8ciVrdx+iXaMSy6V2u8mZB/fjc3D795oLJyIidd7RvELuG7eEyJAAXr62KwF+VbB4yM4l8OFV4BsAt34LDdu7/zFEpMbT0kV11IlhlLtOLfQPhgGPOb1w67/zfHAiIiLVzAtTV7Np72H+eU1XIkIC3P8Ae9c7yVtgKNym5E1EyqYEro6KCg2kb8tovlm+89RhlOD0wkW2gB+fdTYpFBERqaOmr97DR/O3cdd5LejfOtr9D3AwDT4cDj5+cMtXzvuviEgZlMDVYcM6NyI18wirdmadWujrD4P+COmrYcVnng9ORESkGkjPyuH/Pl9O+0bhPHxRFawCeWSf0/OWmwU3fa7kTUTOSAlcHXZxhzh8fQyTl5WyGiVAh6sgrhP8/AIU5Hk2OBERES8rKrI8/OkyjuQV8OrIrgT6uXnRktxsZ5+3/akwchw06uze9kWkVlICV4dFhgQwKCmWzxfvIL+w6NQKPj5wwVNwYCssft/zAYqIiHjRmLmpzNqQwZ+GtadVbJh7Gy/Ig4k3w87FMOI9SOzv3vZFpNZSAlfH3dC7KRnZufy4Zk/pFVpdCAl9YebfIe+wZ4MTERHxkvV7DvHit2u5sF1Dbuyd4N7Gi4pg0t2w6Se47FVod6l72xeRWk0JXB03oE0sjeoH8cmv20uvYAxc+BQcTocFb3g2OBERES/5388bCfTz4cWrO2HcvZ3OvP/Ays/hwqeh+83ubVtEaj0lcHWcr4/hmuSmzNqwl+37jpReKeEcaDME5vzb2VhURESkFks/lMOUFbsYkRxPVGigexvP3AQ//wWShkG/B9zbtojUCUrghOt6NgVgYkoZvXAA5/8ZcrJg9iseikpERMQ7PlmwjfxCyy19Et3bsLUw+X5no+5hLzmjXEREKkgJnNCkQTAD2sQwMWU7BaUtZgIQ1xE6jYAFb0J2umcDFBER8ZC8giI+XrCNQUkxNI8OcW/jSz6C1Fkw+BkIb+zetkWkzlACJwCM7JXAnqxcZqzbW3alAY9DQQ7Mf91zgYmIiHjQtyt3sfdQLqP6Jrq34UN74Ps/QrN+0H20e9sWkTpFCZwAcH7bWGLCAhn367ayK0W3gvaXw8J3IOeg54ITERHxkDFzUmkRHcJ5rWPc2/C3j0J+Dlz2b2ebHhGRStL/IAKAv68P1ybH8/O6dHYdPFp2xX4PQG4WpIzxXHAiIiIesHT7AZZuP8AtfZrh4+PG+WlrvoHVX8GAxyC6tfvaFZE6SQmcHHddcgJFFiYuTCu7UpPu0GIgzP+f802iiIhILfH+3FRCA/24uke8+xrNOQhTH4HYDtDvfve1KyJ1lhI4OS4hqh7nto5mYsp2Cots2RX7PwTZe2DZJ54LTkREpAqlH8rhm+U7GdEjnrAgf/c1PP0p5z3ziv+ArxvbFZE6SwmcnOT6ngnsOHCUWRtOs5hJ8/OgcXdnX7jCAs8FJyIiUkXGLdju2jqgmfsaXTYBFo2B3vdAkx7ua1dE6jQlcHKSwe0bEhUScPrFTIyB/g/C/lRYPcljsYmIiFQFZ+uArQxoE0OLmFD3NLr+e/jqt5B4LlzwpHvaFBFBCZyUEODnw4ge8fy4Jp30rNPMcWt7KUS1djb2tqcZbikiIlLNfbtyF+mHchndL9E9DW5bABNvgYYd4PpPwD/IPe2KiKAETkpxfa8ECoos437dXnYlHx/o/wDsWQEbf/RccCIiIm42dm4qzaNDGOCOrQP2rIZPrnE26r7xcwgKP/s2RUSKUQInp2geHcKgpBg+nL+V3ILCsit2uhbCGsPsf3kuOBERETdatv0AS7a5aeuA/Vvho6vAvx7c/CWEunkvORERlMBJGW7v34KM7FwmL9tVdiW/AOh7L2ydDdt/9VxwIiIibvL+3FRCAnwZcbZbB2TvhQ+HQ/5RuOkLiHDjYigiIsUogZNS9WsVRZuGobw3ewv2dHPcuo+C4AiY9bLnghMREXGDjOxcvlm+6+y3Dti+EMYOg6ydcMNEaNjefUGKiJSgBE5KZYzhtn7NWb0ri/mb95VdMTAUet8N67+F3Ss9F6CIiMhZGrdgG3mFRdzSN7FyDRw9AN88BO8OhrxsuGECJPR2a4wiIiWdMYEzxrxnjEk3xpT66dwYc6MxZrkxZoUxZq4xpkuxslTX8aXGmBR3Bi5V78puTYgMCeC9OVtOX7HXXRAQBrP+6ZnAREREzlJ+YREfLdjKeW1iaFnRrQOshZWfw2u9nH3ezvkt/G4BtBhQNcGKiBRTnh64scCQ05RvAQZYazsBzwFvlSgfZK3taq1NrlyI4i1B/r7c2DuBH9bsYWvm4bIr1ouEXnfAqi8hY4PnAhQREamk71buZk9WLqP7VnCu2oHt8PEI+Ow2CGsEd/4MQ/4CgWFVE6iISAlnTOCstb8AZY6hs9bOtdbud92dD5zlLGCpTm4+pxl+PoYxc1JPX/Gc34FfkObCiYjUUXkFRUxasoPM7Fxvh1IuY+em0iyqHgPbxJb/pKJCmHAjbJsPl/wd7vwJGnetuiBFRErh7jlwtwPfFrtvge+NMYuMMXed7kRjzF3GmBRjTMrevXvdHJZUVmx4EJd1bsynKdvJyskvu2JoDCTfCssnwP5Uj8UnIiLVw8cLtvLAhKX0e/EnnvpqJdv3HfF2SGVakXaQRVv3c0ufxIptHbDkI9i1DC77N/T+Dfj4Vl2QIiJlcFsCZ4wZhJPA/V+xw/2ttd2BS4DfGWPOK+t8a+1b1tpka21yTIz2TalObu3XnMN5hUxceJqNvQH6/t55M5v9imcCExGRaqGoyPL+3FTaNwrnss6N+eTXbQx8aQYPTljK2t1Z3g7vFGPnplIvwJdrkiswaOjoAfjxGUjoAx2vrrrgRETOwC0JnDGmM/AOcIW1NvPYcWvtDte/6cCXQC93PJ54Vqf4+vRKjGTs3FQKi06zpUB4Y+h2Eyz9GA7u8FyAIiLiVTM37CU18wi/GdCCf1zThV8eG8StfROZtmo3Q16Zxe8+WUx+YZG3wwRw7XG6k6u7xxNeka0DZr4IR/bBJS+COcsNv0VEzsJZJ3DGmATgC+Bma+36YsdDjDFhx34GLgK0znwNdVv/5qTtP8r01btPX7HfA84cgbn/8UxgIiLidWPnpBITFsglHRsB0Kh+MH+6tD1zHz+f3w1qyZTlu3j1x+qxyNX4X52tA0ZVZPGS9LXw61vQYxQ06nLm+iIiVag82wiMA+YBScaYNGPM7caYu40xd7uqPAlEAf8rsV1AQ2C2MWYZ8CswxVr7XRU8B/GAwe0bEh8RzLuzz7ClQEQz6HI9LBoL2ZrLKCJS223em83M9Xu5qXczAvxO/ljRoF4Aj17clhE94nnt5438uuU0+4p6QH5hER/N38a5raNpFVvOVSOthe8eB/8QOP/PVRugiEg5lGcVypHW2kbWWn9rbby19l1r7RvW2jdc5XdYayNcWwUc3y7AWrvZWtvFdetgrX2hqp+MVB1fH8OoPoksTN3PlozTbCkA0P8hKMiB+a95JjgREfGaD+Ztxd/XMLJ30zLrPH15B5pG1uPBCUs5ePQ0C2JVsWmrdrM7K4fRFdm4e91U2PwzDHoCQqKrLDYRkfLy83YAUnNc3CGOF6auYca6dJpHNy+7YnQr6HgV/Po29L3P2SdORERqnezcAj5blMalnRsTGxZUZr3QQD9eua4rI96Yx58mreTV67tiqnAe2WeL0pi6Ytcpx9fsyiIhsh4Dk8q5dUB+Dkz7A8S0hZ53uDlKEZHKcfc2AlKLJUTVo0V0CDPWlWNo5LkPQ142zH+96gMTERGv+HxRGtm5BYwqR49Wt4QIHrywNZOX7eTLJVW30NXUFbt45NNlbEzPZu+h3JNuMWGB/N+QtviWd+uAef91tsYZ8jfwrcCCJyIiVUg9cFIhA5Ji+GTBNnLyCwnyP83+Nw07QPsrnTe/HqOhfhOPxSgiIlXv2NYBXZs2oGvTBuU6556BrfhlfQZPfrWK5GaRJETVc2tMS7bt58EJS+nRLIKP7+h9+vepMzmYBrNehraXQstB7gtSROQsqQdOKmRgUiy5BUXM25x55sqDn3FWpPzxmaoPTEREPGrWxgw2Zxyu0HwyXx/Dy9d1wRh4YMISCgqLOJxbwOwNGfxr+npufGc+HZ78jgH/+JmHJy5j/K/b2JiejbWn2cLGZfu+I9z5QQoNw4N46+YeZ5e8HdgG71/m/HzR85VvR0SkCqgHTiqkd/NIgvx9mLluL4PONIcgIhH6/A5mvwy97oL4ZI/EKCIiVW/snC1EhwYytFOjCp0XH1GPF4Z34r5xSxj0zxnsPJBDYZHFx0C7RuEM796E9KxcZqxL5/PFaQBE1POnZ2IkI3snMLBNzCnz57Jy8rlt7ELyCooYf1dPokIDK//E9q6HD690pgHc/CVEnmbOt4iIFyiBkwoJ8velT4soZqxLBzqc+YRzH3I29v7ucbh9ujY/FRGpBbZkHObndXu5/4LWp2wdUB6Xd2nMqh0HWZ52kOFdm5CcGEm3hAaEFdtY21rLlozDpKTuZ2HqPn7ZsJfvV++hbVwY9wxsybBOjfDz9SG/sIjffbyYLRmH+eC2XrSKDa38E9u5FD66CowPjJ4CcZ0q35aISBVRAicVNjAplp/XrSI14zCJ0SGnrxwYBhc8CV/9DlZ8Bp2v8UyQIiJSZT6Yl4qfj+HG3gmVbuOJoe1OW26MoUVMKC1iQrm2Z1PyCor4etlO3py5ifvHL+Uf09Zx13ktWL0zi1kbMvj7iM70bXUWy/xvnQufXAdB9eGWryCqZeXbEhGpQpoDJxU2MCkGwNULVw5dboC4zvDDU5B3pAojExGRqpaTX8hnKWkM69yI2PCytw5wtwA/H0b0iGfaA+fx9i3JxIQF8uRXqxi/cDu/HdiSa5PL3ofujNZ/Dx8Oh7A4uG2akjcRqdaUwEmFNYsKoXl0CDPWl2M7AQAfH7jkRcjaAXP/U7XBiYhIlZq9IYNDuQVc1T3eK4/v42MY3L4hX9zTlwl3ncNzV3TgkYuSKt9g6mwYPxJikuDWb7VqsohUe0rgpFIGJsUwb1MmOfmF5TuhWV9nW4E5r8DBqtv/R0SkujDGDDHGrDPGbDTGPF5GnWuNMauNMauMMZ94OsbK+GHNHkID/TinRaRX4zDG0LtFFDf3ScSnvPu6lXRgO0y8BSJbwC1fQ8hZDMEUEfEQJXBSKRXaTuAYbSsgInWEMcYXeA24BGgPjDTGtC9RpzXwBNDPWtsBeMDjgVZQUZHlhzXpDEiKIdDvLJbprw7yj8KEG6EwH67/BILLt5ediIi3KYGTSim+nUC5HdtWYPkESEupsthERKqBXsBGa+1ma20eMB64okSdO4HXrLX7Aay15ZxY7D3L0g6QkZ3L4HYNvR3K2bEWJt8Pu5bDVW9DdGtvRyQiUm5K4KRSTt5OoALOfQhCYuD7PztvoCIitVMTYHux+2muY8W1AdoYY+YYY+YbY4Z4LLpK+mHNHnx9zPHFrGqs+a87XyYO+gMkVfuXXUTkJErgpNIGJsWSmnmE1IzD5T8pMAwG/B9smwvrv6u64EREqj8/oDUwEBgJvG2MOWUcnzHmLmNMijEmZe/eCox6qAI/rE6nZ2IEDeoFeDWOs7J5Jnz/J2h7KZz7iLejERGpMCVwUmkV3k7gmB6jIbIl/PC0MydORKT22QEUX9c+3nWsuDTga2ttvrV2C7AeJ6E7ibX2LWttsrU2OSbGez1f2zKPsG7PIQa3j/NaDGdt/1b4dLQzZHL4G84qySIiNYz+55JKq/B2Asf4+jube+9dC0trxKJrIiIVtRBobYxpbowJAK4Hvi5RZxJO7xvGmGicIZWbPRlkRUxfsweAC9vFejmSSjqcAeNGOl8cXv+JMyJERKQGUgInZ2VAmwpuJ3BM+yugSTL8/Bdt7i0itY61tgC4F5gGrAEmWmtXGWOeNcZc7qo2Dcg0xqwGfgYetdZWYGlfz/ph9R7aNAylWVRI5RspKoStc2HaH+HV7vDuxbBzqfuCLMvBNHhvCOzbBNe+r426RaRG8/N2AFKzDUyKYezcVOZvzmRgUgW+lTUGBj8LY4fCgjecxU1ERGqR/2/vzsOjrM7/j7/vTPY9kLAGwr5vAgKC4q5YFdyqWGu1tVXb2rp00S4/tVqrta2239baqqVqa2utVYuKte5oEQUV2TfZwxYIk20mySQ5vz+eQcImASaZJZ/Xdc018yzzzH1k5OGec859nHOzgFn77Lu12WsH3BR+xLSKQIj315VzzeQ+Bz/JOdi5Ghpq9z/m3wDLZ8HKlyCwE3yp0OsE2LoIHj4Zxl/rFRRpjV6xnZ/A49OgtgIuf9Zbl1REJI4pgZOjMqFPR9KSk3hzRdnhJXAAJvBGVwAAIABJREFUvSbBgCnwzq+9eXGZ0V0UVkREDuzNldtpbHKcPuQgyweUr4VZ34XVrx78Iml5MOAMGHQ29DvNS9aCfm9t0Lm/h6X/hrPuhcHnRC7wrYvgL+eDa4IrnoduoyJ3bRGRKFECJ0clPcXHxL4deX35dm47dwhmdngXOO12eHAizP4lTPlZa4QoIiJH6b9Lt1GYncbI4n2KZDaGYM5v4a2fQ1Ky93d6x377XyA9H3qMh+R9qldm5MM598PIS+H5G7yFtQeeDWf+FDp8Rm9fS2x4D/72eUjNhsufg6IBR3c9EZEYoQROjtqZQ7twyzOLWLK5kmHd8w7vzZ0Gw6jLYN7DMP4aKChpnSBFROSI1Dc08daKMs4Z0ZWkpGY/0m2Y6yVdZcu8kvxn3Qt5+y5110I9xsE1b3k9cW/cDb8dA0POg+NvgK4jD+9aQT8smwkv3Qw5XeFLz0F+zyOLS0QkBqmIiRy1M4Z2wZdkvLhoy5Fd4OQfgvng9Z9GNjARETlq763dSXVdA6cNDg+fdA5mfR9mnAl1VTD97zD9iSNP3nbzpcCk6+H6BTDxW7DqFfjjZPjLBbB2tve5B1NRCu8/7M11+0VfmPktb6mAr/xHyZuIJBz1wMlR65CVysS+HZm1aAvfP3Pg4Q+jzO0GE74O79znTWQvHtM6gYqIyGF7dek20lOSmNSv0Nux8X14/48w+ktw5t2Qlh3ZD8zp4hW5Ov4mmD/D65V77FzoMgLyivc/v7IUtnzsve7YD467zptn132s1nkTkYTUor/ZzGyGmW03s8UHOW5m9n9mttrMFprZ6GbHrjCzVeHHFZEKXGLL2cO7sn5ngCWbK4/sAsffCNld4MUbtbi3iEiMcM7x6rLtnNC/iIxUn7dzwV8hJRPO/Fnkk7fmMvK9CsU3LIKz74PkdKjYuP8jNRtOvQ2+OQ++9QGc/hNvSKaSNxFJUC3tgXsU+B3w+EGOnwX0Dz/GAw8C482sA3AbMBZwwAdmNtM5t+togpbYc+bQLvzoucW8uGjL4c+DA0jPhTPvgn9d5f3iOu5rkQ9SREQOy7ItVZT6g1x/an9vR30AFj/rzU9rq4WwUzLg2Ku8h4iItKwHzjk3Gyj/jFOmAY87z1wg38y6AmcCrzjnysNJ2yvAlKMNWmJPQXgY5YsLt+A+a57CZxl2IfSeDK/fCdVlkQ1QREQO2ytLt2EGJw8KLxOz7Hmor4JRX4huYCIi7Vikxhd0BzY2294U3new/fsxs6vNbL6ZzS8r0z/e49E5I7qyofwohlGawed+5f3C+8qthz5fRERa1exVZYwszqcoJ83bseAJyC+BkknRDUxEpB2LmQHizrmHnHNjnXNji4qKoh2OHIEzhnjVKF9YeITVKMFbp2fidfDx32D9u5ELTkREDtu2ylr6FGZ5G/4NXjXIUZdpfpmISBRF6m/gUqBHs+3i8L6D7ZcEVJCVyqR+hcxadBTDKAEmfw/yesCL34HGhsgFKCIih6UiECIvM8XbWPB3wMHI6VGNSUSkvYtUAjcT+FK4GuUEoMI5twV4GTjDzArMrAA4I7xPEtTZw7sc3TBKgNQsmHI3bF/ilaoWEZE2F2psoqqugfyMVGhq8oZP9p4MBSXRDk1EpF1r6TICfwfeBQaa2SYzu8rMrjWza8OnzALWAKuBh4FvADjnyoE7gXnhxx3hfZKgzhjSheSjHUYJMOgc6Hc6vHE3VB7ltURE5LBVBkMA5GemwIY54F8Po74Y5ahERKRFywg45y49xHEHfPMgx2YAMw4/NIlHBVmpTAwPo7x5yhEs6r2bGXzuXnhgAvznFvj8o94+ERFpExXNE7gFf4PUHBh8bpSjEhERzUKWiDtnuFeNcnHpUQyjBOjQB078Hix9Dj58LDLBiYhIi/jDCVyH5HpY8hwMOx9SM6MclYiIKIGTiDtjaGeSk4wXF0Vg6OPxN0HfU2DW96D0w6O/noiItEhFwEvgSra9CqEar/qkiIhEnRI4ibj8TK8a5YuLNh9dNUqAJB9c8Ahkd4anroCAplCKiLQFf7AegE6fPA0d+0GP8VGOSEREQAmctJKzh3dlY3nw6IdRAmR1hIsfg+qt8MzXvGpoIiLSqvyBED1tG+mb58KoL2gesohIjFACJ61i9zDKZz7aFJkLdh8DU+6B1a/C7Hsjc00RETkofyDERb7ZOEuCEVr7TUQkViiBk1aRn5nK1JHdeOK9DWwsD0TmomO/AiMvhTfvgVWvRuaaIiJyQBXBEKclL8BKJkFe92iHIyIiYUrgpNV8b8pAkgzueWl5ZC5oBmffB52HwjNfBf+GyFxXRET24w/U08GqIa9HtEMREZFmlMBJq+mal8G1J/blxUVbeH9thIqPpGbCxY9DUyP844tQH6HePRER2Ys/GCKHGkjPi3YoIiLSjBI4aVXXTO5L17x07nhhCU1NR1mRcreOfeGCh2HLQnj+23C0lS5FRGQ/lTW1ZLkAZORHOxQREWlGCZy0qoxUHzdPGcTi0kr+9WGECpoADJwCp/wYFv0T5vw2ctcVEREAGoIV3gv1wImIxBQlcNLqpo7sxqge+dz78gpq6hoid+ETvgNDzoNXb/OqU4qISMQ0BfzeCyVwIiIxRQmctLqkJOPWc4dQVlXHg29+ErkLm8F5v4dOQ+Dpr8DOCF5bRKQda2pyWJ164EREYpESOGkTo3sWMG1UNx56ew2bdkWw8EhqFkx/AswHT34B6qoid20RkXaqqq6BbMJ/V6drDpyISCxRAidt5uYpgyK7rMBuBb3g4sdgxyp45hpoaors9UVE2pmKQIg8arwN9cCJiMQUJXDSZrrlZ3D15L68sHALH27YFdmL954MZ/4MVrwI//2xKlOKiBwFf7CeXFMCJyISi5TASZu6ZnIfslJ9PDVvY+QvPv4aGH8tzH0AXv9p5K8vItJO+AMhcj8dQqkETkQkliiBkzaVlZbMGUO78NLirdQ3RHiooxlMuQdGXwFv/xJm/yKy1xcRaSf8wRC5VoOzJEjLiXY4IiLSjBI4aXPnjuxKRTDE7JVlkb+4GZzzaxgx3euFm/O7yH+GiEiCqwjUk0cNLi3P+3tVRERihhI4aXPH9ysiPzOF5xdubp0PSEqCaQ94a8T990fw/sOt8zkiIgnKHwiRawFMwydFRGJOcrQDkPYnNTmJs4Z15d8LSgnWN5KR6ov8h/iS4cJHoKEOZn0XktNh9OWR/xwRkQRUEQwxPCmIZWgJARGRWKMeOImKqSO7Eahv5NVl21rvQ3wp8PlHoe8pMPNb8NYvtMSAiEgL+IMhCpICKmAiIhKDlMBJVIzr3YFOOWnM/LiVhlHulpIOlzwBwy+CN34Kf7sYAuWt+5kiInHOHwiRb0rgRERikRI4iQpfknHOiG68taKMimCodT8sNRMueBjOvg/WvgV/OAE2zW/dzxQRiWMVwXpyqFECJyISg1qUwJnZFDNbYWarzeyWAxy/38wWhB8rzczf7Fhjs2MzIxm8xLepo7pR39jEy0u2tv6HmcGxV8FXXvaKnMyYAu89pAW/RUQOwB8IkeVqQHPgRERiziETODPzAQ8AZwFDgEvNbEjzc5xzNzrnRjnnRgG/BZ5pdji4+5hzbmoEY5c4N7I4j54dMnm+tYdRNtd9NFwzG/qdCi99D57+MgT9h36fiEg7Uh0IkOZq1QMnIhKDWtIDNw5Y7Zxb45yrB54Epn3G+ZcCf49EcJLYzIxzR3blf6t3UFZV13YfnFEA0/8Op90OS2fCH46H9XPa7vNFRGKYcw4XrPA20tUDJyISa1qSwHUHNjbb3hTetx8zKwF6A683251uZvPNbK6ZnXfEkUpCmjqyO00OZi3a0rYfnJQEx98IV/0XkpLh0bO9hb8bW3k+nohIjAuGGsloqvY21AMnIhJzIl3EZDrwtHOusdm+EufcWOALwK/NrO+B3mhmV4cTvfllZWURDkti1cAuOQzsnNO2wyibKx4L174NI78As3/hzY0rXxOdWEREYoA/ECKXGm9DPXAiIjGnJQlcKdCj2XZxeN+BTGef4ZPOudLw8xrgTeCYA73ROfeQc26sc25sUVFRC8KSRDF1VDfmr99FqT8YnQDScuC8B7w143au8qpUzp8BTY2HfKuISKLxB0Lk2e4ETj1wIiKxpiUJ3Dygv5n1NrNUvCRtv2qSZjYIKADebbavwMzSwq8LgUnA0kgELonjnBFdAaLXC7fb0PPh63Og2zHwwo3wxxNh7ezoxiQi0sb8wXpyCXgbSuBERGLOIRM451wDcB3wMrAMeMo5t8TM7jCz5lUlpwNPOrdXXfbBwHwz+xh4A7jHOacETvZS0jGLkT3ymbkgygkcQF4xXPE8XPRnqK2Ax86FJy+DnZ9EOzKR2BEKwo7VULUt2pFIK6gIhMg1JXAiIrEquSUnOedmAbP22XfrPtu3H+B9c4DhRxGftBPTRnbjjheW8sH6csaUdIhuMGYw7AIY+DmY+wC8fR88MB4mXAsTvw3ZnaIbn0hramyA6q1QuRkqNkFlKVSUQsVGb7tiEwR2eOee8mOY/L3oxhvDzGwK8BvABzzinLvnIOddCDwNHOucm9+GIR6QP9hsDpzWgRMRiTktSuBEWtslx/bg4bfX8OPnlvD8dZNI9kW6vs4RSEmHE74Doy6D1++EOb+Ddx+A3pNh2EUw+Fz940bih3MQ3AX+DV4yVr0Nqrfv/Vy1Faq2gGva+70pWZDfw+uh7jbKe87rAd3HRKctcaDZGqqn41VvnmdmM/cdhWJmOcD1wHttH+WB7Z4D53ypWHJ6tMMREZF9KIGTmJCVlsxt5w7h2r9+yKNz1vHVE/pEO6Q9crrAtAdg4vWw6ClY9DTMvA5evAn6nQbDLoQBZ3rFUESiqbYCdq0H//o9z/4Nex711fu8wSCrELI7e4/CgZDXHXLDj7zukNvNq0RoFpUmxbFP11AFMLPda6juO43gTuDnQMx0ZfqD9fRKCnrDJ/XnLiISc5TAScw4c2gXTh5YxP2vrOTsEV3pmpcR7ZD2VjTAGzJ28o9g84ew+BnvsWIW+NKg36kweCoMnOItFi4SKY0NUFfpPWp2NhvO2OzZv8HrYWsuLRfyS6CgN/Q+0etFy+/p9aDldIXMQvDpNtBKDrSG6vjmJ5jZaKCHc+5FM4uZBK4iEKKjL4hp/puISEzSnVtihpnxk6nDOP3+t/jpC8t44LLR0Q7pwMy8oWPdx8Dpd8LGubB0Jix73kvmkpK9fywPPhf6n+H1Yog0Vx8Izy3b6M0vC+yAQDkEyyGwy3sO7vJ61GorIVRz4OukZoeHMxZDt9FQ0Cv8KPGe9UNCzDKzJOA+4MoWnHs1cDVAz549WzcwvCGUBb6A1oATEYlRSuAkpvTsmMl1J/fjV6+s5OKVZZw4IMbXBExKgpKJ3mPK3VD6ISz7t5fQvXCDd06nodD/NOh3OvScAL6U6MYskdVQB/U13vDEumrvda0fanZ4iVnNjj2vdxcGCZbvfx1fGmR2gIwO3nNhf28IW3q+15OWnus9Z3bck7RpiFssO9QaqjnAMOBN8/4MuwAzzWzqvoVMnHMPAQ8BjB07tnml51bhD9aTbwFIL27tjxIRkSOgBE5iztUn9uHZBaXc+u/FvHzDZNJTfNEOqWXMoHiM9zjtJ1C2HFa9AqtfgXd/D//7jfcP8B7jofMQ6DQEOg2GwgGQEmPDRQUaQ14P2a513vDEyi1edcaqbXuea8qgKfTZ1/GlekMVszp6wxaLx3rJV244CcvrDllFkJKpZCyxfLqGKl7iNh34wu6DzrkKoHD3tpm9CXw3JqpQBkLkUKMlBEREYpQSOIk5ack+7pw2jMseeY/fv/kJN50+INohHT4zLznrNBgmfRvqqmDNW14yt+kDWPsWNNaHz03y5ih16B0uHFHcrIBEsTdvKTktuu1JFE1N3nDEumqvF6ymDKrLoGb7ntcVG7ykrWLT/tUYMwu9ojY5Xbye1axCr3hNajakZUNqlvc6Pc/rKcsq9JJ2JWbtjnOuwcx2r6HqA2bsXkMVmO+cmxndCA+uIhgiq0kJnIhIrFICJzFpUr9Cpo7sxh/e/ITzRnWjT1F2tEM6Omk5MPgc7wFeUYryNbB9KWxf5j37N8CWj71EojlL8pK6Dn32PAp670nwsgrbR4LQUO8NTQz6veGKoWD4UeM911d7x4K79swnC+7y9tVVeY/6auAgI9CSUryesLxi6DEBRpTsmVOW3xOyu0Byahs2WOJdS9ZQbbb/pLaIqSUqgvVk+Kq1TIqISIxSAicx68fnDOaN5du5/fmlPP6VcdEOJ7J8yV5Vy6IBMPS8vY+FaqFqc3jx5E2wa62X7JWvgSXP7l9p0JfmlXrPK/aeszvtKQu/+3VaDiRneD15yenRqTzoHIQC3hyxuqo988bqa7xiHYGde+aKBXZ61RaD5V4CVuv33tsSKZnheWQFXhGPooHe/LHUHO+/Q1qO11uWnu/998nqBNlFKpUvAtQ1NNJUHyQ5PaQeOBGRGKUETmJWp5x0vnlKP+55aTlLNlcwtFs7+cdESvqenrYDCZR7Q/wqS70kr3JT+LkU1r/rLcjcWPfZn5GUHE7kUps9Urzn5FRv2F9azp7n9FwvATQA83oFzbzXDXVQH+7hqqsOF/Oo2ruox+5k7WC9X58yr4BHZqE3BLFDHy+xygg/0vO9pCw125s3mJLpPadmeq/T873/fiJyRCqCIXIJ/1iiBE5EJCYpgZOYdumxPfnNq6t4bM467r1oZLTDiQ2Z4SqF3Q+yzIJz3nph1du9ZK56m5dINdSGH3Xec6jWm4fXWO8V7Gis9wpyhGq9ZMu/EeoqvGSsthJc48FjSs7werU+nQ+W4w057JAVnhu2e35Y1p7jzV+n5XhJW0Y+JMVJ0RqRBFQRCJFr4WUrtIyAiEhMUgInMS0vM4ULRnfnnx9s4pazBtMhS3OQDsksXH4+zytFHwnOQVOD94xr9tzkDeHUYtAiCcEfDJHH7gROPXAiIrEoKdoBiBzKFRN7Ud/QxN/f3xDtUNovM2+IZXKqN48uJT08dDFLyZtIAvEHQuTa7iGU6oETEYlFSuAk5g3onMOkfh3569z1NDQ2HfoNIiJyRPyBenLVAyciEtOUwElcuHJib7ZU1PLfpduiHYqISMKqCDbrgdMyAiIiMUkJnMSFUwZ1okeHDB7937pohyIikrD8gRAFu4uYpOVGNxgRETkgJXASF3xJxhXH9eL9deUs2VwR7XBERBKSP1hPYUqttyyHFq4XEYlJSuAkbnx+bA8yUnw8NmddtEMREUlI/kCIjr6g5r+JiMQwJXASN/IyvCUFnluwmfKa+miHIyKScCqCIQp8QVWgFBGJYUrgJK5cqSUFRERajT8QIt9q1AMnIhLDlMBJXOnfOYfj+xVqSQERkVbgD9aTQ0AJnIhIDFMCJ3Hnyom9tKSAiEgr8AdCZDVVK4ETEYlhSuAk7pw8qBMlHTO5/5WV1IYaox2OiEhCaGhsoqq2gYymGq0BJyISw1qUwJnZFDNbYWarzeyWAxy/0szKzGxB+PHVZseuMLNV4ccVkQxe2idfkvGTqUNZtb2a37y2KtrhiIgkhMraBowm0hqq1AMnIhLDDpnAmZkPeAA4CxgCXGpmQw5w6j+cc6PCj0fC7+0A3AaMB8YBt5lZQcSil3brpIGdmH5sD/741id8tGFXtMMREYl7/kA9WdSSRJMSOBGRGNaSHrhxwGrn3BrnXD3wJDCthdc/E3jFOVfunNsFvAJMObJQRfb2o7MH0yU3ne/882MNpRQROUr+YIhcAt6GEjgRkZjVkgSuO7Cx2fam8L59XWhmC83saTPrcZjvFTlsOekp3HvRSNaU1fCr/66IdjgiInGtIhAiz2q8Da0DJyISsyJVxOR5oJdzbgReL9tjh3sBM7vazOab2fyysrIIhSWJ7vj+hVw2viePvLOW+evKox2OiEjcqlAPnIhIXGhJAlcK9Gi2XRze9ynn3E7nXF148xFgTEvf2+waDznnxjrnxhYVFbUkdhEAfvC5wXTPz+C7//yYYL2GUoqIHAl/oJ7cT3vglMCJiMSqliRw84D+ZtbbzFKB6cDM5ieYWddmm1OBZeHXLwNnmFlBuHjJGeF9IhGTnZbMvReNYN3OAPe+vDza4YiIxKW95sBpGQERkZiVfKgTnHMNZnYdXuLlA2Y455aY2R3AfOfcTODbZjYVaADKgSvD7y03szvxkkCAO5xzGucmETexbyFXHFfCn/+3jhP6F3LKoM7RDklEJK74AyE6pwa9DfXAiYjErEMmcADOuVnArH323drs9Q+AHxzkvTOAGUcRo0iL3HzWIOat28XVj3/Ary4eybRRqpcjItJSFcEQJcm13k+xabnRDkdERA4iUkVMRKIuMzWZJ6+ZwJiSAq5/cgF/emdttEMSEYkb/kA9HZNrveQtyRftcERE5CCUwElCyU1P4bGvjGPK0C7c+cJS7nlpOc65aIclIhLz/MEQBUkBLSEgIhLjlMBJwklP8fHAZaO5bHxP/vDWJ3zv6YU0NDZFOywRkZj26Tpwmv8mIhLTWjQHTiTe+JKMn543jKKcNH796ip21dTz+y+OJi1Zw4JERA7EHwyRkx5QAiciEuPUAycJy8y44bQB3HneMF5bvp0H3vgk2iGJiMSkpiaHP1BPlqtWAiciEuOUwEnCu3xCCeeN6saDb65mxdaqaIcjIhJzqusbaHKQ0VStNeBERGKcEjhpF249dyg56Snc/K+FNDapqImISHMVgRAA6Q1V6oETEYlxSuCkXeiQlcqt5wxhwUY/j81ZF+1wRERiij8QwkcjKQ0qYiIiEuuUwEm7MW1UN04aWMQv/7uCjeWBaIcjIhIz/MF6sgl6G0rgRERimhI4aTfMjLvOH44BP3pusdaHExEJ8wdC5FqNt6F14EREYpoSOGlXuudn8P0pg5i9soxnPyqNdjgiIjHBHwyRx+4ETj1wIiKxTAmctDtfnFDC6J753PHCUnZU10U7HBGRqKsI1JNr4aHlSuBERGKaEjhpd3xJxs8vHEGgrpFb/71YVSlFpN3bFQhR6Kv1NpTAiYjENCVw0i7175zDjacPYNairVz55/fZVVMf7ZBERKJm3Y4aemWH/x7UOnAiIjFNCZy0W18/qS93XzCc99aUc85v32FxaUW0QxIRiYrlW6vok93gbagHTkQkpimBk3bt0nE9eera42hyjgsfnMPTH2yKdkgiIm2qsjZEqT9IcUYILAlSs6MdkoiIfAYlcNLujeqRz/PfOp7RPQv47j8/5sfPLaK+oSnaYYmItIkVW6sA6JJW6/W+mUU5IhER+SxK4ESAwuw0/nLVOK6Z3Ie/zt3AVY/NI9SoJE5EEt/ycALXISmoNeBEROKAEjiRsGRfEj/43GDuuWA4b6/awY+f1WLfIpL4lm+pJCc9mYymas1/ExGJA8nRDkAk1kwf15NSf5Dfvr6aXoVZfP2kvtEOSUSk1SzfWsXgLrlYbYUSOBGROKAeOJEDuOn0AZw7shs//89yZi3aEu1wRERahXOOFVurGNQ1B2ortISAiEgcUAIncgBmxi8uGsGYkgJu/McCPtywa79zauoaeGzOOq788/ts2BmIQpQiEsvMbIqZrTCz1WZ2ywGO32RmS81soZm9ZmYlbR3jpl1BqusaGNglB4J+9cCJiMQBJXAiB5Ge4uOhy8fQOTedqx+fz8ZyL0nbtCvAXS8uZcLdr3HbzCXMXlnGD59dpPlyIvIpM/MBDwBnAUOAS81syD6nfQSMdc6NAJ4G7m3bKPcUMBnUJdfrgVMCJyIS81qUwB3Nr4hm1mhmC8KPmZEMXqS1dcxOY8aVx1Lf0MSXH53HN574gMn3vsGM/63jpIGdeOYbE7l96lDeWb2DmR9vjna4IhI7xgGrnXNrnHP1wJPAtOYnOOfecM7t7r6fCxS3cYys2FoJwMDCVGgIKoETEYkDhyxi0uxXxNOBTcA8M5vpnFva7LTdvyIGzOzreL8iXhI+FnTOjYpw3CJtpl+nbP5w+RiumPE+ZVV1XD25L186roRu+RkAjCzO518fbOLOF5Zy0oBO5GWmRDliEYkB3YGNzbY3AeM/4/yrgJdaNaIDWLa1ih4dMsjenUdqGQERkZjXkh64uPgVUaQ1TexbyBvfPYl3f3AKt5w16NPkDcCXZNx1/nDKa+r5+cvLoxiliMQjM/siMBb4xUGOX21m881sfllZWUQ/e8XWqvDwSb+3QwmciEjMa0kCd6BfEbt/xvn7/oqYHr7xzDWz844gRpGYUFyQSWbqgTuth3XP48uTevO39zbwwfr9C56ISLtTCvRotl0c3rcXMzsN+BEw1TlXd6ALOececs6Ndc6NLSoqiliAtaFG1pRVM6hLuAIlaAiliEgciGgRk4P8iljinBsLfAH4tZkdcFGt1vyFUaQt3HT6ALrmpfOjZxcRamyKdjgiEl3zgP5m1tvMUoHpwF7zwM3sGOCPeMnb9rYOcPX2apoc+/TAKYETEYl1LUngjupXROdcafh5DfAmcMyBPqS1fmEUaStZacncPnUoy7dWMeOdtdEOR0SiyDnXAFwHvAwsA55yzi0xszvMbGr4tF8A2cA/o1Ho69MKlF2b9cBpHTgRkZh3yCImNPsVES9xm47Xm/apZr8iTmn+K6KZFQAB51ydmRUCk4hCmWSRtnLm0C6cNrgz97+6ks8N70qPDpnRDklEosQ5NwuYtc++W5u9Pq3Ng2pm+ZZK0pKT6NUxC9apB05EJF4csgfuKH9FHAzMN7OPgTeAe/apXimScH4ybShJZtzyzEI2+4PRDkdE5ICWb61iQOccfEmmOXAiInGkJT1wR/wronNuDjD8aAIUiTfd8zP4wecG8/+eW8ykn7/OCf2LuGRsD04b0om0ZF+0wxMRAbwE7uSB4SkLtRXgS4Xk9OgGJSIih9SiBE6V1OdfAAATVklEQVREDs/lE0o4aUAR/5y/kac/2MQ3//YhBZkpnHdMd744oYS+RdnRDlFE2rEd1XXsqK5jYJccb0f5GsjpCmbRDUxERA4polUoRWSPHh0yuemMgbx98yk8/pVxTOxXyBNzN3Dm/bO59z/LqQ01RjtEEWmnVoQLmAzumgvOwYZ3oeeEKEclIiItoR44kVbmSzImDyhi8oAidlTXcc9Ly/n9m5/wwsIt/PS8YUwecPCqq845TL+Ii0iELdtSCeCtAVe+BmrKoOdxUY5KRERaQgmcSBsqzE7jl58fyYWji/nRs4v40oz3mTaqGz8+ewhFOWlsqQgyf90uPli/i/nry/lkew2fH1vMd84YSF5GSrTDF5EEsXxrFYXZaXTMToOVc7ydJROjG5SIiLSIEjiRKDiub0deuuEEHnzzE37/xie8sXw7OekplIarVmak+BjVI5/Th3Tmr3PXM2vRVv7fOYOZOrKbeuRE5Kit2FrF4K7h+W8b3oXMjlA4ILpBiYhIiyiBE4mStGQfN5w2gHNHduOXL68gyYyrju/N2F4FDO6aS4rPm6J69eQ+/OjZRVz/5AKemr+RO6YN26sIyvaqWpZurmTplkq652cwbVT3aDVJROJAQ2MTK7dVcfmEEm/H+jne8En9OCQiEheUwIlEWd+ibB784piDHh/WPY9nvjGJv72/gXv/s5yzfv02U0d1Y3tVHUs3V7Kjuu7Tc82gW34Gx/bq0Bahi0gcWrczQF1DE4O65kLVVti1Fo79arTDEhGRFlICJxIHfEnG5RNKmDK0Cz+btYwXF26hd2EWJw0sYmi3XIZ0zaWkYxaf/+McvvPUx7x0/Qlkpel/bxHZ3+4KlIO65MD617ydKmAiIhI39C88kThSlJPG/ZeM4r6LD1yd8lefH8UlD73Lz2Yt467zh0chwiPT2OTwJWn4lkhbWL61El+S0a9TNnw8F1IyoeuIaIclIiItpHXgROLQwQqZjOvdga8e35sn3tvAWyvL2jiqI3P3S8sY/7PX+HijP9qhiLQLy7ZU0bswi/QUH2yYA8XHgk9VbkVE4oUSOJEE850zBtK/Uzbff/pjKgKhA57jnGPhJj+l/iDOuTaOcI8FG/08NHsN/kA9X3h4LnNW74haLCLtxYptld7wydoK2LpYyweIiMQZJXAiCSY9xcd9F49iR3U9t81cvN/xeevKufDBOUz93f+YdM/rjPvZa3z1sfk88MZq3lm1g8raAyd9B7Krpp6GxqYjijPU2MQt/1pI55x0Xr5xMsUFmVz553m8vGTrEV1PRA6tuq6BjeVBL4Hb+D7gNP9NRCTOaA6cSAIaXpzHdSf34zevreLMoV04a3hXVm+v5t7/LOe/S7fRKSeNn0wdCsDHG/0s2Ojn1WXbAEgyGNkjn8n9i5g8oIiRxXkkh5c0cM6xZHMlry3bzmvLt7FwUwV9i7J4/KrxdM/POKwYH3l7Lcu3VvHQ5WPoW5TNP66ZwJcfncfX//oB91w4govH9ojsfxQRaVbAJNdbPiApGYrHRjkqERE5HErgRBLUdaf04/Xl2/nhs4uYvaqMp+ZvIiPFx3dOH8BVJ/QmM3Xv//0rAiEWlvqZt24Xs1eW8X+vr+I3r60iNz2Z4/sXkpeRwhvLy9haWYsZjOqRzzdO6stf5q7nogfn8PhXxtG/c06LYlu/s4Zfv7qSM4d25oyhXQDIz0zlr1eN59q/fsD3n15IZTDEV0/oE/H/LiLt2ZaKICk+Y1DXHJg7F7qOhNSsaIclIiKHwaI5/+Vgxo4d6+bPnx/tMETi3qptVZz923doanJcNr4n3zq1P4XZaS16rz9Qzzurd/DWijJmryqjuraBE/oXcergTpw8qNOn11m6uZIr/vw+ocYmZlx5LKN7FnzmdZ1zXP6n971ev5tOpEte+l7H6xoaufEfC5i1aCtfntSLm6cM8ootSEIysw+cc+oCaqFI3B9DjU0kN9Vj9/SEcV+DM++KUHQiIhJJB7tHqgdOJIH175zDM1+fSE56MiUdD+9X9vzMVM4Z0Y1zRnTDOYdzkHSAUv9DuuXyr2sncvmM97js4ff4/RdHc/LATge97nMLSnln9Q7unDZ0v+QNIC3Zx28vHU2nnKX8+X/reHNFGXdfMJwJfToeVvwbdgb42axlLN5cwTUn9uUL43pqqQIRIMWXBJs+gsY6FTAREYlDKmIikuCGdc877ORtX2Z2wORtt54dM3n62on0Kcria4/N57mPSg94XnlNPXe+sIxjeuZz2fiSg17Pl2TcPnUoT3x1PI1NjukPzeWHzy5qUYGVmroGfvHyck67/y1mryqjY1Yq/++5xZzz23d4b83OQze2DdWGGvn3glIWl1ZEtRqotEMb5njPKmAiIhJ31AMnIhFRlJPGk1dP4OrHP+CGfyzgT++spX/nbAZ0zmFA52z6d8rh/ldXUhkMcfcFwz8zIdxtUr9CXr5hMve9soI/vbOW15dt567zh3Hq4M77neuc47kFpdzz0nK2VdZx/jHduXnKIDrnpvHS4q3c9eIyLnloLueO7MYPzhpEt8MsuhJpry/fxu0zl7KhPABAzw6ZnDW8C58b1pURxXkHXetPJCLWvwtFgyCzQ7QjERGRw6Q5cCISUbWhRn7/xmo+2uhn5bYqtlXW7XX8myf35XtnDjrs6y7Y6OfmpxeyYlsVGSk+ctKTw48UctKTKa+pZ8nmSkYU53HbuUMZU7L3XLxgfSN/eOsT/vDWJySZccqgTjQ5R6jR0dDURKixiYZGR+fcdAZ2yWFQlxwGdsmhe35GRJOpjeUB7nhhKa8s3Ua/Ttn84KxB7KiuY9airfxv9Q4amhzd8zOYMqwLJw4oYlzvDoc9B7C+oYm3VpYxe2UZU4Z1YVK/wojFH2maA3d4InJ/bGqEn/eCYRfCub+OSFwiIhJ5B7tHKoETkVZVEQixansVK7dVsytQz1XH9z7ioiT1DU38Y/5GNuysoaq2garaBiprQ1TVNtDY5Lj8uBIuGl38mb17G8sD3PvyChZt8pPiSwo/jGRfEr4ko3RXkFJ/8NPzc9KSGdAlh+Hd8zimZz4ji/Mp6Zi5X1IXamxi064ga3dUU13XSF5GCvkZKeRnppCfkUpaShIPz17D795YjS/JuP7U/nx5Um9Sk/eMZK8IhHhl2TZeWrSFt1ftoL6xidTkJMb16sDx/Qs5oX8hg7vkHrB9zjk+3ODnuY9KeWHhZnYFQviSjCbn+MZJfbnxtAGfLgcRS5TAHZ6I3B+3LoI/HA/nPwQjL4lMYCIiEnFK4EREWqiqNsTKbVUs21LFiq1VLNtSyZLNlQRDjQDkZaQwskc+JR0y2bQrwLqdATaWB2hoOvTfp2eP6MqPzx5M17zPHsIZqG/gvbXlvLNqB2+vKmPltmoAUpOTyM9IIS/8yM9MISc9hY827GLdzgBpyUmcMbQLFxzTnTG9CrjrhWX8Y/5GxpYU8H+XHhP1oaP7UgJ3eCJyf3zvIXjpe3DDIsjvGZnAREQk4pTAiYgchYbGJlZuq+bjTf5PFz8v3RWkuEMmfQqz6FWYSa+OWfQpyiI3PYWKYAh/IOQ9B0NUBkOM792BiUc4nHFbZS1vr9rBym1VVISv2/zaJR0zOf+Y7kwZ1oWc9JS93vvvBaX88JlFpCQn8YuLRnL6kP3nEEaLErjDE5H74z+vhI3z4KYlEYlJRERah5YREBE5Csm+JIZ0y2VIt1wuHdf2vRadc9O5aEzxEb132qjujCjO57q/fcjXHp/PZeN7MrFvIR2zUynMTqMwO5W8DC/p21Fdz8ZdXo/ipl1BNu0KkJWazNheHRjbq6DF6whKjHLOK2DS+4RoRyIiIkeoRQmcmU0BfgP4gEecc/fsczwNeBwYA+wELnHOrQsf+wFwFdAIfNs593LEohcRkRbpXZjFM9+YyN2zlvPonHU88d6GvY4nJxnJPqM21LTX/o5ZqVTXNfDIO2sB6FOYxZiSAo7t1YGJ/TpSXJDZZm2QCNi1Fqq3avkAEZE4dsgEzsx8wAPA6cAmYJ6ZzXTOLW122lXALudcPzObDvwcuMTMhgDTgaFAN+BVMxvgnGuMdENEROSzpSX7uH3qUK4/tT/bqmrZWV3Pjuo6dlTXs7O6jvqGJooLMujRIZMeHTIpLsggMzWZuoZGFpdWMn9dOfPW7eKVZdv45webuPG0AVx/Wv9oN0sOx4a53rMW8BYRiVst6YEbB6x2zq0BMLMngWlA8wRuGnB7+PXTwO/MK9E2DXjSOVcHrDWz1eHrvRuZ8EVE5HAVZKVSkJXa4vPTkn2MKSlgTEkB15wITU2ONTuqyUrTKPy4M/QCKOgNhQOjHYmIiByhltSU7g5sbLa9KbzvgOc45xqACqBjC98rIiJxJCnJ6Ncp55CVNCUGpaRDyXGQFHtLSoiISMvEzN/gZna1mc03s/llZWXRDkdERERERCTmtCSBKwV6NNsuDu874Dlmlgzk4RUzacl7AXDOPeScG+ucG1tUVNSy6EVERERERNqRliRw84D+ZtbbzFLxipLM3OecmcAV4dcXAa87b4G5mcB0M0szs95Af+D9yIQuIiIiIiLSvhxyBrpzrsHMrgNexltGYIZzbomZ3QHMd87NBP4E/CVcpKQcL8kjfN5TeAVPGoBvqgKliIiIiIjIkWlRCTHn3Cxg1j77bm32uhb4/EHeexdw11HEKCIiIiIiIsRQERMRERERERH5bErgRERERERE4oQSOBERERERkTihBE5ERERERCROKIETERERERGJE+Yt1xZbzKwMWH+UlykEdkQgnFjXHtrZHtoI7aOd7aGNoHYejhLnXFEkgmkPdH88LGpn4mgPbQS1M5FEqo0HvEfGZAIXCWY23zk3NtpxtLb20M720EZoH+1sD20EtVNiW3v5c1M7E0d7aCOonYmktduoIZQiIiIiIiJxQgmciIiIiIhInEjkBO6haAfQRtpDO9tDG6F9tLM9tBHUTolt7eXPTe1MHO2hjaB2JpJWbWPCzoETERERERFJNIncAyciIiIiIpJQEi6BM7MpZrbCzFab2S3RjidSzGyGmW03s8XN9nUws1fMbFX4uSCaMUaCmfUwszfMbKmZLTGz68P7E6atZpZuZu+b2cfhNv4kvL+3mb0X/u7+w8xSox1rJJiZz8w+MrMXwtsJ104zW2dmi8xsgZnND+9LmO8sgJnlm9nTZrbczJaZ2XGJ1sb2QPfI+NUe7o/Qvu6Ruj8mxncW2v4emVAJnJn5gAeAs4AhwKVmNiS6UUXMo8CUffbdArzmnOsPvBbejncNwHecc0OACcA3w3+GidTWOuAU59xIYBQwxcwmAD8H7nfO9QN2AVdFMcZIuh5Y1mw7Udt5snNuVLOywYn0nQX4DfAf59wgYCTen2mitTGh6R4Z99/P9nB/hPZ1j9T9MTG+s9DG98iESuCAccBq59wa51w98CQwLcoxRYRzbjZQvs/uacBj4dePAee1aVCtwDm3xTn3Yfh1Fd7/AN1JoLY6T3V4MyX8cMApwNPh/XHdxt3MrBg4G3gkvG0kYDsPImG+s2aWB0wG/gTgnKt3zvlJoDa2E7pHxrH2cH+E9nOP1P0xcb6z0bhHJloC1x3Y2Gx7U3hfoursnNsSfr0V6BzNYCLNzHoBxwDvkWBtDQ+bWABsB14BPgH8zrmG8CmJ8t39NfB9oCm83ZHEbKcD/mtmH5jZ1eF9ifSd7Q2UAX8OD/d5xMyySKw2tge6RyaIRL4/Qru5R+r+6EmE72yb3yMTLYFrt5xXTjRhSoqaWTbwL+AG51xl82OJ0FbnXKNzbhRQjPer+KAohxRxZnYOsN0590G0Y2kDxzvnRuMNTfummU1ufjABvrPJwGjgQefcMUAN+wwFSYA2SgJLpO9not8fIfHvkbo/7pEg39k2v0cmWgJXCvRotl0c3peotplZV4Dw8/YoxxMRZpaCd3N6wjn3THh3QrY13MX+BnAckG9myeFDifDdnQRMNbN1eEO1TsEbI55o7cQ5Vxp+3g48i/cPjkT6zm4CNjnn3gtvP413s0qkNrYHukfGufZ0f4SEvkfq/phY39k2v0cmWgI3D+gfruKTCkwHZkY5ptY0E7gi/PoK4N9RjCUiwmPA/wQsc87d1+xQwrTVzIrMLD/8OgM4HW8uwxvAReHT4rqNAM65Hzjnip1zvfD+X3zdOXcZCdZOM8sys5zdr4EzgMUk0HfWObcV2GhmA8O7TgWWkkBtbCd0j4xj7eH+CO3jHqn7Y2J9Z6Nxj0y4hbzN7HN444p9wAzn3F1RDikizOzvwElAIbANuA14DngK6AmsBy52zu07iTuumNnxwNvAIvaMC/8h3jj/hGirmY3Am8zqw/sR5Snn3B1m1gfvl7gOwEfAF51zddGLNHLM7CTgu865cxKtneH2PBveTAb+5py7y8w6kiDfWQAzG4U32T4VWAN8mfD3lwRpY3uge2T8fj/bw/0R2t89UvfH+P/OQtvfIxMugRMREREREUlUiTaEUkREREREJGEpgRMREREREYkTSuBERERERETihBI4ERERERGROKEETkREREREJE4ogRMREREREYkTSuBERERERETihBI4ERERERGROPH/AeiLs0a3LmvZAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以看到，GAT模型可以達到很好的效果。"
      ],
      "metadata": {
        "id": "BbYEHJYr1PkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型預測"
      ],
      "metadata": {
        "id": "_mqbNBTR1Wur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#输出模型预测结果\n",
        "output = model(features, adj)\n",
        "\n",
        "samples = 10\n",
        "idx_sample = idx_test[torch.randperm(len(idx_test))[:samples]]\n",
        "\n",
        "idx2lbl = {v:k for k,v in lbl2idx.items()}\n",
        "df = pd.DataFrame({'Real': [idx2lbl[e] for e in labels[idx_sample].tolist()],\n",
        "                   'Pred': [idx2lbl[e] for e in output[idx_sample].argmax(1).tolist()]})\n",
        "print(df)"
      ],
      "metadata": {
        "id": "T_9yX7Mz1O0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5cb48b-b1c6-47f3-a266-e95d0039f43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Real                   Pred\n",
            "0                 Theory                 Theory\n",
            "1          Rule_Learning          Rule_Learning\n",
            "2     Genetic_Algorithms     Genetic_Algorithms\n",
            "3                 Theory                 Theory\n",
            "4                 Theory                 Theory\n",
            "5  Probabilistic_Methods  Probabilistic_Methods\n",
            "6                 Theory                 Theory\n",
            "7  Probabilistic_Methods  Probabilistic_Methods\n",
            "8  Probabilistic_Methods  Probabilistic_Methods\n",
            "9          Rule_Learning          Rule_Learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J7H-q9aH1Zsx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}