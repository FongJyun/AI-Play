{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM3WDqQzRnEmXNqWs6QXwfq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 實作"],"metadata":{"id":"5GBbbHJUd645"}},{"cell_type":"markdown","source":["## 管線方式"],"metadata":{"id":"GoTSQZM6eAx4"}},{"cell_type":"markdown","source":["### 文本分類"],"metadata":{"id":"6QCpdHXjeOvK"}},{"cell_type":"code","source":["from transformers import pipeline\n","nlp_sentence_classif= pipeline(\"sentiment-analysis\") \t#自動載入模型\n","print(nlp_sentence_classif (\"I like this book!\"))\t\t#呼叫模型進行處理"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["c79ab3e13b7a490384eab56e3a39b7cd","4553b1f0011247ab9cfc48dd082bacd5","d496c8b178ae4cd590dee0a6ec05124b","feb78241e96c4a4fbd5ad7955952460a","33ce6cf21d5c407ebb7e96961a3fd8ae","11075e75da3e4b16b32e698017a5dddd","ea512012a18a4ceba8b24f7df822061c","aaaed7807b314970af2cd081cd30900f","6f819dbfc6134a7a88b7812191700e18","c494e07bbcb24dbfbef9bc0053ad9d87","9a14d8162c8d44c09ed7550151ca49d3","b26850760989466587f5dece9e181a80","4ba89f604da1427ba169842dd9ff943e","f9bebc8f6c99412cb6f20bde52ae7429","c1d2a711849c4fcbb6b66268ff026e00","ba09dfc24eba4e0ba1df4a0e4f479076","645fdaa1f6bc45ce94b5eef0586709e6","3459995791014a1fb9f4b6d4ccdbbcd4","d9c6e13064574a1d8ca38d0256bff17b","2bf7ab3fc53c48a48562b98fef478504","5be5720928404a96bab89ac8a837c237","248c0c2d44784040bae147d9b333bff9","b6be75e91b1b4c3ba35fa7d262a37699","788bcb75d85d4eee92420056377ca3a4","75bc065a7cd44249bb1835ea43495aac","1c54bebf9b834d13bf92e4367c14d074","f714ed9f7aa44387a8e5e529b143a6d7","849ca909200243cda1298032bab88609","cbba6261f5a841fb95e25857e8922f24","e7f33d066b484a0793ad890d2a48a55b","bbef5d59763d48ac87c9b10f0c855f03","1ccff3689f2848c1b9f3259a63f2cd95","700fd6313f3e4b9e914ce7cdee0c244f","49ec4eef10f14830ab3e53a4da47dddf","e977624e0f4c40b38d81aea8dcf4e314","29094248171b4806ba08e4e17f7f9b9b","fddc8198116d44bfae0819c46748a438","b6e25f70f8e04e7383e9253cc0553d07","8d783fa1c0104775a358e2038880955e","1abc16bcc12f4016a9942f0ab937e604","37e7090a0cf14660926f8ee8037150d2","5605bdb617b34b5c889c742d71ddb155","feaeb30c0c624884b0d614c92132d2d4","056d795e799649ee9214af2356d5392d"]},"id":"53Vn1JMOb3EK","executionInfo":{"status":"ok","timestamp":1669648177750,"user_tz":-480,"elapsed":16364,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"bf452613-088c-43c2-c069-23bd3a42c26d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c79ab3e13b7a490384eab56e3a39b7cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b26850760989466587f5dece9e181a80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6be75e91b1b4c3ba35fa7d262a37699"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49ec4eef10f14830ab3e53a4da47dddf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[{'label': 'POSITIVE', 'score': 0.9998674392700195}]\n"]}]},{"cell_type":"markdown","source":["### 特徵提取"],"metadata":{"id":"NOQrILe1etmQ"}},{"cell_type":"code","source":["import numpy as np\n","nlp_features = pipeline('feature-extraction')\n","output = nlp_features(\n","           'Code Doctor Studio is a Chinese company based in BeiJing.')\n","print(np.array(output).shape)   #輸出特征形狀"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306,"referenced_widgets":["320b4042d98643ac9d771e0381ab498f","81a57573a49d4b2b807d586797c8dfad","0e87214284a94a8284dcfbcc06d65914","8ebb00f2463343069aa6bd96c3bd9501","82e8cf4cd3f44c48ba98b860a9265ccc","4fe53c284223449088fe515346a0f4e2","ecb2ce4b012b4400b8b31196db9a428a","db8fcc1e08134605a1eaadc2704a64f5","595e88d946a940f3af07caac5da40475","ef7aea9ecd204390a9f5a1d4e183e53e","8b07f790f3344c739f14f9848382125f","3da71f971dc24670ad4af38b8ece5a40","f7a552926c514c4bbdb9d069f2e9e1f3","a808cb994a5f48908e046ffbad5a686a","100ea7a970ab4b5db49960c3a9b43c65","161b48c9ceb141d8bdde6ec69ee1ea79","398862b33546420aaf911dc6063b9199","9852e82a79df440faea1f9d29377d0bd","e3472ad2ff0c44c7a95bc787ddc61801","7a306fc297ee4d4fbc1bd3155e705c58","2e01ecb6e5c54ae3ac3ebba6f947284b","e2929e3ddb604050b0c19f6b3cb65db4","2df98fc6f6e54be18d8af6cb8f73afe2","c260a27528094926b5699f4eab11e125","1384715ddc41428590dd992c00c525f1","fcb87dfb855f4c45bcf4485ac6126e97","87ae86f3bb664f1fa0b571aa20a07619","cf4007a5c6014566a85543620d12c8ba","09935040446e4085b2429796c0054c77","c579880a25ab4bc69826833af2265fc7","a7ff2c0df27241059bdd6903391e1817","4cde493a517847ebb6d6440a6bfd72ac","a844d28325dc4cc886ef09c1ae8e29db","26fd01b59bc445e285b851a1977a5c97","68fd370d1af94894a660cc56f914a526","64165cd0b2eb494ba164a9a19f35db4c","c4a6b82e0ef8455bb72f1f4d5d1e0ef9","36e8d5404e3f466cbf3240a0bbc9e0a9","2101e05e4e524d1891eb32f6972cef7f","86448f5ba301435bb58a96861cfac2f5","7c05c75189ed482c892f93dcb65df83a","54dd4b05bb8d47939b31e213621473cb","f7cfb7211c21438aa95af6193d351cdf","46e3a03c1fa04ef0a1c31d53595876b7","2ebac21fe0254172b525f490cf607f91","6fd18045759e4199874be5c28e03556a","c5fedc016efe45fc999615c7a022f35d","a87cf7f7fb0643188eb8149fccf62bcf","7cffd3f4652040fb8bd0cd262b1cd309","30cb90c721fc4f79adf5eb2693f936ae","61026767c90643e2a844ffafb366eaba","d8643dcf2449437799f96a0c635475a8","36afbf2d3f0940fcb9a1cadcc38b0b9f","eea570ee7bc045ffbcbf19a70a8aabdb","798f4cae8cda49d186c10bcb3c8675a6"]},"id":"5oPQG2H9erO2","executionInfo":{"status":"ok","timestamp":1669648195887,"user_tz":-480,"elapsed":13878,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"1def839f-973f-47ad-8fcf-22ce9c2dfb42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-cased and revision 935ac13 (https://huggingface.co/distilbert-base-cased).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"320b4042d98643ac9d771e0381ab498f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/263M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da71f971dc24670ad4af38b8ece5a40"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2df98fc6f6e54be18d8af6cb8f73afe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26fd01b59bc445e285b851a1977a5c97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ebac21fe0254172b525f490cf607f91"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(1, 16, 768)\n"]}]},{"cell_type":"markdown","source":["### 完形填空"],"metadata":{"id":"KvA8vUjwe9fR"}},{"cell_type":"code","source":["nlp_fill = pipeline(\"fill-mask\")\n","print(nlp_fill.tokenizer.mask_token) #輸出遮蔽字元：'[MASK]'\n","#呼叫模型進行處理\n","print(nlp_fill(f\"Li Jinhong wrote many {nlp_fill.tokenizer.mask_token} about artificial intelligence technology and helped many people.\"))\t"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270,"referenced_widgets":["385beb4927ad4de99c0e7edaa2ce8aed","1dbac279905a4912a5df22fda92ca996","82009dc18b9f40ba8b53f30706a857bc","ffcb07e6a6544d16a4c722b62d598ac8","279d920ee57941a1b9d8aebcb80d3d0f","ca57bfc86d774928ab049dc5287f3794","e90ad40c980e492c9e0769fbe59af8b8","3610f93a2a7746fbacfab5f578734b19","25387016cd25439fa689bec96b7bae35","ab5abad8373c45d7a97b876465c6102f","f6a727ef42324fc28da5c91ba516acdb","e933049f88044f94bcf01ca7b38bbabd","ef80ad213d4e482aa08a1fcb20b41761","e08024814ac54ffb9096bb66abc17af8","00b1befac72c400d9f0c74fdbfcbd847","a4c05daa9e8d47b5bd0535c49215f1d0","71ff435e7deb46af90a4966928f65607","fec282f7868a4d4285e7a073ae1305cc","cfdf4679015b4a6ca97f210cbb1670f1","0110b9325374448c840d7e63e13ddff6","e6e678f917da45dca65e742bd964090a","13343c3d70134b9cb3265755930cfbad","dd7cea8826534005934685b1e11289fa","19c0519ea38849b3b668672e41236fce","ce77bdccca714496850f0bcfe942417b","2715ebee6d21405b87cc425c89a7743e","ff010b24fcef405085d195620fc7f277","1697c6865275400cb5efc9642713b2da","074166fd6ee748ea9379989d42e2f345","87fc44f189b04b42b286c152db6b3274","991312cd0e984ff4b3f72b59123737f3","90c57dbfecb140359abaf9a413dfe966","c359c2a5fbb049fbb58e19abfa1d82c3","252c1a0fe45e483c9130243f2fbf08a4","db19f2b9a01944dc84e9b71d58c6406e","6d56fed567c544cf953d2d67fa572876","d79397d67b2b4f5cab7502a15b564042","063f1a542fde43c59fa533db8e9508f7","b76781475fc7406fb147cdf7ceb7e552","abce317b7a4a4fd79f27292fc1066249","65db854930164e3698f37941d58bbb37","0be2f12eec814e67b0440c48626f7d32","fc9bd481257e4e6a9c0b9d4a6149b93f","3c2af0166a7d4a8bb6bf906591e4b8a1","c83b07e8f6414422809f566c0eaff207","9ff51d6f72c64816affdce3accc13418","e1aa351ab42e416f841938b9f152add8","e72193f54e004e159799e218f3d48837","704e273faf8e4c2284be12ceaed3b04b","6557f30e50de4f1fb38efbbdfbbc5b4b","0a3b0bf8d92742b8abe1d77e37111031","b70048044c8a477581edfc8fd2bf72a5","980cf2e0b9f0445f9c1a50470c15765c","1adf20a9e8c74c80b80759206c60e655","2a11073d60a344f3a4ae533cc22ae1b5"]},"id":"HPXEK2fXe0sr","executionInfo":{"status":"ok","timestamp":1669648219038,"user_tz":-480,"elapsed":17241,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"15cd65d3-3ff9-479f-d2b7-34b2f9eb4f41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"385beb4927ad4de99c0e7edaa2ce8aed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/331M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e933049f88044f94bcf01ca7b38bbabd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7cea8826534005934685b1e11289fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252c1a0fe45e483c9130243f2fbf08a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c83b07e8f6414422809f566c0eaff207"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<mask>\n","[{'score': 0.5444343090057373, 'token': 2799, 'token_str': ' books', 'sequence': 'Li Jinhong wrote many books about artificial intelligence technology and helped many people.'}, {'score': 0.3202725946903229, 'token': 7201, 'token_str': ' articles', 'sequence': 'Li Jinhong wrote many articles about artificial intelligence technology and helped many people.'}, {'score': 0.02494569681584835, 'token': 27616, 'token_str': ' essays', 'sequence': 'Li Jinhong wrote many essays about artificial intelligence technology and helped many people.'}, {'score': 0.021165847778320312, 'token': 6665, 'token_str': ' papers', 'sequence': 'Li Jinhong wrote many papers about artificial intelligence technology and helped many people.'}, {'score': 0.018288157880306244, 'token': 22064, 'token_str': ' blogs', 'sequence': 'Li Jinhong wrote many blogs about artificial intelligence technology and helped many people.'}]\n"]}]},{"cell_type":"markdown","source":["### 閱讀理解"],"metadata":{"id":"aW4sgQDkfRhJ"}},{"cell_type":"code","source":["nlp_qa = pipeline(\"question-answering\") \t\t#案例化模型\n","print(\t\t\t\t\t\t\t\t\t\t#輸出模型處理結果\n","  nlp_qa(context='Code Doctor Studio is a Chinese company based in BeiJing.',\n","           question='Where is Code Doctor Studio?') )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232,"referenced_widgets":["d3424896ff244e849034a8d08c00c91a","f0298d4ef0bd4158bf65bddf8995bbdc","c3a59579318441b6b73d59ceb6cdd012","9d2bcf824b1b464e8f08628ac8cbc205","8e1eaf3a831743909bf1f0e8944f390a","2267c6b821ba45839c21a1177b698e40","7e519bc0c9384ed7af6bfb15ab9ed2c6","cc0bc8ce643b473d9eedc5d988a551f5","079e24ae387c493d8a05ba5b9fda3533","835e693e062f425d8a001ca5fb0ba809","66641bdfe90e41b0aecb6749c79ec2f2","84db7b6981d94ad5a45b7badf8a6d545","015afe57595e45f08f0ef9cddace4f21","7db78a7a85f74b058aa3def70d0e541f","bcb679f957db420794e68694492b37a7","83d074a45a694888884bbeec59057620","56e6dee582b94bbbad705d74ada99e44","685a31619fff4a2ea0e425a106619071","099a102a72944b658afb18023c50f6a5","ead70eb24f1a40c08b84dccb0aa27f42","3e6b498a60aa457494a27b11b550e8f0","c852f6522b2141abbf979153ffc311bf","25c81ae44db94af6934c557e96ae6dbb","8960b5f16fcf4153aa6fe77912c28d79","7ee36b00eeb34c5d9e9b1bef8632993d","e376a6c6bb0042e8be46c4fe78fe59cd","d5a95bee4cf94e29b5e7263da7ca0c92","6f9574cf1f4243398e7d906c1b29916e","5822a811874d48ac96749bd274d11366","0408569fd4a749079de876ffdcc65edf","a4b98a742a73494da6fc772aa9d48aee","af3368499da84e4191f13967ff2c61d1","40d5e01e23fb4d78b7e596c4655846d9","e5e8efeb7b584de882797dc7029a11c9","07364f93f44246eaaaafe0eb7696039d","f2e428177e774072b7e386c9d7bc2376","3f3cb05b23ef428884eace6590064097","127ac8e4bd844a49b677f7fbbec1e3d7","9c076322c60c47edb6823fd95bc70ee1","1690127a62834a7e860e6862be4c3743","8d9db7ea06164bbf84d35ded8b98d2b7","b4a0c6e087334c6c8d447c57eec8da63","c230234208344759a9bd0f518ba986be","9cf92213263446af8de06ba5f8fd30bc","7d8950c3957b43fe9171183e4150284f","6ae72ec85caa40a489dc2cdc3a64a515","982e95e329524b418f204bfd789819b5","d60721394eb24133be6c69bb37f9b27e","0526ff5ff700483d88831ab47d1f4bc6","9354bf1b255941949df3e0b05ef19476","8c5633b4c5eb4f20bf85449778505081","3d79e78c43cf48ff9c48f6fd91994117","87e55e70f1084d3086dcf39ba7f8b67b","eb4e7b750c2b48369e52aeb39ba18435","9701e2ca3f8547aeab430f751b041eec"]},"id":"8zhqd2pnfSsO","executionInfo":{"status":"ok","timestamp":1669648231316,"user_tz":-480,"elapsed":9753,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"f6facd3f-2260-4f67-bac8-3db14a99e9b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3424896ff244e849034a8d08c00c91a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84db7b6981d94ad5a45b7badf8a6d545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25c81ae44db94af6934c557e96ae6dbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e8efeb7b584de882797dc7029a11c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8950c3957b43fe9171183e4150284f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'score': 0.9549620151519775, 'start': 49, 'end': 56, 'answer': 'BeiJing'}\n"]}]},{"cell_type":"markdown","source":["### 摘要生成"],"metadata":{"id":"dsVc4f_ofZm1"}},{"cell_type":"markdown","source":["該管線的預設模型是\"bert-large-cnn\"，但Transformers函數庫中，還沒有TensorFlow版的BERT預先編譯模型，所以需要手動指定一個支援TensorFlow架構的摘要產生模型。這裡使用的[T5模型](https://cloud.tencent.com/developer/article/1537682)，是Text-to-Text模型"],"metadata":{"id":"OSEZe9XvfrPg"}},{"cell_type":"code","source":["TEXT_TO_SUMMARIZE = '''\n","In this notebook we will be using the transformer model, first introduced in this paper. Specifically, we will be using the BERT (Bidirectional Encoder Representations from Transformers) model from this paper.\n","Transformer models are considerably larger than anything else covered in these tutorials. As such we are going to use the transformers library to get pre-trained transformers and use them as our embedding layers. We will freeze (not train) the transformer and only train the remainder of the model which learns from the representations produced by the transformer. In this case we will be using a multi-layer bi-directional GRU, however any model can learn from these representations.\n","'''\n","summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\",\n","                     framework=\"tf\") # 使用TensorFlow框架\n","print(summarizer(TEXT_TO_SUMMARIZE,min_length=5, max_length=150))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178,"referenced_widgets":["19fdff0c70484983b61a7dc3e6adcf24","0d91767a0f6741618b341503214c37db","e0c7dc16159e4f7fbf58abc82591bd22","5f771834796e4678ba909187ab149d74","59e144662afb40be945489274880549a","5a0ec4c7ab0e4784b68956a6acf6eac2","5d8c09cbe90d45408b99d6347d004039","42be8a27616a4ec2b777973d5c51ee8a","75617b5112d94a61864c8256c4e923ae","cb4ee9d0459744f79fef13694e1b3f95","cfc04f67aa814a2da364cadcca26e9cb"]},"id":"2l1h-3TifZYP","executionInfo":{"status":"ok","timestamp":1669648387963,"user_tz":-480,"elapsed":60787,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"fee3e5f2-7aa3-4e29-dfe0-549258963bf8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19fdff0c70484983b61a7dc3e6adcf24"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n","\n","All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","Your max_length is set to 150, but you input_length is only 149. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=74)\n"]},{"output_type":"stream","name":"stdout","text":["[{'summary_text': 'in this notebook we will be using the transformer model, first introduced in this paper . we will use the transformers library to get pre-trained transformers .'}]\n"]}]},{"cell_type":"markdown","source":["### 手動加載模型"],"metadata":{"id":"c4nU0oPZjc12"}},{"cell_type":"markdown","source":["在pipeline類的初始化接口中，可以直接指定加載模型的路徑，從本地預訓練模型文件進行載入。"],"metadata":{"id":"OVQ-3SeH2H96"}},{"cell_type":"markdown","source":["但有一個前提條件: 所要載入的預訓練模型文件，必須使用固定的文件名稱。"],"metadata":{"id":"BK9ixI2d2Z10"}},{"cell_type":"markdown","source":["在pipeline類接口中，預訓練模型文件，是以套為單位的。每套預訓練模型文件的組成，及其固定的文件名稱如下:\n","* 詞表文件: 以.txt、.model或.json為擴展名，存放模型中使用的詞表文件\n","* 詞表擴展文件(可選): 以.txt為擴展名，補充原有的詞表文件\n","* 配置文件: 以.json為擴展名，存放模型的超參數配置\n","* 權重文件: 以.h5為擴展名，存放模型中，各個參數的具體值"],"metadata":{"id":"YBzKTHE22jIv"}},{"cell_type":"code","source":["from transformers.models.auto.processing_auto import AutoTokenizer, AutoConfig\n","# 指定NLP任務對應的字符串\n","config = AutoConfig.from_pretrained(r'./t5-small/t5-small-config.json')\n","tokenizer = AutoTokenizer.from_pretrained(r'./t5-small', config=config) # 指定資料夾，會自動從資料夾載入所需的檔案\n","\n","# 指定本地模型\n","nlp_sentence_classif = pipeline('summarization', # 任務\n","                model = r'./t5-small/t5-small-tf_model.h5', # 加載的模型\n","                config = config,\n","                tokenizer = tokenizer \n","                                )"],"metadata":{"id":"Jgy4-1cdhsPb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 實體詞識別"],"metadata":{"id":"g9c6wK_plNi8"}},{"cell_type":"code","source":["nlp_token_class = pipeline(\"ner\")\n","print(nlp_token_class(\n","        'Code Doctor Studio is a Chinese company based in BeiJing.'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["e7a30867b47c40d58e9b2ac6a98ac1c7","c07dfa34483e4c3f8508e5dfcf137b20","3e0080589649400b91653cd230b53d49","9bea91f45d1c40eab74cb465f2849504","51daa83615e7405586098ad340d06c46","f6cccd6d3be14a9d9858d13f6cba2772","b65274eb74f04092b7ad74f012dd74b3","be5fb618edd747a9a1c5fe4535ce4ecb","6ed08ca6719d4c40b73780e7e6282550","314d2ae9863f4704abd5fab7c1695233","97c2c610f09c4ba78f7704fa319879ca","3db19e8a0b524f5e80f12b4ecd268e43","a5a2e40444104159a192e2aa8934de6f","c7dd1d3715334546bc52e4a1db91d44d","5114f92719f04d2ab1b44dbec46036cb","1b22ef44bb294826b7db5315996140cf","c837d5cdf3a34ddeaf10646fc4916a17","5d43277b8b4d49738b96fa57eef3ace5","f9efe5e4255f4ebca4de26c526675e8f","a3f2881ad46c497abbb99ffbfb144879","e0776072858d4e34a65b21d7c30e36f9","d0502b0bbc5a4dcdbaf41f7ff9ce5df8","b0dad89ae25b4043bc965bfc5cbc7272","d70925a074414fe8aa311d0ecf69eed8","0d305a5e6c5444b3af3fbe7882a8f599","38e20fe2f56743ddb392ebfa2e5a3d4c","43a7bdad0e1f45f8aab6bc2fa75cb734","b2a6a548c4494ad9af88c5b3ce174e8e","884aafa4aaa649a09b7fc64eb5a7b175","e1e00f7b1d61487eb531c910698df395","f3ced0f08f864ab0a3bc8cb824cf01fb","645448d08b1b4ee9bb2549416ae22631","430a3be48a1b4f678143343421933a2f","660f5328aedc48a198939b90e4d50a95","76d13591dcdc4394a35a97c0abdd46b4","1da326cd64c14f1f9f6e1a1c13efe574","b59c483969ac4ee7950a68d18f08a880","0287fe739baf47ec9967969f4b728a41","361e83d16aca4b0a8c6d4639aec66f4c","347888d27fe64c709b23acba80647971","8eb5ca76679841659e0eb03e5beac807","346af26b688e4db98b5595c710273c0c","0f969a3704bd460ba1adb86bc592a830","1e2b09264d444ef1953793dd23edc50a"]},"id":"_SpAFia7lRb4","executionInfo":{"status":"ok","timestamp":1669648453748,"user_tz":-480,"elapsed":48656,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"f4463c31-57a5-4db9-84a2-481c3b0b6c6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/998 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7a30867b47c40d58e9b2ac6a98ac1c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db19e8a0b524f5e80f12b4ecd268e43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0dad89ae25b4043bc965bfc5cbc7272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"660f5328aedc48a198939b90e4d50a95"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[{'entity': 'I-ORG', 'score': 0.99824834, 'index': 1, 'word': 'Code', 'start': 0, 'end': 4}, {'entity': 'I-ORG', 'score': 0.9986155, 'index': 2, 'word': 'Doctor', 'start': 5, 'end': 11}, {'entity': 'I-ORG', 'score': 0.99831945, 'index': 3, 'word': 'Studio', 'start': 12, 'end': 18}, {'entity': 'I-MISC', 'score': 0.99704957, 'index': 6, 'word': 'Chinese', 'start': 24, 'end': 31}, {'entity': 'I-LOC', 'score': 0.9869552, 'index': 10, 'word': 'Be', 'start': 49, 'end': 51}, {'entity': 'I-LOC', 'score': 0.96116525, 'index': 11, 'word': '##i', 'start': 51, 'end': 52}, {'entity': 'I-LOC', 'score': 0.9435933, 'index': 12, 'word': '##J', 'start': 52, 'end': 53}, {'entity': 'I-LOC', 'score': 0.95518416, 'index': 13, 'word': '##ing', 'start': 53, 'end': 56}]\n"]}]},{"cell_type":"markdown","source":["## 用BERT實現完形填空"],"metadata":{"id":"vCMl23tkmLEM"}},{"cell_type":"markdown","source":["### 載入詞表"],"metadata":{"id":"qk-aDvR48aXc"}},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import BertTokenizer, BertForMaskedLM, TFAutoModelWithLMHead\n","\n","#載入預訓練模型 tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"],"metadata":{"id":"hPFteAscnpcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#輸入文字\n","text = \"[CLS] Who is Li Jinhong ? [SEP] Li Jinhong is a programmer [SEP]\"\n","tokenized_text = tokenizer.tokenize(text)\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vNbSuQTP8mFQ","executionInfo":{"status":"ok","timestamp":1669648734751,"user_tz":-480,"elapsed":318,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"4cb334bb-786f-40d2-ea93-0e820d67f9b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', 'Who', 'is', 'Li', 'Jin', '##hong', '?', '[SEP]', 'Li', 'Jin', '##hong', 'is', 'a', 'programmer', '[SEP]']\n"]}]},{"cell_type":"markdown","source":["### 遮蔽單詞"],"metadata":{"id":"-Iwg7e-28raJ"}},{"cell_type":"code","source":["masked_index = 8 #遮罩一個標示，用' BertForMaskedLM '預測回來\n","tokenized_text[masked_index] = '[MASK]'\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zq3D_di38o_B","executionInfo":{"status":"ok","timestamp":1669648736772,"user_tz":-480,"elapsed":3,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"b80eb15b-d5fe-422e-82c5-99b1bb5f1696"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', 'Who', 'is', 'Li', 'Jin', '##hong', '?', '[SEP]', '[MASK]', 'Jin', '##hong', 'is', 'a', 'programmer', '[SEP]']\n"]}]},{"cell_type":"code","source":["# 將標示轉為詞彙表索引\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","# 將輸入轉為張量\n","tokens_tensor = tf.constant([indexed_tokens])\n","print(tokens_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwCnU72N8xfJ","executionInfo":{"status":"ok","timestamp":1669648738867,"user_tz":-480,"elapsed":402,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"3845371f-0d35-4cc6-deaf-105e9c1077f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[  101  2627  1110  5255 10922 15564   136   102   103 10922 15564  1110\n","    170 23981   102]], shape=(1, 15), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["### 加載預訓練模型"],"metadata":{"id":"I5eZt4cI9anh"}},{"cell_type":"code","source":["# 載入預訓練模型 (weights)\n","model = TFAutoModelWithLMHead.from_pretrained('bert-base-uncased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybyNU_qP84AD","executionInfo":{"status":"ok","timestamp":1669648745111,"user_tz":-480,"elapsed":4277,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"0b3ec4d8-7622-4a83-f3bb-539d3ac98231"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForMaskedLM.\n","\n","All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"]}]},{"cell_type":"markdown","source":["### 進行預測"],"metadata":{"id":"LctHCNc79tOx"}},{"cell_type":"code","source":["# 段標記索引，標記輸入文本中的第一句和第二句。\n","# 0對應第一句，1對應第二句\n","segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] # 第一句有8個單詞，第二句有7個單詞\n","segments_tensors = tf.constant([segments_ids])"],"metadata":{"id":"0L95bJA19iRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 預測所有的tokens\n","# output = model(tokens_tensor)\n","outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","    \n","predictions = outputs[0]  #[1, 19, 30522]，30522是詞表中，詞的個數。輸出的結果，表示詞表中，每個單字在句子中，可能出現的機率\n","\n","predicted_index = tf.argmax(predictions[0, masked_index]) # 取出[MASK]對應的預測索引值\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0] #轉成單字\n","print('Predicted token is:',predicted_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JduXPMFd9xl6","executionInfo":{"status":"ok","timestamp":1669648751660,"user_tz":-480,"elapsed":316,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"f5e93933-0fde-42ce-df86-596b87784df2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted token is: Li\n"]}]},{"cell_type":"markdown","source":["## 使用GPT2進行文本生成"],"metadata":{"id":"8p82JNyWAPH5"}},{"cell_type":"markdown","source":["我們使用Transformers庫中的[GPT-2](https://www.gushiciku.cn/pl/gade/zh-tw)模型，實現下一詞的預測功能，通過循環生成下一詞，實現將一句話補充完整。"],"metadata":{"id":"5nGgeX0Pogx4"}},{"cell_type":"markdown","source":["### 載入詞表"],"metadata":{"id":"RdHVO8hLA18g"}},{"cell_type":"code","source":["# 安裝transformers\n","\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FISPImBH8YOn","executionInfo":{"status":"ok","timestamp":1669650989213,"user_tz":-480,"elapsed":11838,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"2efdcc0e-36e1-4ce4-b596-2d784f2081bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 60.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 51.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.24.0\n"]}]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n","\n","# 載入預訓練模型（權重）\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"g47ZoAUEASj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#解碼輸入\n","indexed_tokens = tokenizer.encode(\"Who is Li Jinhong ? Li Jinhong is a\")\n","\n","print( tokenizer.decode(indexed_tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04gCEMlNBs80","executionInfo":{"status":"ok","timestamp":1669651602480,"user_tz":-480,"elapsed":396,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"3b09a08e-450a-4710-9f89-518439852c37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Who is Li Jinhong? Li Jinhong is a\n"]}]},{"cell_type":"code","source":["indexed_tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4mEMTns61Yz","executionInfo":{"status":"ok","timestamp":1669651604821,"user_tz":-480,"elapsed":519,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"f67cb40d-9c0a-469c-cb19-5d865c1b36d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[8241, 318, 7455, 17297, 71, 506, 5633, 7455, 17297, 71, 506, 318, 257]"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["tokens_tensor = tf.constant([indexed_tokens])#轉為張量\n","print(tokens_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-7wfml5Chu9","executionInfo":{"status":"ok","timestamp":1669651607015,"user_tz":-480,"elapsed":2,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"deed84c4-9a55-4885-ad00-f74a116288da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 8241   318  7455 17297    71   506  5633  7455 17297    71   506   318\n","    257]], shape=(1, 13), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["### 加載預訓練模型"],"metadata":{"id":"nFGzuV77Cl8k"}},{"cell_type":"code","source":["# 載入預訓練模型（權重）\n","model = TFGPT2LMHeadModel.from_pretrained('gpt2')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GA9UO0JCButA","executionInfo":{"status":"ok","timestamp":1669651617170,"user_tz":-480,"elapsed":7454,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"2829203e-3392-4274-8a27-889467ab285d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n","\n","All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0mJbELg9mdj","executionInfo":{"status":"ok","timestamp":1669651617171,"user_tz":-480,"elapsed":15,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"55289462-f331-4fcd-f21a-863385b3511f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel at 0x7f1d05bc2890>"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["# 預測所有標示\n","\n","outputs = model(tokens_tensor)\n","predictions = outputs[0]#(1, 13, 50257)\n","predictions.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiWVFSIYB5rZ","executionInfo":{"status":"ok","timestamp":1669651617604,"user_tz":-480,"elapsed":443,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"296b22c7-9f60-44e3-ab4d-bbdc3bb5c205"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([1, 13, 50257])"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# 得到預測的下一詞\n","predicted_index = tf.argmax(predictions[0, -1, :])\n","predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","print(predicted_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gazk2x4B8nM","executionInfo":{"status":"ok","timestamp":1669651617604,"user_tz":-480,"elapsed":5,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"c7f391fd-5656-42bc-f307-df390927d5b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Who is Li Jinhong? Li Jinhong is a young\n"]}]},{"cell_type":"markdown","source":["### 生成句子"],"metadata":{"id":"zw7wxckJCNtW"}},{"cell_type":"code","source":["#產生一段完整的話\n","stopids = tokenizer.convert_tokens_to_ids([\".\"])[0]  # 定義結束字符\n","past_key_values = None # 定義模型參數\n","for i in range(100):\n","\n","    output = model(tokens_tensor, past_key_values=past_key_values) # 預測下一詞。模型開啟連續預測狀態\n","    token = tf.argmax(output.logits[..., -1, :],axis= -1)\n","\n","    past_key_values = output.past_key_values\n","    indexed_tokens += token.numpy().tolist()\n","\n","    if stopids== token.numpy()[0]:\n","        break\n","    tokens_tensor = token[None,:] #增加一個維度\n","    \n","sequence = tokenizer.decode(indexed_tokens) # 進行字符串解碼\n","\n","print(sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gjmwlMVFCrn9","executionInfo":{"status":"ok","timestamp":1669651631173,"user_tz":-480,"elapsed":4455,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"c620d974-8cc1-4645-a4d6-5479ffc49919"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Who is Li Jinhong? Li Jinhong is a young man who is a member of the Li Clan.\n"]}]},{"cell_type":"markdown","source":["GPT模型的使用範例，也可以參考[Kaggle](https://www.kaggle.com/code/vimalpillai/finetuning-gpt2-model-tensorflow)。"],"metadata":{"id":"fPy3KWKoG65-"}},{"cell_type":"markdown","source":["## 遷移BERT對中文分類"],"metadata":{"id":"TyMTuM3SEx4B"}},{"cell_type":"markdown","source":["Transformers函數庫中，提供了大量的預訓練模型，這些模型都是在通用資料集中，訓練出來的。它們並不能適用於實際工作中的NLP任務。"],"metadata":{"id":"4YXNxlq7E8RE"}},{"cell_type":"markdown","source":["如果要根據自己的文字資料，來訓練模型，則還需要用\"遷移學習\"的方式，對預訓練模型，進行微調。"],"metadata":{"id":"FWy1El8mFQIV"}},{"cell_type":"markdown","source":["### 載入套件"],"metadata":{"id":"CIpRdxNYF1CK"}},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import (\n","        BertTokenizer,\n","        TFAutoModelForSequenceClassification,\n","        AutoConfig\n","        )\n","import os"],"metadata":{"id":"ddW6GqH6F3Sj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 樣本資料"],"metadata":{"id":"MgSQkpArFclA"}},{"cell_type":"markdown","source":["本例的數據集包含從[THUCNews](https://github.com/649453932/Chinese-Text-Classification-Pytorch/tree/master/THUCNews/data)數據集中，隨機抽取的20萬條新聞標題，每個樣本長度為20-30，一共10個類別，每類2萬條新聞標題。"],"metadata":{"id":"RUMj7YIx1NME"}},{"cell_type":"markdown","source":["* 類別被放在class.txt中\n","* 訓練數據集: 18萬條，放在train.txt中\n","* 測試數據集: 1萬條，放在test.txt中\n","* 驗證數據集: 1萬條，放在dec.txt中"],"metadata":{"id":"R5hNkm131oys"}},{"cell_type":"markdown","source":["每條樣本，分為兩個部分:\n","* 文本字符串\n","* 所屬的類別標籤索引\n","\n","其中的類別標籤索引，對應於class.txt中的類別順序。"],"metadata":{"id":"eoFiLQsK2Urv"}},{"cell_type":"code","source":["# 加載數據集\n","!git clone https://github.com/649453932/Chinese-Text-Classification-Pytorch.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kELgzOchE39f","executionInfo":{"status":"ok","timestamp":1669653623842,"user_tz":-480,"elapsed":2547,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"3a08cc6c-74ad-4c96-966b-c9a50ba2973a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Chinese-Text-Classification-Pytorch'...\n","remote: Enumerating objects: 215, done.\u001b[K\n","remote: Total 215 (delta 0), reused 0 (delta 0), pack-reused 215\u001b[K\n","Receiving objects: 100% (215/215), 42.09 MiB | 40.05 MiB/s, done.\n","Resolving deltas: 100% (118/118), done.\n"]}]},{"cell_type":"code","source":["# 變更預設路徑\n","%cd ./Chinese-Text-Classification-Pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRxymB-VFkBF","executionInfo":{"status":"ok","timestamp":1669653625970,"user_tz":-480,"elapsed":279,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"f4170e3f-3aca-41e6-9f74-3b2bc34a6640"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Chinese-Text-Classification-Pytorch\n"]}]},{"cell_type":"code","source":["# 加載類別名稱\n","data_dir='./THUCNews/data' #定義資料集根目錄\n","\n","class_list = [x.strip() for x in open(\n","        os.path.join(data_dir, \"class.txt\")).readlines()]\n","len(class_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2VNiIfYhFsqD","executionInfo":{"status":"ok","timestamp":1669653647694,"user_tz":-480,"elapsed":286,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"69b53c19-12a6-413b-d30e-296c9f1a2c9a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["### 加載預訓練模型"],"metadata":{"id":"DVcUURUVGH4h"}},{"cell_type":"code","source":["# tokenizer = BertTokenizer.from_pretrained(r'./bert-base-chinese/bert-base-chinese-vocab.txt')\n","pretrained_weights = 'bert-base-chinese' # 建立模型\n","tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n","\n","#定義指定分類別的組態檔\n","# config = AutoConfig.from_pretrained(\n","#         r'./bert-base-chinese/bert-base-chinese-config.json',num_labels=len(class_list)) \n","\n","config = AutoConfig.from_pretrained(pretrained_weights,num_labels=len(class_list)) \n","#起始化模型，單獨指定config，在config中指定分類別個數\n","# model = TFAutoModelForSequenceClassification.from_pretrained(\n","#        r'./bert-base-chinese/bert-base-chinese-tf_model.h5',\n","#         config=config)\n","model = TFAutoModelForSequenceClassification.from_pretrained(pretrained_weights,config=config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218,"referenced_widgets":["74b3ee5071ab4defa67244c704b7af84","e74b1c07d0c84ae98216d35678006f31","9ff41d0a32544feca7852b85775332dc","762298d0227f41ce984b8873827094e9","fc98a090a5b9451e89153238f228f910","cb215f618e4a4ae8b4d97f651dadd11b","2dbc90e805f54c1d9383aa56534a020a","0f422557b1224dc693219b31ae2005e7","d34f9f6adffd4fc8b44e023caa4ac8a4","ec7e8f43c02a46b18d277c674ea0cfeb","c0a2f9f537db431eada2b41ba2a30658","f494456620be41358875136495e9749e","c4432d0e75bd40509f92f4c633984b64","9f8572c1acf0405ba96eae30729d7da7","8aa22e04a99245f0b11097d691ae9a95","1e239c08dbe94b54a855a663f9efa595","bbceb8cee5084c00a31b87232785c779","d28a209851184812be7ef6b6c8c7c645","bad4db89d3644b5fb50a1ae1510c8ccb","1c78d05161df4fb0818f2e01ee12d302","459b8ee53e684b0fa02febc0dddd6141","5ab71b855e37417cbf54288484cf5731","6202c25bb77e472a87e4de59ba517989","cdac5336074a4ff380afa2167fae2819","06ebf0bd76a6450b9ed6cc4586b93556","0324a6d7385a47e697fa352201ae12e5","9f02d691793442c999103aade0f1f7d6","fc169c547cf940339929f35952ccbf7e","bc2734e099b44c41a10629472405cfb9","d9889d4c68634d4fab2debd78823e42e","751e8eba1e214cfe87a4e74c6f513b47","f36f24cc8f944da4a56d0f8792cf53c6","2f106b01d1064f4aa80c2b545891cea2","e58acd66da964cf2856db2dc4b7eb765","b2795cb00e8f47aa81028000306459f5","6fc12dd8da27406f886704810b773d13","957d892aaf0c4c919540b09fb0e5d16d","2fcc399e30684487819f7f6662d6933e","a5aee491db5c49bb9da08567c669fdfa","db7719e875d3420886ec66bb6a7766bf","759681686dc1429a816a6a50beeddbd5","6fbcb52ba6774678aa6a9aa2e60c0095","b9dcb0b34e994c66a7d905ce0e160718","39c65824ec9c42e195bd14171d4de960"]},"id":"NWdZgJjaGxos","executionInfo":{"status":"ok","timestamp":1669653673145,"user_tz":-480,"elapsed":19863,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"2283bebf-c505-437f-f8a6-b62810fad823"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b3ee5071ab4defa67244c704b7af84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f494456620be41358875136495e9749e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6202c25bb77e472a87e4de59ba517989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58acd66da964cf2856db2dc4b7eb765"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["### 建立資料集"],"metadata":{"id":"RrX-GSQ2DyLT"}},{"cell_type":"code","source":["def read_file(path): #讀取資料集檔案內容\n","    with open(path, 'r', encoding=\"UTF-8\") as file:\n","        docus = file.readlines()\n","        newDocus = []\n","        labs = []\n","        for data in docus:\n","            content, label = data.split('\\t')\n","            label = int(label)\n","            newDocus.append(content)\n","            labs.append(label)\n","            \n","    ids = tokenizer.batch_encode_plus( newDocus,\n","                #！！！！！模型的組態檔中就是512，當有超過這個長度的會顯示出錯\n","                max_length=model.config.max_position_embeddings,  \n","                pad_to_max_length=True)#,return_tensors='tf')#沒有return_tensors會傳回list！！！！\n","  \n","    return (ids[\"input_ids\"],ids[\"attention_mask\"],labs)"],"metadata":{"id":"CUzxPXX-HjHd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["在使用詞表工具時，指定模型的設定檔中的最大長度model.config.max_position_embeddings。在本實例中，該長度為512，當輸入文字大於這個長度，則會被自動截斷。"],"metadata":{"id":"h8Qyyp5kHlB6"}},{"cell_type":"code","source":["#獲得訓練集和測試集\n","\n","trainContent = read_file(os.path.join(data_dir, \"train.txt\")) \n","testContent = read_file(os.path.join(data_dir, \"test.txt\"))\n","len(trainContent)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669653747416,"user_tz":-480,"elapsed":51642,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"4cea2c08-3a02-401b-f0cf-3615a07bc0b3","id":"RK0u1kIpD4ZQ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["def getdataset(features): #定義函數，封裝資料集\n","    \n","    def gen():              #定義產生器\n","        for ex in zip(features[0],features[1],features[2]):\n","            yield (\n","                {\n","                    \"input_ids\": ex[0],\n","                    \"attention_mask\": ex[1],\n","                },\n","                ex[2],\n","            )  \n","      \n","    return tf.data.Dataset.from_generator( #傳回資料集\n","                gen,\n","                ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32}, tf.int64),\n","                (\n","                    {\n","                        \"input_ids\": tf.TensorShape([None]),\n","                        \"attention_mask\": tf.TensorShape([None]),\n","                    },\n","                    tf.TensorShape([]),\n","                ),\n","            ) "],"metadata":{"id":"aVMhgfarDyLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#製作資料集    \n","valid_dataset = getdataset(testContent) \n","train_dataset = getdataset(trainContent) \n","#設定批次\n","train_dataset = train_dataset.shuffle(100).batch(8).repeat(2)\n","valid_dataset = valid_dataset.batch(16)"],"metadata":{"id":"2Ie4udAQDyLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for count_batch in train_dataset.take(2):\n","    print(count_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wsu-GWLHEU_z","executionInfo":{"status":"ok","timestamp":1669653146393,"user_tz":-480,"elapsed":454,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"cb08be61-5019-47c9-fe41-f0de118d17de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["({'input_ids': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n","array([[ 101, 1957, 2094, ...,    0,    0,    0],\n","       [ 101, 2126, 5661, ...,    0,    0,    0],\n","       [ 101, 2548, 2054, ...,    0,    0,    0],\n","       ...,\n","       [ 101,  915, 5384, ...,    0,    0,    0],\n","       [ 101, 7032, 6395, ...,    0,    0,    0],\n","       [ 101, 1101, 2548, ...,    0,    0,    0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n","array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       ...,\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(8,), dtype=int64, numpy=array([5, 4, 7, 9, 6, 6, 2, 9])>)\n","({'input_ids': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n","array([[ 101, 3119, 4669, ...,    0,    0,    0],\n","       [ 101,  517, 1506, ...,    0,    0,    0],\n","       [ 101, 3862, 3895, ...,    0,    0,    0],\n","       ...,\n","       [ 101,  517, 7608, ...,    0,    0,    0],\n","       [ 101, 4408, 5709, ...,    0,    0,    0],\n","       [ 101,  677, 3862, ...,    0,    0,    0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n","array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       ...,\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(8,), dtype=int64, numpy=array([2, 9, 1, 4, 5, 9, 5, 3])>)\n"]}]},{"cell_type":"markdown","source":["### 訓練模型"],"metadata":{"id":"NYkYBqycJLpw"}},{"cell_type":"code","source":["#定義改善器\n","optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0) # clipnorm限制傳播過程中，梯度的變化範圍\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"],"metadata":{"id":"6dRjapN-JNT0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["請開啟大量RAM。"],"metadata":{"id":"KvL0eUEdH9Yj"}},{"cell_type":"code","source":["#訓練模型\n","history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n","                    validation_data=valid_dataset, validation_steps=7)\n","\n","#儲存模型\n","savedir = r'./myfinetun-bert_chinese/'\n","os.makedirs(savedir, exist_ok=True)\n","model.save_pretrained(savedir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPlky2eaJQpC","executionInfo":{"status":"ok","timestamp":1669660161073,"user_tz":-480,"elapsed":6393377,"user":{"displayName":"劉封均","userId":"11061424056557264029"}},"outputId":"da453aaf-330f-4d69-a332-e0b09eeaaa52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","115/115 [==============================] - 3222s 28s/step - loss: 1.4156 - accuracy: 0.5913 - val_loss: 0.3368 - val_accuracy: 0.9554\n","Epoch 2/2\n","115/115 [==============================] - 3169s 28s/step - loss: 0.6822 - accuracy: 0.8217 - val_loss: 0.2770 - val_accuracy: 0.9375\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kPJ4FZviBs7k"},"execution_count":null,"outputs":[]}]}