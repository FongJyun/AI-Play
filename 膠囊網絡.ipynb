{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 簡介"
      ],
      "metadata": {
        "id": "zJVFy1_eIcIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hinton等人表示，人工神經網絡不應當追求\"活動中的視角不變性\"(即使用單一的標量輸出，來總結一個局部池中的重複特徵檢測器的活動)，而應當使用局部的\"膠囊\"。每個膠囊，學習辨識一個有限的觀察條件，和變形範圍內，隱式定義的視覺實體，並輸出實體在有限範圍內，存在的機率，及一組\"實例參數\"，這些實例參數，可能包括實體的姿勢、照明條件，以及變形訊息。"
      ],
      "metadata": {
        "id": "KDY_0hhKIhYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "當膠囊工作正常時，視覺實體存在的機率，具有**局部不變性**: 當時體在膠囊覆蓋的範圍內的外觀流形上，進行移動時，其機率不會改變。"
      ],
      "metadata": {
        "id": "iW0uHwauJlyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "而實例參數，是**等變的**: 即隨著觀察條件的變化，實體在外觀流形上，進行移動時，實例參數也會相應地變化。因為實例參數是，實體在外觀流形上的內在座標。"
      ],
      "metadata": {
        "id": "tnQ_tYZKJ7cv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnn_48QaIY56",
        "outputId": "c39031f8-4494-43ef-8bd0-0693a6461973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 掛載雲端硬碟\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "詳見[此處](https://blog.csdn.net/m0_46384757/article/details/121559514?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-121559514-blog-79763486.pc_relevant_multi_platform_whitelistv4&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-121559514-blog-79763486.pc_relevant_multi_platform_whitelistv4&utm_relevant_index=1)。"
      ],
      "metadata": {
        "id": "3HDoni3SiDcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 膠囊網絡"
      ],
      "metadata": {
        "id": "2S4MlLwFKgbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "當我們通過計算機圖形渲染(設計)，來建構對象時，我們需要指定並提供一些幾何信息，比如，告訴計算機:\n",
        "* 在何處繪製對象\n",
        "* 該對象的比例\n",
        "* 角度及其他空間信息\n",
        "\n",
        "其中的對象，通常是通過參數設置來呈現的。當這些信息全部表示出來，就是屏幕上的一個對象。"
      ],
      "metadata": {
        "id": "kaq50XyVK24h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "但是，如果只是透過觀察照片中的物體，來提取信息，則**膠囊網絡(Capsule Network)**提出一個思想: **逆渲染(Inverse redering)**。"
      ],
      "metadata": {
        "id": "8UfXGaL2LuEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在膠囊網絡中，網絡要學習如何反向渲染圖象: 通過觀察圖像，嘗試預測圖象的實例參數。"
      ],
      "metadata": {
        "id": "GXuYyjvOMJR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "膠囊網絡通過重現它檢測到的對象，將重現結果與訓練數據中的標記，進行比較，來學習如何預測。通過反覆地學習，它將可以實現較準確的實例參數預測。"
      ],
      "metadata": {
        "id": "ElBeXE4KMdIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 膠囊"
      ],
      "metadata": {
        "id": "xrUv8n8CNlAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "一個膠囊網絡，是由膠囊，而不是神經元構成的。它將個體神經元，替換成了一組神經元組成的向量，這些神經元被包裹在一起，就組成了膠囊。它們可以學習在一張圖片中，在一定區域內，檢查一個特定的對象。輸出是一個向量。"
      ],
      "metadata": {
        "id": "K26ObNsEM4mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "其中，每個向量的長度，代表了物體是否存在的機率。而每個向量的方向，記錄了物體的姿態參數。如果物體有稍微的變化(移動、旋轉、尺寸變化等)，則膠囊也會輸出一個，長度相同，但方向稍微變化的向量。因此，膠囊是\"等變的\"。"
      ],
      "metadata": {
        "id": "vD5Dw_DJNoxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在傳統神經網絡裡，一個神經元，一般會進行如下的操作:\n",
        "1. 輸入標量的標量加權\n",
        "2. 對加權後的標量，進行求和\n",
        "3. 對結果進行非線性變換，生成新的標量"
      ],
      "metadata": {
        "id": "AzrRvnrjQ_Cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "而膠囊可以理解為，上述步驟的向量版，並增加對輸入的仿射變換:\n",
        "1. 輸入向量的乘法:\n",
        " * 膠囊的輸入向量，編碼了低層膠囊檢測出的相應對像的機率，\n",
        " * 而向量的方向，編碼了檢測出的對象的一些內部狀態\n",
        "2. 仿射變換:\n",
        " * 將這些像量，乘以相應的權重矩陣W\n",
        " * W編碼了低層特徵(眼睛、嘴巴)和高層特徵(臉)之間的空間關係，以及其他重要關係\n",
        "3. 動態路由:\n",
        " * 使用權重，決定來自下一層的像量，將會如何進入高一層的向量，這一過程，不加入偏移項\n",
        " * 動態路由的關鍵在於，權重係數該如何確定。對於最大池化而言，只有一個值能夠進入本層，其效果就是一個獨熱向量。而動態路由通過一種類似具類的方式，對向量賦予權重\n",
        "4. 輸出的非線性變換:\n",
        " * 膠囊網絡的非線性啟動函數，接收一個向量，然後在不改變方向的前提下，將其長度壓縮到1以下\n"
      ],
      "metadata": {
        "id": "OUaoo0J3Rl2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN的缺陷"
      ],
      "metadata": {
        "id": "CG2u6eIXOcTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "膠囊網絡的目的，是要解決CNN的一個缺陷: 它只會觀察圖像的特徵，而不會關注特徵的姿態。"
      ],
      "metadata": {
        "id": "5OBa8O65Oe7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "這主要的原因在於，CNN中的pooling會丟失空間中的大部分信息。Geoffrey Hinton實際上認為: 在CNN中應用pooling，是一個很大的錯誤，它工作得很好是事實，是一場災難。"
      ],
      "metadata": {
        "id": "kBgiOLZpO_dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 解決方法"
      ],
      "metadata": {
        "id": "-GA8z-mjPkj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "膠囊網絡的解決方法是，實現對空間信息進行編碼的同時，也計算物體的存在機率。"
      ],
      "metadata": {
        "id": "H12V-Z0ZPwTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "這可以用向量來表示: \n",
        "* 模: 特徵的存在機率\n",
        "* 方向: 特徵的姿態信息"
      ],
      "metadata": {
        "id": "bwXqtXBSP9CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "此外，也可以透過自動編碼器的\"重建損失\"，來改善。"
      ],
      "metadata": {
        "id": "lVap15JYZgWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 動態路由"
      ],
      "metadata": {
        "id": "TJMY2y6iT70q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在膠囊網絡中，權重參數並不是通過訓練得到的，而是由**動態路由**決定的。"
      ],
      "metadata": {
        "id": "b4QykuVhUZyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在這裡的迭代計算過程中，在係數最終確定前的步驟，是不會計算梯度的，而單純通過\"聚類\"的方式得到，並不會參與梯度下降的調整參數過程。"
      ],
      "metadata": {
        "id": "CWKHHXBjUqm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "它會使用softmax，表示輸入向量進入每一個高層膠囊的機率。即，向量在上層的某個膠囊空間中，越接近中心簇，則該膠囊感知到的可能性越大，因此分配到這個膠囊的權重就越大。"
      ],
      "metadata": {
        "id": "Ig_jqHpWVFss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在論文中，列出了動態路由的實際算法:\n",
        "1. 假設該路由演算法，發生在網絡的第k層，i為膠囊單元的個數，j為每個膠囊單元向量的維數，則輸入值$\\hat u_{j|i}$是主膠囊網絡的輸出特徵$u_i$與權重$w_{ij}$的乘積。\n",
        "2. 初始化變數$b_{ij}$，使其為0。\n",
        " * 它與耦合係數c，具有相同的長度\n",
        " * 在迭代時，c就是由b做softmax計算得來。隨著算法的繼續，分布將會改變\n",
        "3. 讓路由演算法，按照指定的迭代次數r，進行迭代\n",
        "4. 對變數b做softmax操作，獲得耦合係數c。它是每個權重的機率\n",
        " * b與c的個數，與膠囊單元的個數相同\n",
        "5. 將c與$\\hat u_{j|i}$相乘，並將乘機的結果，進行相加，得到數字膠囊(k+1層)的輸出向量s\n",
        "6. 透過啟動函數squash，對s做非線性變換，獲得輸出$v_j$。這確保了向量的方向被保存下來，而長度被限制在1以下\n",
        "7. 將$v_j$與$\\hat u_{j|i}$進行點積運算，再與b相加，得到新的b值\n",
        " * 點積的作用是: 計算膠囊的輸入和膠囊的輸出之相似度，利用相似度，來更新b值\n",
        "8. 該路由演算法，需要迭代計算r次。\n",
        " * 在進行路由更新的同時，也更新了最後的輸出結果$v_j$\n",
        " * 在迭代結束後，將最後的$v_j$傳回，計算損失與輸出，並確定路由的權重"
      ],
      "metadata": {
        "id": "GtvBXHVrwOYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在路由(Routing)過程中，下層膠囊，將輸入向量，傳送到上層膠囊。"
      ],
      "metadata": {
        "id": "ppODIfiJXdiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "對於可以路由到的每個更高層膠囊，下層膠囊通過將自己的輸出，乘上權重矩陣，來計算預測向量。如果預測向量與上層的輸出，具有較大的標量積，則存在從頂向下的反饋，其具有增加上層膠囊耦合係數，減小其他膠囊耦合係數的效果。"
      ],
      "metadata": {
        "id": "AXi7HlJEXqDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "就類似聚類，並剔除離群點。"
      ],
      "metadata": {
        "id": "o7Q7JkEgYQ8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## squash啟動函數"
      ],
      "metadata": {
        "id": "cJhZnKouXfbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "因為膠囊網絡的輸出是向量，所以有必要為膠囊網絡，設計一套全新的啟動函數:\n",
        "$$y=\\frac{||x||^2}{1+||x||^2}\\frac{x}{||x||}$$\n",
        "其中\n",
        "* 前項: 將數值轉換成[0,1]之間\n",
        "* 後項: 保留原有向量的方向\n",
        "\n",
        "兩者結合，使值域在[-1,1]。"
      ],
      "metadata": {
        "id": "oqMz02uFXt_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果拋開理論，單純從輸入、輸出的數值上看，則squash與一般的啟動函數沒什麼區別，把它換成一般的啟動函數，也能夠執行。只不過大量的實驗表明，squash在膠囊網絡中的表現，確實比其他啟動函數要好。"
      ],
      "metadata": {
        "id": "4f1Fe1DWacGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 損失函數"
      ],
      "metadata": {
        "id": "Aw4FbfXteIu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "膠囊網絡的損失函數，由兩部分組成:\n",
        "* 為了應付重疊數字識別任務，因為目標可能包含多個標籤，也可能只有一個標籤，所以使用\"邊距損失\"，用於懲罰偽陽性與偽陰性的識別結果\n",
        "* 為了能夠保留特徵的空間信息，設置一個圖像的重構任務，並設置\"重建損失\"，即使用一個解碼器，進行還原，並使用MSE損失，與原圖像進行像素級別地比較，並計算損失(用於計算空間信息、向量方向)"
      ],
      "metadata": {
        "id": "rlXo-EzXgIA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "對於k個類別的數字，我們希望最高層的膠囊，若且維若，一個數字出現在圖象中時，具有一個長的實例化向量。為了允許多個數字，對於每一個膠囊，使用獨立的損失函數:"
      ],
      "metadata": {
        "id": "f2U-HH3kd8DT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **邊距損失(margin loss)**是一種最大化正負樣本到超平面距離的演算法:\n",
        "$$L_k=T_kmax(0, m^+-||V_k||)^2+\\lambda(1-T_k)max(0, ||V_k||-m^-)^2$$\n",
        "其中\n",
        "* $L_k$ : 損失值\n",
        "* $T_k$ : 標籤\n",
        "* $:m^+$ : 最大值的錨點\n",
        "* $:m^-$ : 最小值的錨點\n",
        "* $V_k$ : 模型輸出的預測值\n",
        "* $\\lambda$ : 縮放參數\n",
        "* $||V_k||$ : $V_k$的範數"
      ],
      "metadata": {
        "id": "DF1pmzhieLRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "然後將所有損失值取平均，就得到最終的損失。"
      ],
      "metadata": {
        "id": "Bgzcjt4JekFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以MNIST為例:\n",
        "* 假設$m^+=0.9、m^-=0.1、\\lambda=0.5$(確保訓練中，數值的穩定性)，因為輸出值的形狀是[10, 16]，所以$L_k$的形狀是[10, 16]\n",
        "* 對每個類別的16維向量，進行相加，使其形狀變成[10, 1]\n",
        "* 再取平均值，就得到loss值"
      ],
      "metadata": {
        "id": "eQ_yKuFkfHLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **MSE損失**就是常見的回歸任務的損失。"
      ],
      "metadata": {
        "id": "wZazJXACh0d1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 優點"
      ],
      "metadata": {
        "id": "Be-rqjT8YvT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 膠囊網絡可以使用少很多的訓練數據，就能夠在MNIST數聚集上，達到業界最佳精度\n",
        "2. 膠囊網絡可以很好地應對**模糊性**，非常適合擁擠的場景\n",
        "3. 在膠囊網絡中，細節的姿態信息，會被網絡保存下來，輸入上的微小變化，會帶來輸出上的小變化，這讓它能使用一個簡單和統一的架構，來應對不同的視覺任務，非常適合目標檢測任務\n",
        "4. 膠囊網絡的路由樹結構，映射著對象組件的層次結構關係，可以告知我們這些布建的層次結構"
      ],
      "metadata": {
        "id": "V9cvN09JYvNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 缺點"
      ],
      "metadata": {
        "id": "ZPEL9uHGaqxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 相較於CNN，膠囊網絡的訓練速度很慢，動態路由過程，非常耗運算資源\n",
        "2. 因為無論是什麼類型的對象，在給定的位置，永遠只有一個膠囊，所以膠囊網絡不可能檢測出，兩個非常靠近的同一類型的對象，稱為**擁擠現象**\n",
        "3. 雖然它可以在MNIST上，表現出很好的性能，但是並未在較大的如ImageNet上，獲得好的結果。這是因為在圖象中，發現的信息過多，會使膠囊脫落\n",
        "\n",
        "總體來看，膠囊網絡仍然處於研究和開發階段，並且不夠可靠，還沒有很成熟的任務。"
      ],
      "metadata": {
        "id": "i6rYKHj5asHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型架構"
      ],
      "metadata": {
        "id": "c8UgAI6JdVKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "低層的膠囊，用於檢測一些特定模式的出現機率與姿態，而高層的膠囊，用於檢測更加複雜的圖像。"
      ],
      "metadata": {
        "id": "vrlSC92gfBw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "整個模型架構，主要分為三層:\n",
        "1. 卷積層: 用於抽取底層特徵\n",
        "2. 初始膠囊層: 用於第二次捲積，並初始化膠囊輸入\n",
        "3. 路由膠囊層: 用於編碼空間信息，並進行最終分類"
      ],
      "metadata": {
        "id": "__rJv2aGfbqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 實作"
      ],
      "metadata": {
        "id": "Cn5YPeaViYln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 載入套件"
      ],
      "metadata": {
        "id": "nL-H7mKwmi35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "USE_CUDA = True if torch.cuda.is_available() else False\n",
        "USE_CUDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQx8NVhifYIp",
        "outputId": "3bbfe0bb-c933-402b-c7c9-8ccda44920ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 建立資料集"
      ],
      "metadata": {
        "id": "v-cEk6P4m4QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, dataset, _batch_size):\n",
        "        super(Dataset, self).__init__()\n",
        "        if dataset == 'mnist':\n",
        "            dataset_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))\n",
        "            ])\n",
        "\n",
        "            train_dataset = datasets.MNIST('/data/mnist', train=True, download=True,\n",
        "                                           transform=dataset_transform)\n",
        "            test_dataset = datasets.MNIST('/data/mnist', train=False, download=True,\n",
        "                                          transform=dataset_transform)\n",
        "\n",
        "            self.train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=_batch_size, shuffle=True)\n",
        "            self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=_batch_size, shuffle=False)\n",
        "\n",
        "        elif dataset == 'cifar10':\n",
        "            data_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])\n",
        "            train_dataset = datasets.CIFAR10(\n",
        "                '/data/cifar', train=True, download=True, transform=data_transform)\n",
        "            test_dataset = datasets.CIFAR10(\n",
        "                '/data/cifar', train=False, download=True, transform=data_transform)\n",
        "\n",
        "            self.train_loader = torch.utils.data.DataLoader(\n",
        "                train_dataset, batch_size=_batch_size, shuffle=True)\n",
        "\n",
        "            self.test_loader = torch.utils.data.DataLoader(\n",
        "                test_dataset, batch_size=_batch_size, shuffle=False)\n",
        "        elif dataset == 'office-caltech':\n",
        "            pass\n",
        "        elif dataset == 'office31':\n",
        "            pass"
      ],
      "metadata": {
        "id": "wwQzj3qQm2ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 建立模型"
      ],
      "metadata": {
        "id": "YV6w68SMnEnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 卷積層"
      ],
      "metadata": {
        "id": "T48b4k5-nHrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=256, kernel_size=9):\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
        "                              out_channels=out_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              stride=1\n",
        "                              )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.conv(x))"
      ],
      "metadata": {
        "id": "8FCLFrbvnAMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 初始膠囊層"
      ],
      "metadata": {
        "id": "1stQfnlcnM66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimaryCaps(nn.Module):\n",
        "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9, num_routes=32 * 6 * 6):\n",
        "        super(PrimaryCaps, self).__init__()\n",
        "        self.num_routes = num_routes\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0)\n",
        "            for _ in range(num_capsules)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = [capsule(x) for capsule in self.capsules]\n",
        "        u = torch.stack(u, dim=1)\n",
        "        u = u.view(x.size(0), self.num_routes, -1)\n",
        "        return self.squash(u)\n",
        "\n",
        "    def squash(self, input_tensor):\n",
        "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
        "        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
        "        return output_tensor"
      ],
      "metadata": {
        "id": "Pnzy23_HnKT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 路由膠囊層"
      ],
      "metadata": {
        "id": "rRcc9Sn0nbd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DigitCaps(nn.Module):\n",
        "    def __init__(self, num_capsules=10, num_routes=32 * 6 * 6, in_channels=8, out_channels=16):\n",
        "        super(DigitCaps, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.num_routes = num_routes\n",
        "        self.num_capsules = num_capsules\n",
        "\n",
        "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
        "\n",
        "        W = torch.cat([self.W] * batch_size, dim=0)\n",
        "        u_hat = torch.matmul(W, x)\n",
        "\n",
        "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n",
        "        if USE_CUDA:\n",
        "            b_ij = b_ij.cuda()\n",
        "\n",
        "        num_iterations = 3\n",
        "        for iteration in range(num_iterations):\n",
        "            c_ij = F.softmax(b_ij, dim=1)\n",
        "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
        "\n",
        "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
        "            v_j = self.squash(s_j)\n",
        "\n",
        "            if iteration < num_iterations - 1:\n",
        "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
        "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
        "\n",
        "        return v_j.squeeze(1)\n",
        "\n",
        "    def squash(self, input_tensor):\n",
        "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
        "        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
        "        return output_tensor"
      ],
      "metadata": {
        "id": "RTrBXFDmndPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 解碼器"
      ],
      "metadata": {
        "id": "aJinYZjFnhBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_width=28, input_height=28, input_channel=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.input_width = input_width\n",
        "        self.input_height = input_height\n",
        "        self.input_channel = input_channel\n",
        "        self.reconstraction_layers = nn.Sequential(\n",
        "            nn.Linear(16 * 10, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, self.input_height * self.input_width * self.input_channel),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, data):\n",
        "        classes = torch.sqrt((x ** 2).sum(2))\n",
        "        classes = F.softmax(classes, dim=0)\n",
        "\n",
        "        _, max_length_indices = classes.max(dim=1)\n",
        "        masked = Variable(torch.sparse.torch.eye(10))\n",
        "        if USE_CUDA:\n",
        "            masked = masked.cuda()\n",
        "        masked = masked.index_select(dim=0, index=Variable(max_length_indices.squeeze(1).data))\n",
        "        t = (x * masked[:, :, None, None]).view(x.size(0), -1)\n",
        "        reconstructions = self.reconstraction_layers(t)\n",
        "        reconstructions = reconstructions.view(-1, self.input_channel, self.input_width, self.input_height)\n",
        "        return reconstructions, masked"
      ],
      "metadata": {
        "id": "NeJD5rB8nd2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 膠囊網絡"
      ],
      "metadata": {
        "id": "6tnlZrPbnn2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CapsNet(nn.Module):\n",
        "    def __init__(self, config=None):\n",
        "        super(CapsNet, self).__init__()\n",
        "        if config:\n",
        "            self.conv_layer = ConvLayer(config.cnn_in_channels, config.cnn_out_channels, config.cnn_kernel_size)\n",
        "            self.primary_capsules = PrimaryCaps(config.pc_num_capsules, config.pc_in_channels, config.pc_out_channels,\n",
        "                                                config.pc_kernel_size, config.pc_num_routes)\n",
        "            self.digit_capsules = DigitCaps(config.dc_num_capsules, config.dc_num_routes, config.dc_in_channels,\n",
        "                                            config.dc_out_channels)\n",
        "            self.decoder = Decoder(config.input_width, config.input_height, config.cnn_in_channels)\n",
        "        else:\n",
        "            self.conv_layer = ConvLayer()\n",
        "            self.primary_capsules = PrimaryCaps()\n",
        "            self.digit_capsules = DigitCaps()\n",
        "            self.decoder = Decoder()\n",
        "\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, data):\n",
        "        output = self.digit_capsules(self.primary_capsules(self.conv_layer(data)))\n",
        "        reconstructions, masked = self.decoder(output, data)\n",
        "        return output, reconstructions, masked\n",
        "\n",
        "    def loss(self, data, x, target, reconstructions):\n",
        "        return self.margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n",
        "\n",
        "    def margin_loss(self, x, labels, size_average=True):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        v_c = torch.sqrt((x ** 2).sum(dim=2, keepdim=True))\n",
        "\n",
        "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
        "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
        "\n",
        "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
        "        loss = loss.sum(dim=1).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def reconstruction_loss(self, data, reconstructions):\n",
        "        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
        "        return loss * 0.0005"
      ],
      "metadata": {
        "id": "9zYHEXMeniyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定義訓練函數"
      ],
      "metadata": {
        "id": "0P8QAifUnx6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_loader, epoch):\n",
        "    capsule_net = model\n",
        "    capsule_net.train()\n",
        "    n_batch = len(list(enumerate(train_loader)))\n",
        "    total_loss = 0\n",
        "    for batch_id, (data, target) in enumerate(tqdm(train_loader)):\n",
        "\n",
        "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        min_value = torch.tensor([0.0])\n",
        "\n",
        "        if USE_CUDA:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            min_value = min_value.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, reconstructions, masked = capsule_net(data)\n",
        "        loss = torch.max(capsule_net.loss(data, output, target, reconstructions),min_value)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        correct = sum(np.argmax(masked.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1))\n",
        "        train_loss = loss.item()\n",
        "        total_loss += train_loss\n",
        "        if batch_id % 100 == 0:\n",
        "            tqdm.write(\"Epoch: [{}/{}], Batch: [{}/{}], train accuracy: {:.6f}, loss: {:.6f}\".format(\n",
        "                epoch,\n",
        "                N_EPOCHS,\n",
        "                batch_id + 1,\n",
        "                n_batch,\n",
        "                correct / float(BATCH_SIZE),\n",
        "                train_loss / float(BATCH_SIZE)\n",
        "                ))\n",
        "    tqdm.write('Epoch: [{}/{}], train loss: {:.6f}'.format(epoch,N_EPOCHS,total_loss / len(train_loader.dataset)))"
      ],
      "metadata": {
        "id": "2DbLOGImnqfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定義測試函數"
      ],
      "metadata": {
        "id": "eOQMn085n4oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(capsule_net, test_loader, epoch):\n",
        "    capsule_net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for batch_id, (data, target) in enumerate(test_loader):\n",
        "\n",
        "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        min_value = torch.tensor([0.0])\n",
        "\n",
        "        if USE_CUDA:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            min_value = min_value.cuda()\n",
        "\n",
        "        output, reconstructions, masked = capsule_net(data)\n",
        "        loss = torch.max(capsule_net.loss(data, output, target, reconstructions),min_value)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        correct += sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n",
        "                       np.argmax(target.data.cpu().numpy(), 1))\n",
        "\n",
        "    tqdm.write(\n",
        "        \"Epoch: [{}/{}], test accuracy: {:.6f}, loss: {:.6f}\".format(epoch, N_EPOCHS, correct / len(test_loader.dataset),\n",
        "                                                                  test_loss / len(test_loader)))"
      ],
      "metadata": {
        "id": "FEFIs05cn2Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定義參數設定函數"
      ],
      "metadata": {
        "id": "AcnV77uxppuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self, dataset='mnist'):\n",
        "        if dataset == 'mnist':\n",
        "            # CNN (cnn)\n",
        "            self.cnn_in_channels = 1\n",
        "            self.cnn_out_channels = 256\n",
        "            self.cnn_kernel_size = 9\n",
        "\n",
        "            # Primary Capsule (pc)\n",
        "            self.pc_num_capsules = 8\n",
        "            self.pc_in_channels = 256\n",
        "            self.pc_out_channels = 32\n",
        "            self.pc_kernel_size = 9\n",
        "            self.pc_num_routes = 32 * 6 * 6\n",
        "\n",
        "            # Digit Capsule (dc)\n",
        "            self.dc_num_capsules = 10\n",
        "            self.dc_num_routes = 32 * 6 * 6\n",
        "            self.dc_in_channels = 8\n",
        "            self.dc_out_channels = 16\n",
        "\n",
        "            # Decoder\n",
        "            self.input_width = 28\n",
        "            self.input_height = 28\n",
        "\n",
        "        elif dataset == 'cifar10':\n",
        "            # CNN (cnn)\n",
        "            self.cnn_in_channels = 3\n",
        "            self.cnn_out_channels = 256\n",
        "            self.cnn_kernel_size = 9\n",
        "\n",
        "            # Primary Capsule (pc)\n",
        "            self.pc_num_capsules = 8\n",
        "            self.pc_in_channels = 256\n",
        "            self.pc_out_channels = 32\n",
        "            self.pc_kernel_size = 9\n",
        "            self.pc_num_routes = 32 * 8 * 8\n",
        "\n",
        "            # Digit Capsule (dc)\n",
        "            self.dc_num_capsules = 10\n",
        "            self.dc_num_routes = 32 * 8 * 8\n",
        "            self.dc_in_channels = 8\n",
        "            self.dc_out_channels = 16\n",
        "\n",
        "            # Decoder\n",
        "            self.input_width = 32\n",
        "            self.input_height = 32\n",
        "\n",
        "        elif dataset == 'your own dataset':\n",
        "            pass"
      ],
      "metadata": {
        "id": "mMGLWsKIptSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 進行訓練與測試"
      ],
      "metadata": {
        "id": "X4S6v0u7n9q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "N_EPOCHS = 2\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(1)\n",
        "    # dataset = 'cifar10'\n",
        "    dataset = 'mnist'\n",
        "    config = Config(dataset)\n",
        "    mnist = Dataset(dataset, BATCH_SIZE)\n",
        "\n",
        "    capsule_net = CapsNet(config)\n",
        "    capsule_net = torch.nn.DataParallel(capsule_net)\n",
        "    if USE_CUDA:\n",
        "        capsule_net = capsule_net.cuda()\n",
        "    capsule_net = capsule_net.module\n",
        "\n",
        "    optimizer = torch.optim.Adam(capsule_net.parameters())\n",
        "\n",
        "    for e in range(1, N_EPOCHS + 1):\n",
        "        train(capsule_net, optimizer, mnist.train_loader, e)\n",
        "        test(capsule_net, mnist.test_loader, e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlIGXjKJn7g6",
        "outputId": "35bff46f-01ba-4c33-e994-b767e0153254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/600 [00:00<06:00,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/2], Batch: [1/600], train accuracy: 0.050000, loss: 0.008995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 101/600 [00:51<04:11,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/2], Batch: [101/600], train accuracy: 0.930000, loss: 0.004105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 201/600 [01:40<03:20,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/2], Batch: [201/600], train accuracy: 0.970000, loss: 0.002332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 301/600 [02:29<02:26,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/2], Batch: [301/600], train accuracy: 0.960000, loss: 0.001191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 401/600 [03:18<01:38,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/2], Batch: [401/600], train accuracy: 0.980000, loss: 0.000694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 501/600 [04:07<00:49,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/2], Batch: [501/600], train accuracy: 0.950000, loss: 0.000875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [04:55<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/2], train loss: 0.002121\n",
            "Epoch: [1/2], test accuracy: 0.983300, loss: 0.055623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/600 [00:00<05:35,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2/2], Batch: [1/600], train accuracy: 0.990000, loss: 0.000341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 101/600 [00:50<04:04,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2/2], Batch: [101/600], train accuracy: 0.970000, loss: 0.000476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 201/600 [01:39<03:15,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2/2], Batch: [201/600], train accuracy: 1.000000, loss: 0.000263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 301/600 [02:28<02:26,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2/2], Batch: [301/600], train accuracy: 0.980000, loss: 0.000365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 401/600 [03:17<01:38,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2/2], Batch: [401/600], train accuracy: 0.970000, loss: 0.000534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 501/600 [04:05<00:48,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2/2], Batch: [501/600], train accuracy: 0.980000, loss: 0.000429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [04:54<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2/2], train loss: 0.000442\n",
            "Epoch: [2/2], test accuracy: 0.989000, loss: 0.037293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JoVOWm0npiiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}